{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing COMP3225\n",
    "Seq2seq Neural Machine Translation (NMT) lab\n",
    "\n",
    "Stuart Middleton, 25/08/2020\n",
    "\n",
    "This lab will provide practical experience with neural machine translation software trained to generate German output text from English input text. You will learn how to use the encoder/decoder model, pre-process a bitext, train the model and use a decoder to translate sentences. You will then learn to use advanced normalization and tokenization, reverse the input sequence and use a beam search decoder for better performamce. \n",
    "\n",
    "In part2 you will learn about how to handle rare words, outside the limited vocabulary supported by NMT solutions. You will generate a statistical phrase alignment using fast_align and use this to generate a lookup dictionary to translate rare words. You will then use the phrase alignment to train the NMT model to predict out of vocabulary words with positional markers, allowing use of the lookup dictionary for translation.\n",
    "\n",
    "## Part 1\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "You will need python3. The code below will work OK on a CPU only machine with small numbers of sentences. We recommend you run the code on a machine with a GPU when training with a large number of sentences (which will give better translation accuracy but take a lot longer to compute).\n",
    "\n",
    "### Task 1 - train a basic NMT model using a seq2seq encoder/decoder architecture\n",
    "\n",
    "Further reading:\n",
    "    [TensorFlow NMT tutorial](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt)\n",
    "    [Luong 2015 NMT paper](https://www.aclweb.org/anthology/D15-1166/)\n",
    "\n",
    "First install python3 and the pre-requisite libraries needed for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```\n",
    "python3 -m pip install numpy\n",
    "python3 -m pip install tensorflow-gpu\n",
    "python3 -m pip install tensorflow_addons\n",
    "python3 -m pip install keras\n",
    "python3 -m pip install sacrebleu\n",
    "python3 -m pip install mosestokenizer\n",
    "python3 -m pip install notebook\n",
    "\n",
    "unzip package for lab\n",
    "jupyter notebook\n",
    "==> will open browser windows from localhost:8888\n",
    "==> load the lab .ipynb file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new python code file for your work.\n",
    "\n",
    "Import this labs required python3 libraries. It is safer to import tensorflow2 last as some complex libaries (e.g. numpy) have been known to conflict with tensorflow implementations of backend code resulting in occasional segmentation faults. Also it is best to use the tensorflow logger, as opposed to the python default logger, as tensorflow2 defines its own logger internally and if this is not used your log messages will probably be ignored inside tensorflow graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata, re, io, time, os, datetime, sys, codecs, math, gc, random, contextlib, itertools, json, string\n",
    "import numpy as np\n",
    "import sacrebleu, mosestokenizer\n",
    "\n",
    "# small library of functions used by this tutorial\n",
    "import lab_seq2seq_nmt_lib\n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import absl.logging\n",
    "formatter = logging.Formatter('[%(levelname)s|%(filename)s:%(lineno)s %(asctime)s] %(message)s')\n",
    "absl.logging.get_absl_handler().setFormatter(formatter)\n",
    "absl.logging._warn_preinit_stderr = False\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "import tensorflow_addons as tfa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the hyperparameters used for the models. The sequence length is 20 words for this lab to speed it up, but lengths are usually 40 or 50 words. As sequences get longer the performance degrades, so more epochs are needed to lower loss, and GPU memory use is proportional to sequence length. The same goes for vocabulary size, which is limited to a maximum of 50k to avoid running out of GPU memory (not a problem for the dataset here but for larger datasets it will be).  The batch size should be large enough to avoid yoyo'ing of loss convergence, which can happen if a single entry in a small batch leads the training astray. For larger models the number of epochs will typically need to be around 100 to achieve a good loss score of 0.01 or lower.\n",
    "\n",
    "The model training will take a long time (hours) if the full bitext corpus is used. Therefore for lab sessions we suggest using a truncated set of sentences (using the DEBUG_TRUNC_SENTS hyperparameter) aiming to get 0.2 loss or lower. At home you can train the model overnight on the full dataset to get much more accurate translations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder embedding dimension\n",
    "EMBEDDING_DIM = 256\n",
    "\n",
    "# encoder LSTM dimension\n",
    "RNN_UNITS = 1024\n",
    "\n",
    "# attension layer dimension\n",
    "DENSE_UNITS = 1024\n",
    "\n",
    "# dataset parameters\n",
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "MAX_SENT_LENGTH = 20\n",
    "MAX_SENT_LENGTH_SOURCE = MAX_SENT_LENGTH\n",
    "MAX_SENT_LENGTH_TARGET = MAX_SENT_LENGTH\n",
    "\n",
    "# beam search parameters\n",
    "BEAM_WIDTH = 12\n",
    "BEAM_LENGTH_NORM_WEIGHT = 1.0\n",
    "\n",
    "# very short run for debug, will probably provide single wrong word translations (in lab)\n",
    "#BATCH_SIZE = 64\n",
    "#EPOCHS = 10\n",
    "#DEBUG_TRUNC_SENTS = 1000\n",
    "#MAX_VOCAB_SIZE = 1000\n",
    "\n",
    "# longer run, better accuracy (in lab with a CPU 1.5h, faster with GPU)\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 10\n",
    "DEBUG_TRUNC_SENTS = 20000\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "\n",
    "# full run, best accuracy (overnight)\n",
    "#BATCH_SIZE = 256\n",
    "#EPOCHS = 100\n",
    "#DEBUG_TRUNC_SENTS = None\n",
    "#MAX_VOCAB_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the model we will use for our neural machine translation application. The architecture follows the seminal sequence to sequence encoder/decoder architecture used by [Luong 2015](https://www.aclweb.org/anthology/D15-1166/). The encoder has an embedding and LSTM layer. The decoder has an embedding later, an attention layer and a LSTM layer. This code is based on the excellent [tensorflow seq2seq NMT tutorial](https://www.tensorflow.org/addons/tutorials/networks_seq2seq_nmt).\n",
    "\n",
    "The focus of this lab is understanding how to use this model for machine translation and get the most from it. It is recommended you read [Luong 2015] to understand fully the inner workings of the model, which hyperparameters are best and get an understanding for how different layer and technique choices affect the final BLEU translation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNetwork(tf.keras.Model):\n",
    "\n",
    "\tdef __init__( self, input_vocab_size, embedding_dims, rnn_units ):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.encoder_embedding = tf.keras.layers.Embedding( input_dim=input_vocab_size, output_dim=embedding_dims )\n",
    "\t\tself.encoder_rnnlayer = tf.keras.layers.LSTM( rnn_units, return_sequences=True, return_state=True )\n",
    "\n",
    "class DecoderNetwork(tf.keras.Model):\n",
    "\tdef __init__(self, output_vocab_size, embedding_dims, rnn_units, dense_units) :\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.decoder_embedding = tf.keras.layers.Embedding( input_dim = output_vocab_size, output_dim = embedding_dims ) \n",
    "\t\tself.dense_layer = tf.keras.layers.Dense( output_vocab_size )\n",
    "\t\tself.decoder_rnncell = tf.keras.layers.LSTMCell( rnn_units )\n",
    "\t\tself.dense_units = dense_units\n",
    "\n",
    "\t\tself.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "\t\t# Create attention mechanism with memory = None\n",
    "\t\tself.attention_mechanism = self.build_attention_mechanism( self.dense_units, None, BATCH_SIZE*[MAX_SENT_LENGTH_SOURCE] )\n",
    "\t\tself.rnn_cell = tfa.seq2seq.AttentionWrapper( self.decoder_rnncell, self.attention_mechanism, attention_layer_size=self.dense_units )\n",
    "\t\tself.decoder = tfa.seq2seq.BasicDecoder( self.rnn_cell, sampler = self.sampler, output_layer = self.dense_layer )\n",
    "\n",
    "\tdef build_attention_mechanism(self, units, memory, memory_sequence_length) :\n",
    "\t\treturn tfa.seq2seq.LuongAttention( units, memory = memory, memory_sequence_length=memory_sequence_length )\n",
    "\n",
    "\tdef build_decoder_initial_state( self, batch_size, encoder_state, Dtype ) :\n",
    "\t\tdecoder_initial_state = self.rnn_cell.get_initial_state( batch_size = batch_size, dtype = Dtype )\n",
    "\t\tdecoder_initial_state = decoder_initial_state.clone( cell_state=encoder_state )\n",
    "\t\treturn decoder_initial_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained on a bitext consisting of sentences from a source language (e.g. EN) and target langauge (e.g. DE). A bitext is simply two sentence aligned files, a source language and its target for translation. Typically a bitext for machine translation will be over 100,000 sentences long. Bitext documents are often sourced from content that has been translated by humans, for example UN meeting minutes which are translated to many languages or wikipedia articles where volunteers have translated them into different languages. The bitext we will use is a EN to DE bitext with 152,820 pairs of sentences.\n",
    "\n",
    "Sentences usually need to be cleaned and normalized before they are ready for use as training data, so the model will not pickup on noise patterns. The below pre-processing function will load a corpus from disk and use simple regex patterns to normalize and clean the text. Later we will replace this function with a more advanced version using machine translation community standard [MOSES](http://www.statmt.org/moses/) tokenization and normalization perl scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_preprocess( bitext_source_file, bitext_target_file, logger ) :\n",
    "\t# load bitext using simple regex normalization\n",
    "\tlist_source_sents = lab_seq2seq_nmt_lib.load_corpus( filename = bitext_source_file, logger = logger, normalize_func = lab_seq2seq_nmt_lib.normalize_sent_regex )\n",
    "\n",
    "\t# translate mode?\n",
    "\tif bitext_target_file == None:\n",
    "\t\treturn list_source_sents, None, None, None\n",
    "\n",
    "\tlist_target_sents = lab_seq2seq_nmt_lib.load_corpus( filename = bitext_target_file, logger = logger, normalize_func = lab_seq2seq_nmt_lib.normalize_sent_regex )\n",
    "\tif len(list_source_sents) != len(list_target_sents) :\n",
    "\t\traise Exception( 'bitext size mismatch' )\n",
    "\n",
    "\t# make bitext\n",
    "\tlist_bitext = []\n",
    "\tfor nSentIndex in range(len(list_source_sents)) :\n",
    "\t\tlist_bitext.append( ( list_source_sents[nSentIndex], list_target_sents[nSentIndex] ) )\n",
    "\n",
    "\tlist_source_sents = []\n",
    "\tlist_target_sents = []\n",
    "\tfor nSentIndex in range(len(list_bitext)) :\n",
    "\t\tlist_source_sents.append( list_bitext[nSentIndex][0] )\n",
    "\t\tlist_target_sents.append( list_bitext[nSentIndex][1] )\n",
    "\n",
    "\treturn list_source_sents, list_target_sents, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sentence needs to be tokenized and converted into a sequence vector. Tokenization is simply converting text into a sequence of tokens, then building a vocabulary index so tokens can be assigned an index value and made into a tensor ready for model training.\n",
    "\n",
    "The below tokenization fits a tf.keras.preprocessing.text.Tokenizer() class on the input sentence lists to build a vocabulary index (one for source and one for target). The index is sorted by frequency of occurance automatically by the Tokenizer() class. For now we allow all words to be kept, but later we will impose a vocabulary limit which is needed for processing a larger bitext corpus. Once fit the tokenizer is applied to each sentence in the corpus, and a tensor created of passed sequences of word indexes representing the input sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_tokenize( list_source_sents, list_target_sents, dict_alignment_matrix, logger, use_tokenizer_source = None, use_tokenizer_target = None ) :\n",
    "\t# train tokenizers (or use existing one if we are in translate phase)\n",
    "\tif use_tokenizer_source == None :\n",
    "\t\ttokenizer_source = lab_seq2seq_nmt_lib.train_tokenizer( list_lines = list_source_sents, logger = logger )\n",
    "\telse :\n",
    "\t\ttokenizer_source = use_tokenizer_source\n",
    "\tif use_tokenizer_target == None :\n",
    "\t\ttokenizer_target = lab_seq2seq_nmt_lib.train_tokenizer( list_lines = list_target_sents, logger = logger )\n",
    "\telse :\n",
    "\t\ttokenizer_target = use_tokenizer_target\n",
    "\n",
    "\t# shuffle corpus and truncate if needed\n",
    "\tif list_target_sents != None :\n",
    "\t\tbitext = list( zip( list_source_sents, list_target_sents ) )\n",
    "\t\trandom.shuffle( bitext )\n",
    "\t\tif DEBUG_TRUNC_SENTS == None :\n",
    "\t\t\tnMax = len(bitext)\n",
    "\t\telse :        \n",
    "\t\t\tnMax = DEBUG_TRUNC_SENTS\n",
    "\t\tlist_source_sents = []\n",
    "\t\tlist_target_sents = []\n",
    "\t\tfor nSentIndex in range(nMax) :\n",
    "\t\t\tlist_source_sents.append( bitext[nSentIndex][0] )\n",
    "\t\t\tlist_target_sents.append( bitext[nSentIndex][1] )\n",
    "\telse :\n",
    "\t\tif DEBUG_TRUNC_SENTS == None :\n",
    "\t\t\tnMax = len(list_source_sents)\n",
    "\t\telse :        \n",
    "\t\t\tnMax = DEBUG_TRUNC_SENTS\n",
    "\t\tlist_source_sents = list_source_sents[:nMax]\n",
    "        \n",
    "\t# apply tokenizers\n",
    "\tsource_tensor_train = lab_seq2seq_nmt_lib.apply_tokenization( list_sents = list_source_sents, tokenizer = tokenizer_source, max_sent_length = MAX_SENT_LENGTH_SOURCE, reverse_seq = False, logger = logger )\n",
    "\n",
    "\t# translate mode?\n",
    "\tif list_target_sents == None :\n",
    "\t\treturn tokenizer_source, tokenizer_target, source_tensor_train, None\n",
    "    \n",
    "\ttarget_tensor_train = lab_seq2seq_nmt_lib.apply_tokenization( list_sents = list_target_sents, tokenizer = tokenizer_target, max_sent_length = MAX_SENT_LENGTH_TARGET, reverse_seq = False, logger = logger )\n",
    "           \n",
    "\treturn tokenizer_source, tokenizer_target, source_tensor_train, target_tensor_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a function to train the model on a bitext corpus, and then run the trained model on a set of unseen sentences to translate them and compute a BLEU score.\n",
    "\n",
    "We start by calling the preprocessing and tokenize functions defined previously. This generates a set of source and target tensors ready for model training.\n",
    "\n",
    "The dict_lookup_dictionary parameter will be used later on so can be ignored for now.\n",
    "\n",
    "The training tensors are loaded into a tf.data.Dataset() class and batched and randomly shuffled to help avoid training bias. Prefetch allows tensorflow to load the next batch in parallel to training on the current batch, multui-tasking for more efficient use of the GPU.\n",
    "\n",
    "Next the encoder and decoder models are created and optimizer defined. At this stage they are untrained. A checkpoint manager is used to allow us to save the best (losest loss) models after each epoch so at the end we have the best model to use. Checkpointing is also helpful to allow resuming training if your computer crashes, helpful if training times are long.\n",
    "\n",
    "We are using a simple tf.keras.losses.SparseCategoricalCrossentropy() function to compute the loss which we will aim to optimize during training. Note the use of [@tf.function](https://www.tensorflow.org/guide/function) to decorate functions used within in the main training loop. This will allow tensorflow to generate an autograph, using tensorflow versions of python variables, and run much more efficiently than it would using normal python variables.\n",
    "\n",
    "The train_step() function feeds a batch of input tensors to the encoder and then decoder, uses a gradiant tape to keep track of the gradients which are then fed to the optimizer to adjust the layer weights. The attention layer takes the (source) sequence encoder output and the (target) sequence decoder input and provides a context vector for the decoder to make a prediction. All batches provided to train_step() every epoch and the overall loss recorded to check for convergence. The last best model is kept at the end of training.\n",
    "\n",
    "Once trained the model is executed on a set of unseen validation sentences. The sentences are preprocesses and tokenized in the same way as the training bitext. A set of translated sentences is also provided as a 'gold' set to allow a BLEU score to be computed at the end.\n",
    "\n",
    "The decode_func() and translate_func() are defined later and do the work of computing the translations using the trained model. The lookup_unkposN() is used later when rare word support is added.\n",
    "    \n",
    "A [BLEU score](https://en.wikipedia.org/wiki/BLEU) is calculated using the [sacrebleu](https://github.com/mjpost/sacrebleu) library, providing a standard measure of the number of tokens shared between the predicted translation and the gold translation. The higher the BLEU score the better, with 20+ providing an error prone but understandable translation and 40+ providing a pretty good translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_task( preprocess_func, tokenize_func, decode_func, translate_func, postprocess_func ) :\n",
    "\tlogger.info( 'task started' )\n",
    "\tlogger.info( '\\tpreprocessing = ' + preprocess_func.__name__  )\n",
    "\tlogger.info( '\\ttokenizer = ' + tokenize_func.__name__  )\n",
    "\tlogger.info( '\\tdecode = ' + decode_func.__name__  )\n",
    "\tlogger.info( '\\ttranslate = ' + translate_func.__name__  )\n",
    "\tif postprocess_func != None :\n",
    "\t\tlogger.info( '\\tpostprocessing = ' + postprocess_func.__name__  )\n",
    "\n",
    "\tmodel_dir = './model'\n",
    "\tmodel_name = 'nmt_en_to_de'\n",
    "\tbitext_source_file = '../corpus/bitext_nmt_en.txt'\n",
    "\tbitext_target_file = '../corpus/bitext_nmt_de.txt'\n",
    "\tlookup_dict_file = './model/lookup_dictionary.txt'\n",
    "\ttranslated_source_file = './model/translate_source.txt'\n",
    "\n",
    "\tif os.path.exists( model_dir ) == False :\n",
    "\t\tos.mkdir( model_dir )    \n",
    "\n",
    "\t#\n",
    "\t# pre-process dataset (load, tokenize, make tensor)\n",
    "\t#\n",
    "\n",
    "\tlist_source_sents, list_target_sents, dict_alignment_matrix, dict_lookup_dictionary = preprocess_func( bitext_source_file, bitext_target_file, logger )\n",
    "\ttokenizer_source, tokenizer_target, source_tensor_train, target_tensor_train = tokenize_func( list_source_sents, list_target_sents, dict_alignment_matrix, logger )\n",
    "\tlogger.info( 'shape of source tensor = ' + repr( source_tensor_train.shape ) )\n",
    "\n",
    "\tvocab_source_size = len(tokenizer_source.word_index)+1\n",
    "\tvocab_target_size = len(tokenizer_target.word_index)+1\n",
    "\tlogger.info( 'tokenizer vocab size = ' + repr( (vocab_source_size,vocab_target_size) ) )\n",
    "\n",
    "\tif dict_lookup_dictionary != None :\n",
    "\t\tlogger.info( 'lookup dict vocab size = ' + repr( len(dict_lookup_dictionary) ) )\n",
    "\t\tlogger.info( 'saving lookup dict to file = ' + lookup_dict_file )\n",
    "\t\twrite_handle = codecs.open( lookup_dict_file, 'w', 'utf-8', errors = 'strict' )\n",
    "\t\twrite_handle.write( json.dumps( dict_lookup_dictionary, indent = 4 ) + '\\n' )\n",
    "\t\twrite_handle.close()\n",
    "\n",
    "\t#\n",
    "\t# make dataset from tensor\n",
    "\t#\n",
    "\n",
    "\tsteps_per_epoch = len(source_tensor_train)//BATCH_SIZE\n",
    "\tlogger.info( 'steps per epoch = ' + repr(steps_per_epoch) )\n",
    "\n",
    "\tdataset = tf.data.Dataset.from_tensor_slices( ( source_tensor_train, target_tensor_train ) )\n",
    "\tdataset = dataset.batch( BATCH_SIZE, drop_remainder=True )\n",
    "\tdataset = dataset.shuffle( SHUFFLE_BUFFER_SIZE, reshuffle_each_iteration=True )\n",
    "\tdataset = dataset.prefetch( buffer_size=tf.data.experimental.AUTOTUNE )\n",
    "\n",
    "\tlogger.info( 'Input batch X shape: {}'.format( tf.random.uniform( (BATCH_SIZE, 1) ).shape) )\n",
    "\tlogger.info( 'Target batch Y shape: {}'.format( tf.random.uniform( (BATCH_SIZE, 1) ).shape) )\n",
    "\n",
    "\t#\n",
    "\t# Train model\n",
    "\t#\n",
    "\n",
    "\tencoderNetwork = EncoderNetwork( vocab_source_size,EMBEDDING_DIM, RNN_UNITS )\n",
    "\tdecoderNetwork = DecoderNetwork( vocab_target_size,EMBEDDING_DIM, RNN_UNITS, DENSE_UNITS )\n",
    "\n",
    "\toptimizer = tf.keras.optimizers.Adam( learning_rate = 0.002, amsgrad = True )\n",
    "\n",
    "\tlogger.info('Encoder params: (vocab_size, embedding_dims, units) {}'.format( repr( (vocab_source_size, EMBEDDING_DIM, RNN_UNITS) ) ))\n",
    "\tlogger.info('Decoder params: (vocab_size, embedding_dims, rnn_units, dense_units) {}'.format( repr( (vocab_target_size, EMBEDDING_DIM, RNN_UNITS, DENSE_UNITS) ) ))\n",
    "\n",
    "\t# setup checkpoint\n",
    "\tcheckpoint_dir = model_dir\n",
    "\tcheckpoint = tf.train.Checkpoint( optimizer=optimizer, encoder=encoderNetwork, decoder=decoderNetwork )\n",
    "\tcheckpointManager = tf.train.CheckpointManager( checkpoint, directory=checkpoint_dir, checkpoint_name=model_name, max_to_keep=3 )\n",
    "\n",
    "\t# setup tf.summary log dir and file handler (to generate output for TensorBoard)\n",
    "\tcurrent_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\ttrain_log_dir = './logs/gradient_tape/' + current_time + '/train'\n",
    "\ttest_log_dir = './logs/gradient_tape/' + current_time + '/test'\n",
    "\ttrain_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\ttest_summary_writer = tf.summary.create_file_writer(test_log_dir)\n",
    "\n",
    "\t# define metrics for TensorBoard\n",
    "\ttrain_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)\n",
    "\n",
    "\t# loss function based on categorical crossentropy\n",
    "\t@tf.function\n",
    "\tdef loss_function(y_pred, y) :\n",
    "\t\tsparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy( from_logits=True,reduction='none' )\n",
    "\t\tloss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
    "\t\tmask = tf.logical_not(tf.math.equal(y,0))\n",
    "\t\tmask = tf.cast(mask, dtype=loss.dtype)\n",
    "\t\tloss = mask* loss\n",
    "\t\tloss = tf.reduce_mean(loss)\n",
    "\t\treturn loss\n",
    "\n",
    "\t# training setep using the tensorflow subclassing API\n",
    "\t@tf.function\n",
    "\tdef train_step( input_batch, output_batch, encoder_initial_cell_state, train_loss ) :\n",
    "\t\tloss = 0\n",
    "\t\twith tf.GradientTape() as tape:\n",
    "\t\t\tencoder_emb_inp = encoderNetwork.encoder_embedding( input_batch )\n",
    "\t\t\ta, a_tx, c_tx = encoderNetwork.encoder_rnnlayer( encoder_emb_inp, initial_state = encoder_initial_cell_state, training = True )\n",
    "\n",
    "\t\t\t# Prepare correct Decoder input & output sequence data\n",
    "\t\t\tdecoder_input = output_batch[:,:-1] # ignore last word in seq as this will always be <end> or <pad> (decoder input -> training)\n",
    "\n",
    "\t\t\t# Compare logits with timestepped +1 version of decoder_input\n",
    "\t\t\tdecoder_output = output_batch[:,1:] #ignore first word to shift seq to right (decoder output -> target for loss function)\n",
    "\n",
    "\t\t\t# Decoder Embeddings\n",
    "\t\t\tdecoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "\t\t\t# Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
    "\t\t\tdecoderNetwork.attention_mechanism.setup_memory(a)\n",
    "\n",
    "\t\t\tdecoder_initial_state = decoderNetwork.build_decoder_initial_state(\n",
    "\t\t\t\tBATCH_SIZE,\n",
    "\t\t\t\tencoder_state=[a_tx, c_tx],\n",
    "\t\t\t\tDtype=tf.float32 )\n",
    "\t\t\t\n",
    "\t\t\t# BasicDecoderOutput\n",
    "\t\t\toutputs, _, _ = decoderNetwork.decoder( decoder_emb_inp, initial_state=decoder_initial_state, sequence_length=BATCH_SIZE*[MAX_SENT_LENGTH_TARGET-1], training = True)\n",
    "\n",
    "\t\t\tlogits = outputs.rnn_output\n",
    "\n",
    "\t\t\t# Calculate loss\n",
    "\t\t\tloss = loss_function(logits, decoder_output)\n",
    "\n",
    "\t\t# Returns the list of all layer variables / weights.\n",
    "\t\tvariables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables\n",
    "\n",
    "\t\t# Differentiate loss wrt variables\n",
    "\t\tgradients = tape.gradient(loss, variables)\n",
    "\n",
    "\t\t# grads_and_vars is a List of(gradient, variable) pairs.\n",
    "\t\tgrads_and_vars = zip(gradients,variables)\n",
    "\t\toptimizer.apply_gradients(grads_and_vars)\n",
    "\n",
    "\t\t# log metrics for TensorBoard \n",
    "\t\ttrain_loss(loss)\n",
    "\n",
    "\t\t# return batch loss\n",
    "\t\treturn loss\n",
    "\n",
    "\tnBest = None\n",
    "\tfor epoch in range(EPOCHS):\n",
    "\t\tstart = time.time()\n",
    "\n",
    "\t\tencoder_initial_cell_state = [tf.zeros((BATCH_SIZE, RNN_UNITS)), tf.zeros((BATCH_SIZE, RNN_UNITS))]\n",
    "\n",
    "\t\ttotal_loss = 0.0\n",
    "\n",
    "\t\tbatch_list = enumerate( dataset.take(steps_per_epoch) )\n",
    "\t\tfor (batch_index, batch_tuple) in batch_list :\n",
    "\t\t\t# input_batch = shape( BATCH_SIZE,MAX_SENT_LENGTH_SOURCE )\n",
    "\t\t\t# target_batch = shape( BATCH_SIZE,MAX_SENT_LENGTH_TARGET )\n",
    "\t\t\tinput_batch, target_batch = batch_tuple\n",
    "\n",
    "\t\t\tbatch_loss = train_step( input_batch, target_batch, encoder_initial_cell_state, train_loss )\n",
    "\t\t\ttotal_loss += batch_loss\n",
    "\n",
    "\t\t\t# print individual batch loss every N batches\n",
    "\t\t\tif (batch_index+1)%100 == 0:\n",
    "\t\t\t\tlogger.info( 'batch loss: {} epoch {} batch {}'.format(batch_loss.numpy(), epoch, batch_index+1))\n",
    "\n",
    "\t\t# write metric to summary for this epoch to disk\n",
    "\t\twith train_summary_writer.as_default():\n",
    "\t\t\t# record the loss at each step\n",
    "\t\t\ttf.summary.scalar( 'loss', train_loss.result(), step=epoch+1 )\n",
    "\n",
    "\t\tlogger.info( 'Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch) )\n",
    "\t\tlogger.info( 'Time taken for 1 epoch {} sec'.format(time.time() - start) )\n",
    "\n",
    "\t\t# checkpoint model if we have made an improvement\n",
    "\t\tif (nBest == None) or (total_loss < nBest) :\n",
    "\t\t\tlogger.info( 'checkpointing' )\n",
    "\t\t\tcheckpointManager.save()\n",
    "\t\t\tnBest = total_loss\n",
    "\n",
    "\t\t# reset TensorBoard metrics every epoch\n",
    "\t\ttrain_loss.reset_states()\n",
    "\n",
    "\t# the model will have been checkpoint saved into the model dir so we are good to go\n",
    "\tlogger.info( 'model training complete - model checkpoints into dir ' + os.path.abspath( model_dir ) )\n",
    "\n",
    "\t#\n",
    "\t# Execute model (translate)\n",
    "\t#\n",
    "\n",
    "\t# add 3 unseen test sentences and the associated gold translation for BLEU score later\n",
    "\t# note: sacrebleu needs the period at end of sents\n",
    "\tlist_validation = [ 'I like apples.' ]\n",
    "\tlist_gold = [ 'Ich mag Äpfel.' ]\n",
    "\n",
    "\t# save as bitext\n",
    "\twrite_handle = codecs.open( translated_source_file, 'w', 'utf-8', errors = 'strict' )\n",
    "\tfor str_sent in list_validation :\n",
    "\t\twrite_handle.write( str_sent + '\\n' )\n",
    "\twrite_handle.close()\n",
    "\n",
    "\t# load as we would for training\n",
    "\tlist_validation, _ , _, _ = preprocess_func( translated_source_file, None, logger )\n",
    "\t_, _, validation_tensor, _ = tokenize_func( list_validation, None, dict_alignment_matrix, logger, use_tokenizer_source = tokenizer_source, use_tokenizer_target = tokenizer_target )\n",
    "\n",
    "\t# load the last model checkpoint (best)\n",
    "\tstrCheckpointFile = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\tcheckpoint.restore( strCheckpointFile ).expect_partial()\n",
    "\n",
    "\t# calc max sent length\n",
    "\tmax_sent_length = 0\n",
    "\tfor nSentIndex in range(len(list_validation)) :\n",
    "\t\tvalidation_sent = list_validation[nSentIndex]\n",
    "\t\tmax_sent_length = max( max_sent_length, validation_sent.count(' ') + 1 )\n",
    "\n",
    "\t# prepare input batch\n",
    "\tinference_batch_size = len( list_validation )\n",
    "\tencoder_emb_inp = encoderNetwork.encoder_embedding( validation_tensor )\n",
    "\n",
    "\t# call encoder using input batch\n",
    "\tencoder_initial_cell_state = [ tf.zeros((inference_batch_size, RNN_UNITS)),tf.zeros((inference_batch_size, RNN_UNITS)) ]\n",
    "\ta, a_tx, c_tx = encoderNetwork.encoder_rnnlayer( encoder_emb_inp, initial_state = encoder_initial_cell_state, training = False )\n",
    "\tencoder_state=[a_tx, c_tx]\n",
    "\n",
    "\t# prepare decoder input batch using a sequence of <start> tokens to start processing seq\n",
    "\tdecoder_input = tf.expand_dims( [ tokenizer_target.word_index['<start>'] ] * inference_batch_size,1 )\n",
    "\tdecoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
    "\n",
    "\t# create decoder\n",
    "\tdecoder_instance, decoder_initial_state = decode_func( decoderNetwork, a, inference_batch_size, encoder_state )\n",
    "\tdecoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0]\n",
    "\n",
    "\t(first_finished, first_inputs, first_state) = decoder_instance.initialize(\n",
    "\t\tdecoder_embedding_matrix,\n",
    "\t\tstart_tokens = tf.fill( [inference_batch_size],tokenizer_target.word_index['<start>'] ),\n",
    "\t\tend_token = tokenizer_target.word_index['<end>'],\n",
    "\t\tinitial_state = decoder_initial_state )\n",
    "\n",
    "\t# limit prediction to a maximum of 2 * source sent length, as a simple heuristic to limit problems when they occur in translation\n",
    "\tmaximum_iterations = max_sent_length * 2\n",
    "\tif maximum_iterations > MAX_SENT_LENGTH_TARGET :\n",
    "\t\tmaximum_iterations = MAX_SENT_LENGTH_TARGET\n",
    "\n",
    "\t# translate\n",
    "\tlist_translated = translate_func( first_inputs, first_state, inference_batch_size, maximum_iterations, decoder_instance, tokenizer_source, tokenizer_target )\n",
    "\n",
    "\t# add period to end for sacrebleu to know its a sentence\n",
    "\tfor nIndex in range(len(list_translated)) :\n",
    "\t\tif list_translated[nIndex].endswith('.') == False :\n",
    "\t\t\tlist_translated[nIndex] = list_translated[nIndex] + '.'\n",
    "    \n",
    "\tlogger.info( 'validation = ' + repr(list_validation) )\n",
    "\tlogger.info( 'gold = ' + repr(list_gold) )\n",
    "\n",
    "\t# post processing of list_translated (if any)\n",
    "\tif postprocess_func != None :\n",
    "\t\tbleu = sacrebleu.corpus_bleu( list_translated, [ list_gold ] )\n",
    "\t\tlogger.info( 'translation (before postprocessing) = ' + repr(list_translated) )\n",
    "\t\tlogger.info( 'BLEU score (before postprocessing) = ' + str(bleu.score) )        \n",
    "\t\tpostprocess_func( sent_list_source = list_validation, sent_list_translated = list_translated, dict_lookup = dict_lookup_dictionary )\n",
    "\n",
    "\t# calculate BLEU score for this corpus\n",
    "\tbleu = sacrebleu.corpus_bleu( list_translated, [ list_gold ] )\n",
    "\tlogger.info( 'translation = ' + repr(list_translated) )\n",
    "\tlogger.info( 'BLEU score = ' + str(bleu.score) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use at this stage a basic decoder function uses a tfa.seq2seq.BasicDecoder() and tfa.seq2seq.GreedyEmbeddingSampler() to do a simple argmax to choose the most likely next word at any position in the sequence. This function sets the decoder up ready for stepping on encoded sequences to make output token predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_decoder( decoderNetwork, a, inference_batch_size, encoder_state ) :\n",
    "\t# use a basic decoder with greedy sampling\n",
    "\tgreedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
    "\tdecoder_instance = tfa.seq2seq.BasicDecoder( cell = decoderNetwork.rnn_cell, sampler = greedy_sampler, output_layer=decoderNetwork.dense_layer )\n",
    "\tencoder_memory = a\n",
    "\n",
    "\t# setup attention layer\n",
    "\tdecoderNetwork.attention_mechanism.setup_memory( encoder_memory )\n",
    "\n",
    "\t# init decoder\n",
    "\tdecoder_initial_state = decoderNetwork.build_decoder_initial_state( batch_size = inference_batch_size, encoder_state=encoder_state, Dtype=tf.float32 )\n",
    "\n",
    "\treturn decoder_instance, decoder_initial_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The translate function takes the decoder and steps through the batch of source sequences and predicts the output sequence for each. Stepping is done at a batch level, and a sequence of token indexes are predicted based on the current state (i.e. step position and tokens that have come previously). The final translated words are looked up using the target tokenizer word index, terminating each sentence whenever an <end> token is predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_translate( first_inputs, first_state, inference_batch_size, maximum_iterations, decoder_instance, tokenizer_source, tokenizer_target ) :\n",
    "\t# loop on each seq position (i.e. token) and predict the next token (i.e. translation)\n",
    "\tinputs = first_inputs\n",
    "\tstate = first_state\n",
    "\tpredictions = np.empty( (inference_batch_size,0), dtype = np.int32 )\n",
    "\n",
    "\t# make token predictions for the input batch\n",
    "\tnToken = 0\n",
    "\twhile nToken < maximum_iterations :\n",
    "\t\t# make final predictions for the batch at this seq position\n",
    "\t\toutputs, next_state, next_inputs, finished = decoder_instance.step( nToken, inputs, state, training = False )\n",
    "\t\tinputs = next_inputs\n",
    "\t\tstate = next_state\n",
    "\t\toutputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
    "\t\tpredictions = np.append(predictions, outputs, axis = -1)\n",
    "\t\tnToken += 1\n",
    "\n",
    "\tvocab_source_size = len(tokenizer_source.word_index)+1\n",
    "\tvocab_target_size = len(tokenizer_target.word_index)+1\n",
    "\tlogger.info( 'tokenizer vocab size = ' + repr( (vocab_source_size,vocab_target_size) ) )\n",
    "\n",
    "\t# convert the predicted token index values to strings for final translation of the batch\n",
    "\tlist_translated = []\n",
    "\tfor i in range(len(predictions)):\n",
    "\t\t# get prediction for this sent index\n",
    "\t\tline = predictions[i,:]\n",
    "\n",
    "\t\t# get seq up to first <end> or <pad> token\n",
    "\t\tseq = list( itertools.takewhile( lambda index: index not in [ tokenizer_target.word_index['<end>'],tokenizer_target.word_index['<pad>'] ], line) )\n",
    "\n",
    "\t\t# lookup words\n",
    "\t\tlistTokens = []\n",
    "\t\tfor nToken in seq :\n",
    "\t\t\tif nToken != 0 :\n",
    "\t\t\t\tlistTokens.append( tokenizer_target.index_word[nToken] )\n",
    "\t\t\telse :\n",
    "\t\t\t\tlistTokens.append( '<unk>' )\n",
    "\t\tlist_translated.append( ' '.join( listTokens ) )\n",
    "\n",
    "\treturn list_translated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE Task 1**\n",
    "\n",
    "Execute code for task 1 using the function call below. The translations and BLEU score will be very bad unless you run with a lot of sentences, so just check it generates a translation for now.\n",
    "\n",
    "Whilst the model is training take some time to step through the code used, including the functions in the library file lab_seq2seq_nmt_lib, to begin to build your understanding of what each are doing. This will take time and you should read after the lab the linked papers where more detailed descriptions of the models used are explained.\n",
    "\n",
    "**END EXERCISE Task 1**\n",
    "\n",
    "The next tasks will ask you to change some of these functions to boost translation performance over this basic setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exec_task( preprocess_func = task1_preprocess, tokenize_func = task1_tokenize, decode_func = task1_decoder, translate_func = task1_translate, postprocess_func = None )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 - use MOSES normalization and tokenization to cleanup the bitext\n",
    "\n",
    "Further reading:\n",
    "    [mosestokenizer](https://pypi.org/project/mosestokenizer/)\n",
    "    [MOSES statistical machine translation system](http://www.statmt.org/moses/)\n",
    "    \n",
    "The regex-based approach in task1_preprocess() is a crude language independant way to clean and tokenize text. We normalized character sets, split tokens on whitespace, converted everything to lowercase and removed punctuation, non-printable characters and numbers. This is losing a lot of valuable information we could train on.\n",
    "\n",
    "The statistical machine translation community have developed over many years a set of perl scripts that do a much more sophisticated job and can handle gramatical structures in many languages. These are shipped with the MOSES toolkit. We can use these scripts via python libraries such as [mosestokenizer](https://pypi.org/project/mosestokenizer/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_preprocess( bitext_source_file, bitext_target_file, logger ) :\n",
    "\t# load bitext using moses perl scripts (tokenizer and normalization)\n",
    "\tlist_source_sents = lab_seq2seq_nmt_lib.load_corpus( filename = bitext_source_file, logger = logger, normalize_func = lab_seq2seq_nmt_lib.normalize_sent_moses, word_tokenizer = mosestokenizer.MosesTokenizer( 'en' ), punc_normalization = mosestokenizer.MosesPunctuationNormalizer('en') )\n",
    "\n",
    "\t# translate mode?\n",
    "\tif bitext_target_file == None:\n",
    "\t\treturn list_source_sents, None, None, None\n",
    "\n",
    "\tlist_target_sents = lab_seq2seq_nmt_lib.load_corpus( filename = bitext_target_file, logger = logger, normalize_func = lab_seq2seq_nmt_lib.normalize_sent_moses, word_tokenizer = mosestokenizer.MosesTokenizer( 'de' ), punc_normalization = mosestokenizer.MosesPunctuationNormalizer('de') )\n",
    "\tif len(list_source_sents) != len(list_target_sents) :\n",
    "\t\traise Exception( 'bitext size mismatch' )\n",
    "\n",
    "\t# make bitext\n",
    "\tlist_bitext = []\n",
    "\tfor nSentIndex in range(len(list_source_sents)) :\n",
    "\t\tlist_bitext.append( ( list_source_sents[nSentIndex], list_target_sents[nSentIndex] ) )\n",
    "\t\n",
    "\tlist_source_sents = []\n",
    "\tlist_target_sents = []\n",
    "\tfor nSentIndex in range(len(list_bitext)) :\n",
    "\t\tlist_source_sents.append( list_bitext[nSentIndex][0] )\n",
    "\t\tlist_target_sents.append( list_bitext[nSentIndex][1] )\n",
    "\n",
    "\treturn list_source_sents, list_target_sents, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE Task 2**\n",
    "\n",
    "Add the below logger statements to exec_task() and run it. Look at the example source (EN) and target (DE) sentences printed.\n",
    "\n",
    "    logger.info( 'example source sent = ' + repr(list_source_sents[-1]) )\n",
    "    logger.info( 'example target sent = ' + repr(list_target_sents[-1]) )\n",
    "\n",
    "Now execute using task2_preprocess() as below to run task 2 (moses preprocessing). Look and understand the difference between the preprocessed sentences using regex and moses. Print a few more sentences, especially those with punctuation, symbolic characters and quotations.\n",
    "\n",
    "    Task 1 (regex)\n",
    "    \n",
    "    example source sent = 'doubtless there exists in this world precisely the right woman for any given man to marry and vice versa but when you consider that a human being has the opportunity of being acquainted with only a few hundred people and out of the few hundred that there are but a dozen or less whom he knows intimately and out of the dozen one or two friends at most it will easily be seen when we remember the number of millions who inhabit this world that probably since the earth was created the right man has never yet met the right woman'\n",
    "\n",
    "    example target sent = 'ohne zweifel findet sich auf dieser welt zu jedem mann genau die richtige ehefrau und umgekehrt wenn man jedoch in betracht zieht dass ein mensch nur gelegenheit hat mit ein paar hundert anderen bekannt zu sein von denen ihm nur ein dutzend oder weniger nahesteht darunter hochstens ein oder zwei freunde dann erahnt man eingedenk der millionen einwohner dieser weltleicht dass seit erschaffung ebenderselben wohl noch nie der richtige mann der richtigen frau begegnet ist'\n",
    "\n",
    "    Task 2 (moses)\n",
    "\n",
    "    example source sent = 'Doubtless there exists in this world precisely the right woman for any given man to marry and vice versa ; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people , and out of the few hundred that there are but a dozen or less whom he knows intimately , and out of the dozen , one or two friends at most , it will easily be seen , when we remember the number of millions who inhabit this world , that probably , since the earth was created , the right man has never yet met the right woman'\n",
    "\n",
    "    example target sent = 'Ohne Zweifel findet sich auf dieser Welt zu jedem Mann genau die richtige Ehefrau und umgekehrt ; wenn man jedoch in Betracht zieht , dass ein Mensch nur Gelegenheit hat , mit ein paar hundert anderen bekannt zu sein , von denen ihm nur ein Dutzend oder weniger nahesteht , darunter höchstens ein oder zwei Freunde , dann erahnt man eingedenk der Millionen Einwohner dieser Welt leicht , dass seit Erschaffung ebenderselben wohl noch nie der richtige Mann der richtigen Frau begegnet ist'\n",
    "\n",
    "**END EXERCISE Task 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_task( preprocess_func = task2_preprocess, tokenize_func = task1_tokenize, decode_func = task1_decoder, translate_func = task1_translate, postprocess_func = None )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - reversing the input sequence for better performance\n",
    "\n",
    "Further reading:\n",
    "    [Luong NMT paper](https://www.aclweb.org/anthology/D15-1166/)\n",
    "\n",
    "It has been shown that reversing the source input sequence (i.e. EN sentence words) can deliver better LSTM memory utilization and results in better translations over longer sentences.\n",
    "\n",
    "**EXERCISE Task 3**\n",
    "\n",
    "Implement your own version of task1_tokenize() function that reverses the input sentence sequence. For example a source sentence ```'<start> hello there <end>'``` should become a tensor with word indexes for ```'<end> there hello <start> <pad> <pad> ...'```.\n",
    "\n",
    "You can reuse lab_seq2seq_nmt_lib.train_tokenizer() but you should not use lab_seq2seq_nmt_lib.apply_tokenization() for this exercise.\n",
    "\n",
    "Confirm your new function works by running exec_task() and providing it with your new functions handle. Log the source and target tensors that are generated to confirm the word indexes of the source tensor are reversed correctly.\n",
    "\n",
    "A solution is below in task3_tokenize() for reference.\n",
    "\n",
    "**END EXERCISE Task 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task3_tokenize( list_source_sents, list_target_sents, dict_alignment_matrix, logger, use_tokenizer_source = None, use_tokenizer_target = None ) :\n",
    "\t# train tokenizers (or use existing one if we are in translate phase)\n",
    "\tif use_tokenizer_source == None :\n",
    "\t\ttokenizer_source = lab_seq2seq_nmt_lib.train_tokenizer( list_lines = list_source_sents, logger = logger )\n",
    "\telse :\n",
    "\t\ttokenizer_source = use_tokenizer_source\n",
    "\tif use_tokenizer_target == None :\n",
    "\t\ttokenizer_target = lab_seq2seq_nmt_lib.train_tokenizer( list_lines = list_target_sents, logger = logger )\n",
    "\telse :\n",
    "\t\ttokenizer_target = use_tokenizer_target\n",
    "\n",
    "\t# shuffle corpus and truncate if needed\n",
    "\tif list_target_sents != None :\n",
    "\t\tbitext = list( zip( list_source_sents, list_target_sents ) )\n",
    "\t\trandom.shuffle( bitext )\n",
    "\t\tif DEBUG_TRUNC_SENTS == None :\n",
    "\t\t\tnMax = len(bitext)\n",
    "\t\telse :        \n",
    "\t\t\tnMax = DEBUG_TRUNC_SENTS\n",
    "\t\tlist_source_sents = []\n",
    "\t\tlist_target_sents = []\n",
    "\t\tfor nSentIndex in range(nMax) :\n",
    "\t\t\tlist_source_sents.append( bitext[nSentIndex][0] )\n",
    "\t\t\tlist_target_sents.append( bitext[nSentIndex][1] )\n",
    "\telse :\n",
    "\t\tif DEBUG_TRUNC_SENTS == None :\n",
    "\t\t\tnMax = len(list_source_sents)\n",
    "\t\telse :        \n",
    "\t\t\tnMax = DEBUG_TRUNC_SENTS\n",
    "\t\tlist_source_sents = list_source_sents[:nMax]\n",
    "\n",
    "\t# apply tokenizers (reverse the source sent)\n",
    "\tsource_tensor_train = lab_seq2seq_nmt_lib.apply_tokenization( list_sents = list_source_sents, tokenizer = tokenizer_source, max_sent_length = MAX_SENT_LENGTH_SOURCE, reverse_seq = True, logger = logger )\n",
    "\n",
    "\t# translate mode?\n",
    "\tif list_target_sents == None :\n",
    "\t\treturn tokenizer_source, tokenizer_target, source_tensor_train, None\n",
    "\n",
    "\ttarget_tensor_train = lab_seq2seq_nmt_lib.apply_tokenization( list_sents = list_target_sents, tokenizer = tokenizer_target, max_sent_length = MAX_SENT_LENGTH_TARGET, reverse_seq = False, logger = logger )\n",
    "\n",
    "\treturn tokenizer_source, tokenizer_target, source_tensor_train, target_tensor_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_task( preprocess_func = task2_preprocess, tokenize_func = task3_tokenize, decode_func = task1_decoder, translate_func = task1_translate, postprocess_func = None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4 - replace the basic decoder with a beam search decoder for better performance\n",
    "\n",
    "Further reading:\n",
    "    [BeamSearchDecoder](https://www.tensorflow.org/addons/api_docs/python/tfa/seq2seq/BeamSearchDecoder)\n",
    "    [Beam search tutorial](https://towardsdatascience.com/word-sequence-decoding-in-seq2seq-architectures-d102000344ad)\n",
    "\t[seq2seq tutorial using beam search decoder](https://github.com/dhirensk/ai/blob/master/English_to_French_seq2seq_tf_2_0_withAttention.ipynb)\n",
    "\n",
    "It has been shown that using a beam search decoder will provide better results than a basic argmax decoder. This is because in sequence to sequence learning the token predictions are based on the information available at the current position in the sequence. With an argmax approach once a token is chosen that choice is locked in and that decision cannot be revisited later if further down the sequence it no longer makes sense. A beam search allows several alternative sequence fragments to be considered at once, with the ability to drop sequence options that become unlikely as decoding progresses.\n",
    "\n",
    "**EXERCISE Task 4**\n",
    "\n",
    "Read the links in the above further reading section and try to write your own decoder() and translate() functions to use the tfa.seq2seq.BeamSearchDecoder class.\n",
    "\n",
    "Confirm the decoder is working correctly by logging the partial prediction sets as the decoder steps through the batch of inputs. Use a beam width of 5 to make it easy to log. You can lookup predicted token indexes in the word_index of the tokenizer instance to log the words as well as the word indices.\n",
    "\n",
    "Observe how the 5 possible sequence fragments are updated each step and the less likely ones removed.\n",
    "\n",
    "A solution is below in task4_decoder() and task4_translate() for reference.\n",
    "\n",
    "**END EXERCISE Task 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task4_decoder( decoderNetwork, a, inference_batch_size, encoder_state ) :\n",
    "\t# beam length normalizaion (avoids short sequences being favoured because probabilities are multiplied so long seq means lower prob)\n",
    "\t# length normalization - see https://opennmt.net/OpenNMT/translation/beam_search/\n",
    "\t# length_penalty = (5 + length_seq)**penalty / 6**penalty\n",
    "\t# scores = logprob / length_penalty\n",
    "\tdecoder_instance = tfa.seq2seq.BeamSearchDecoder( cell = decoderNetwork.rnn_cell, beam_width = BEAM_WIDTH, output_layer=decoderNetwork.dense_layer, length_penalty_weight = BEAM_LENGTH_NORM_WEIGHT )\n",
    "\tencoder_memory = tfa.seq2seq.tile_batch( a, multiplier=BEAM_WIDTH )\n",
    "\n",
    "\t# setup attention layer\n",
    "\tdecoderNetwork.attention_mechanism.setup_memory( encoder_memory )\n",
    "\n",
    "\t# init decoder\n",
    "\tdecoder_initial_state = decoderNetwork.rnn_cell.get_initial_state( batch_size = inference_batch_size * BEAM_WIDTH, dtype = tf.float32 )\n",
    "\ttiled_encoder_final_state = tfa.seq2seq.tile_batch( encoder_state, multiplier=BEAM_WIDTH )\n",
    "\tdecoder_initial_state = decoder_initial_state.clone( cell_state=tiled_encoder_final_state )\n",
    "\n",
    "\treturn decoder_instance, decoder_initial_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task4_translate( first_inputs, first_state, inference_batch_size, maximum_iterations, decoder_instance, tokenizer_source, tokenizer_target ) :\n",
    "\tinputs = first_inputs\n",
    "\tstate = first_state\n",
    "\tpredictions = np.empty((inference_batch_size, BEAM_WIDTH,0), dtype = np.int32)\n",
    "\tbeam_scores =  np.empty((inference_batch_size, BEAM_WIDTH,0), dtype = np.float32)\n",
    "\tfinished = False\n",
    "\n",
    "\tnToken = 0\n",
    "\twhile (nToken < maximum_iterations) and (finished == False) :\n",
    "\t\tbeam_search_outputs, next_state, next_inputs, finished_outputs = decoder_instance.step( nToken, inputs, state, training = False )\n",
    "\t\tinputs = next_inputs\n",
    "\t\tstate = next_state\n",
    "\t\toutputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)\n",
    "\t\tscores = np.expand_dims(beam_search_outputs.scores,axis = -1)\n",
    "\t\tpredictions = np.append(predictions, outputs, axis = -1)\n",
    "\t\tbeam_scores = np.append(beam_scores, scores, axis = -1)\n",
    "\n",
    "\t\t# check if all beams have finished (numpy array of bool, one for each beam)\n",
    "\t\tif finished_outputs.numpy().all() == True :\n",
    "\t\t\tfinished = True\n",
    "\t\telse :\n",
    "\t\t\tfinished = False\n",
    "\n",
    "\t\tnToken += 1\n",
    "\n",
    "\tlist_translated = []\n",
    "\tfor i in range(len(predictions)):\n",
    "\t\toutput_beams_per_sample = predictions[i,:,:]\n",
    "\t\tscore_beams_per_sample = beam_scores[i,:,:]\n",
    "\t\tbeam_result = zip(output_beams_per_sample,score_beams_per_sample)\n",
    "\n",
    "\t\tlistTopK = []\n",
    "\t\tfor beam, score in beam_result :\n",
    "\t\t\t# get seq up to first <end> or <pad> token\n",
    "\t\t\t# note: the decoder should already have stopped when it reached the <end> so this is really about removing erronenous <pad> sequences\n",
    "\t\t\tseq = list( itertools.takewhile( lambda index: index not in [tokenizer_target.word_index['<end>'],tokenizer_target.word_index['<pad>']], beam) )\n",
    "\n",
    "\t\t\tlistTokens = []\n",
    "\t\t\tfor nToken in seq :\n",
    "\t\t\t\tif nToken != 0 :\n",
    "\t\t\t\t\tlistTokens.append(  tokenizer_target.index_word[nToken] )\n",
    "\t\t\t\telse :\n",
    "\t\t\t\t\tlistTokens.append( '<unk>' )\n",
    "\t\t\tstrSent = ' '.join( listTokens )\n",
    "\n",
    "\t\t\tscore_indexes = np.arange(len(seq))\n",
    "\t\t\tbeam_score = score[score_indexes].sum()\n",
    "\n",
    "\t\t\tlistTopK.append( ( strSent, beam_score ) )\n",
    "\t\t\n",
    "\t\tlistTopK = sorted( listTopK, key=lambda entry: entry[1], reverse=True )\n",
    "\n",
    "\t\t# take best result\n",
    "\t\tlist_translated.append( listTopK[0][0] )\n",
    "\n",
    "\treturn list_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_task( preprocess_func = task2_preprocess, tokenize_func = task3_tokenize, decode_func = task4_decoder, translate_func = task4_translate, postprocess_func = None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "### Task 5 - use statistical phrase alignment to create a rare word translation lookup dictionary\n",
    "\n",
    "Further reading:\n",
    "    [fast_align](https://github.com/clab/fast_align)\n",
    "    [Dyer 2013 paper](https://www.aclweb.org/anthology/N13-1073/)\n",
    "\n",
    "One of the major problems with NMT is that the vocabulary size has to be limited otherwise the models will not fit into GPU memory. In 2020 vocabulary sizes of 50k were typically used as an upper limit for seq2seq models. On a realistic bitext such as [WikiMatrix](https://arxiv.org/abs/1907.05791) there are millions of sentences (e.g. EN to RU is 1.6 million sentences) with 100,000's of words (e.g. EN to RU has TODO words). This means only the more frequent words that make it to the 50k vocabulary will be trained on. Any other work will be labelled as ```<unk>```. Whilst we can still get good BLEU scores realistic translation systems need to be able to handle rare words and untranslatable symbols in an efficient way.\n",
    "\n",
    "In this task we will use the well established [fast_align](https://github.com/clab/fast_align) statistical phrase alignment tool to generate an alignment matrix for our bitext training corpus. We will then use this to make a lookup dictionary containing DE words which are often found to be aligned to EN words. This lookup dictionary will allow us in the next task to translate ```<unk>``` tokens by taking the predicted aligned source word and looking its translation up in the lookup dictionary we compute in this task.\n",
    "\n",
    "**EXERCISE Task 5**\n",
    "\n",
    "Read the readme of [fast_align](https://github.com/clab/fast_align), install and compile it.\n",
    "\n",
    "Write a new preprocessing function that will save the moses tokenized bitext file in a format suitable for fast_align. Run it and generate the fast_align coprus file.\n",
    "\n",
    "Run fast_align and pipe the output to an alignment matrix file.\n",
    "\n",
    "    ./fast_align -i bitext_for_fast_align.txt -d -o -v > bitext_nmt_alignment_matrix.txt\n",
    "\n",
    "Example alignment file format:\n",
    "\n",
    "    0-0 2-1 16-2 19-3 3-4 ...\n",
    "    0-0 1-1 4-2 5-3 2-4 6-5 ...\n",
    "    0-0 4-1 2-2 1-3 12-4 ...\n",
    "\n",
    "Update your new preprocessing function so it can read in the alignment matrix file as a python dict. e.g. ```{ sent_index : { source_token_index : [ target_token_index1, target_token_index2 ... ] }```\n",
    "\n",
    "Use this alignment matrix dict to generate an index of source word alignments to target words, including a frequency count of how many times the alignment occurs in the corpus. e.g. ```{ source_token : { target_token : freq_count } }```\n",
    "\n",
    "Finally make a translation lookup index for each source_token where there is an aligned target_token that has an occurance frequency of more than 100. If there are multiple aligned tokens to choose from pick the most frequent one. e.g. ```{ source_token : target_token }```\n",
    "\n",
    "Save the translation lookup dict to disk and check it looks OK. It should have good translations from EN words to DE words.\n",
    "\n",
    "Example translation lookup dict:\n",
    "\n",
    "    {\n",
    "    \"Go\": \"Geh\",\n",
    "    \"Stop\": \"auf\",\n",
    "    \"on\": \"auf\",\n",
    "    \"I\": \"Ich\",\n",
    "    \"ran\": \"rannte\",\n",
    "    \"see\": \"sehen\",\n",
    "    \"try\": \"versuchen\",\n",
    "    \"won\": \"wird\",\n",
    "    \"me\": \"mir\",\n",
    "    \"it\": \"es\",\n",
    "    ...\n",
    "    }\n",
    "\n",
    "\n",
    "A solution is below in task5_preprocess() for reference.\n",
    "\n",
    "**END EXERCISE Task 5**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task5_preprocess( bitext_source_file, bitext_target_file, logger ) :\n",
    "\tfast_align_output = '../corpus/bitext_nmt_alignment_matrix.txt'\n",
    "\tbitext_file = './model/bitext_for_fast_align.txt'\n",
    "\n",
    "\t# load bitext using moses perl scripts (tokenizer and normalization)\n",
    "\tlist_source_sents = lab_seq2seq_nmt_lib.load_corpus( filename = bitext_source_file, logger = logger, normalize_func = lab_seq2seq_nmt_lib.normalize_sent_moses, word_tokenizer = mosestokenizer.MosesTokenizer( 'en' ), punc_normalization = mosestokenizer.MosesPunctuationNormalizer('en') )\n",
    "\n",
    "\t# translate mode?\n",
    "\tif bitext_target_file == None:\n",
    "\t\treturn list_source_sents, None, None, None\n",
    "\n",
    "\tlist_target_sents = lab_seq2seq_nmt_lib.load_corpus( filename = bitext_target_file, logger = logger, normalize_func = lab_seq2seq_nmt_lib.normalize_sent_moses, word_tokenizer = mosestokenizer.MosesTokenizer( 'de' ), punc_normalization = mosestokenizer.MosesPunctuationNormalizer('de') )\n",
    "\tif len(list_source_sents) != len(list_target_sents) :\n",
    "\t\traise Exception( 'bitext size mismatch' )\n",
    "\n",
    "\t# make bitext\n",
    "\tlist_bitext = []\n",
    "\tfor nSentIndex in range(len(list_source_sents)) :\n",
    "\t\tlist_bitext.append( ( list_source_sents[nSentIndex], list_target_sents[nSentIndex] ) )\n",
    "\n",
    "\t# serialize full bitext using fast_align format so we can create the alignment matrix needed next\n",
    "\t# run this code once to create bitext, then run fast_align to make file 'bitext_nmt_alignment_matrix.txt', then run this code again to process alignment file\n",
    "\t# e.g. ./fast_align -i bitext_for_fast_align.txt -d -o -v > bitext_nmt_alignment_matrix.txt\n",
    "\tlogger.info( 'saving bitext to file = ' + bitext_file )\n",
    "\twrite_handle = codecs.open( bitext_file, 'w', 'utf-8', errors = 'strict' )\n",
    "\tfor list_sent_pair in list_bitext :\n",
    "\t\twrite_handle.write( list_sent_pair[0] + ' ||| ' + list_sent_pair[1] + '\\n' )\n",
    "\twrite_handle.close()\n",
    "\n",
    "\t# load fast_align alignment matrix file from disk\n",
    "\tif os.path.exists( fast_align_output ) == False :\n",
    "\t\traise Exception('fast_align output file missing (run fast_align on the file ' + bitext_file + ' to make required file ' + fast_align_output)\n",
    "\tdict_alignment_matrix = lab_seq2seq_nmt_lib.read_alignment_matrix( file = fast_align_output, logger = logger )\n",
    "\n",
    "\t# create a rare word lookup dict from source to target statistically frequent word alignments\n",
    "\tdict_lookup_dictionary = lab_seq2seq_nmt_lib.create_lookup_dict( align_matrix = dict_alignment_matrix, bitext = list_bitext, freq_threshold = 100, logger = logger )\n",
    "\n",
    "\tlist_source_sents = []\n",
    "\tlist_target_sents = []\n",
    "\tfor nSentIndex in range(len(list_bitext)) :\n",
    "\t\tlist_source_sents.append( list_bitext[nSentIndex][0] )\n",
    "\t\tlist_target_sents.append( list_bitext[nSentIndex][1] )\n",
    "\n",
    "\treturn list_source_sents, list_target_sents, dict_alignment_matrix, dict_lookup_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exec_task( preprocess_func = task5_preprocess, tokenize_func = task3_tokenize, decode_func = task4_decoder, translate_func = task4_translate, postprocess_func = None )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6 - train model to predict positionally aware unk tokens and translate them with a lookup dictionary\n",
    "\n",
    "Further reading:\n",
    "    [Luong 2015 paper](https://www.aclweb.org/anthology/P15-1002/)\n",
    "\n",
    "An effective approach [Luong 2015 paper](https://www.aclweb.org/anthology/P15-1002/) for handling rare words has been shown to be training the NMT model so it learns to predict target ```<unk>``` tokens with an alignment index to the source sequence. For example, ```<unkpos-2>``` means ```<unk> aligned to source token index 2 positions to left of this target <unk> token```. Then as a post-processing stage all ```<unkposN>``` tokens can be replaced with either the lookup translated value or a default of simply the source token. The later is useful for words with no real translation, such as proper names or symbolic tokens.\n",
    "\n",
    "In this task we will use the last task's [fast_align](https://github.com/clab/fast_align) statistical phrase alignment matrix  and lookup dictionary to implement a rare word strategy similar to Luong.\n",
    "\n",
    "**EXERCISE Task 6**\n",
    "\n",
    "Read the links in the above further reading section to understand how to implement the ```<unkposN>``` strategy.\n",
    "\n",
    "Try to write your own tokenize() function to limit the vocabulary size of the trained tokenizers (source and target) to something small like 1000 words. Then replace any source token not in this limited vocabulary with an ```<unk>``` token. Replace any target tokens not in the limited vocabulary with an ```<unkposN> where N is in range[-7 ... 7]```. Use the alignment matrix to lookup what the relative position value should be of the source token aligned to the unk target token. If the relative alignment position is > 7 tokens, the alignment points to a token outside source seq or no alignment is specified for that token then default to ```<unkpos0>```. This will provide the NMT model with a training set that will teach it to predict the likely source alignment of unk tokens.\n",
    "\n",
    "Write your own post processing function to replace target ```<unkposN>``` tokens with a value from the look dictionary, or if no translatino available simply copy the aligned source token. This function should have the arguments below and will replace token values directly within the reference list sent_list_translated.\n",
    "\n",
    "\t\tpostprocess_func( sent_list_source, sent_list_translated, dict_lookup )\n",
    "\n",
    "A solution is below in task6_tokenize() and lab_seq2seq_nmt_lib.lookup_unkposN() for reference.\n",
    "\n",
    "**END EXERCISE Task 6**    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task6_tokenize( list_source_sents, list_target_sents, dict_alignment_matrix, logger, use_tokenizer_source = None, use_tokenizer_target = None ) :\n",
    "\t# train tokenizers (or use existing one if we are in translate phase)\n",
    "\tif use_tokenizer_source == None :\n",
    "\t\ttokenizer_source = lab_seq2seq_nmt_lib.train_tokenizer_top_N( list_lines = list_source_sents, top_N = MAX_VOCAB_SIZE, is_source = True, logger = logger )\n",
    "\telse :\n",
    "\t\ttokenizer_source = use_tokenizer_source\n",
    "\tif use_tokenizer_target == None :\n",
    "\t\ttokenizer_target = lab_seq2seq_nmt_lib.train_tokenizer_top_N( list_lines = list_target_sents, top_N = MAX_VOCAB_SIZE, is_source = False, logger = logger )\n",
    "\telse :\n",
    "\t\ttokenizer_target = use_tokenizer_target\n",
    "\n",
    "\t# replace target rare words with an <unkposN> token so they can be looked up (using the aligned source token) in the decoding stage from the lookup dict.\n",
    "\t# source rare words will be replaced with <unk>\n",
    "\tlab_seq2seq_nmt_lib.unkpos_replacement( align_matrix = dict_alignment_matrix, list_source_sents = list_source_sents, list_target_sents = list_target_sents, source_tokenizer = tokenizer_source, target_tokenizer = tokenizer_target, logger = logger )\n",
    "\n",
    "\t# shuffle corpus and truncate if needed\n",
    "\tif list_target_sents != None :\n",
    "\t\tbitext = list( zip( list_source_sents, list_target_sents ) )\n",
    "\t\trandom.shuffle( bitext )\n",
    "\t\tif DEBUG_TRUNC_SENTS == None :\n",
    "\t\t\tnMax = len(bitext)\n",
    "\t\telse :        \n",
    "\t\t\tnMax = DEBUG_TRUNC_SENTS\n",
    "\t\tlist_source_sents = []\n",
    "\t\tlist_target_sents = []\n",
    "\t\tfor nSentIndex in range(nMax) :\n",
    "\t\t\tlist_source_sents.append( bitext[nSentIndex][0] )\n",
    "\t\t\tlist_target_sents.append( bitext[nSentIndex][1] )\n",
    "\telse :\n",
    "\t\tif DEBUG_TRUNC_SENTS == None :\n",
    "\t\t\tnMax = len(list_source_sents)\n",
    "\t\telse :        \n",
    "\t\t\tnMax = DEBUG_TRUNC_SENTS\n",
    "\t\tlist_source_sents = list_source_sents[:nMax]\n",
    "        \n",
    "\t# apply tokenizers (reverse the source sent)\n",
    "\tsource_tensor_train = lab_seq2seq_nmt_lib.apply_tokenization( list_sents = list_source_sents, tokenizer = tokenizer_source, max_sent_length = MAX_SENT_LENGTH_SOURCE, reverse_seq = True, logger = logger )\n",
    "\n",
    "\t# translate mode?\n",
    "\tif list_target_sents == None :\n",
    "\t\treturn tokenizer_source, tokenizer_target, source_tensor_train, None\n",
    "\n",
    "\ttarget_tensor_train = lab_seq2seq_nmt_lib.apply_tokenization( list_sents = list_target_sents, tokenizer = tokenizer_target, max_sent_length = MAX_SENT_LENGTH_TARGET, reverse_seq = False, logger = logger )\n",
    "\n",
    "\treturn tokenizer_source, tokenizer_target, source_tensor_train, target_tensor_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exec_task( preprocess_func = task5_preprocess, tokenize_func = task6_tokenize, decode_func = task4_decoder, translate_func = task4_translate, postprocess_func = lab_seq2seq_nmt_lib.lookup_unkposN )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
