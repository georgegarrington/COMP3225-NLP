{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing COMP3225\n",
    "\n",
    "CRF Named Entity Recognition (NER) lab\n",
    "Stuart Middleton, 18/09/2020\n",
    "\n",
    "This lab will provide practical experience with named entity recongition (NER) software trained to label named entities (NE's) within English sentences using a Conditional Random Field (CRF) model. You will learn how to use the CRF model to label NE's and adjust features to deliver better performance. You will explore how changing the L1 regularization and using all possible transitions changes the learnt transition weights and thus the type of patterns learnt. Finally you will use a randomized hyperparameter search to find an optimal set of hyperparameters for your CRF NER model.\n",
    "\n",
    "# Part 1\n",
    "\n",
    "# Pre-requisites\n",
    "You will need python3. The code below will work OK on a CPU only machine. Increasing the number of training files and iterations will significantly improve the quality of NER performance if you are prepared to wait for the longer compute to complete.\n",
    "\n",
    "# Task 1 - Install pre-requistes and run the baseline NER\n",
    "\n",
    "Further reading: [Scikit Learn CRF model](https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#module-sklearn_crfsuite)\n",
    "\n",
    "Further reading: [CRF paper](https://repository.upenn.edu/cis_papers/159/) \n",
    "\n",
    "Further reading: Course Text - Speech and Language Processing >> Information Extraction >> Named Entity Recognition \n",
    "\n",
    "First install python3 and the pre-requisite libraries needed for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "python3 -m pip install numpy\n",
    "python3 -m pip install tensorflow-gpu\n",
    "python3 -m pip install sklearn\n",
    "python3 -m pip install sklearn_crfsuite\n",
    "python3 -m pip install eli5\n",
    "python3 -m pip install matplotlib\n",
    "python3 -m pip install notebook\n",
    "\n",
    "unzip package for lab\n",
    "jupyter notebook\n",
    "==> will open browser windows from localhost:8888\n",
    "==> load the lab .ipynb file\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new python code file for your work.\n",
    "\n",
    "Import this labs required python3 libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, codecs, json, math, time, warnings\n",
    "warnings.simplefilter( action='ignore', category=FutureWarning )\n",
    "\n",
    "import nltk, scipy, sklearn, sklearn_crfsuite, sklearn_crfsuite.metrics, eli5\n",
    "from sklearn.metrics import make_scorer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display    \n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import absl.logging\n",
    "formatter = logging.Formatter('[%(levelname)s|%(filename)s:%(lineno)s %(asctime)s] %(message)s')\n",
    "absl.logging.get_absl_handler().setFormatter(formatter)\n",
    "absl.logging._warn_preinit_stderr = False\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now define the hyperparameters used for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of CRF iterations to train for. Using 150 will provide much better results, but take a lot longer to compute.\n",
    "max_iter = 20\n",
    "\n",
    "# number of ontonotes training files to load. Using a value of None will load the entire dataset, taking the longest\n",
    "# to train but providing a much larger sentence corpus to train over and thus is able to learn a larger vocabulary.\n",
    "max_files = 50\n",
    "\n",
    "# set of NE label types to display in results. this is simply to limit the amount of logging that is perfoemed later\n",
    "# when displaying details such as state transitions and top N features per state.\n",
    "display_label_subset = [ 'B-DATE', 'I-DATE', 'B-GPE', 'I-GPE', 'B-PERSON', 'I-PERSON', 'O' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a function to load a parsed JSON formatted file with the ontonotes 5.0 dataset. The dataset is parsed and training and testset created, each a list of sentences constisting of lists of (token, POS_tag, NER_IOB_tag) tuples. IOB tagging is a scheme defining Begin, Inside, Outside tags for labels.\n",
    "\n",
    "For example \"I like New York in the spring\" might be tagged \"O O B-LOC I-LOC O O O\" for the named entity \"New York\".\n",
    "\n",
    "Ontonotes is an annotated dataset created from various genres of text (news, conversational telephone speech, weblogs, usenet newsgroups, broadcast, talk shows) in three languages (English, Chinese, and Arabic). Annotations include structural information (syntax and predicate argument structure) and shallow semantics (word sense linked to an ontology and coreference). We will only use a parsed version here with the words, POS tags and NER tags.\n",
    "\n",
    "Further reading: [Ontonotes 5.0 dataset](https://catalog.ldc.upenn.edu/LDC2013T19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset( max_files = None ) :\n",
    "\tdataset_file = '../corpus/ontonotes_parsed.json'\n",
    "    \n",
    "\t# load parsed ontonotes dataset\n",
    "\treadHandle = codecs.open( dataset_file, 'r', 'utf-8', errors = 'replace' )\n",
    "\tstr_json = readHandle.read()\n",
    "\treadHandle.close()\n",
    "\tdict_ontonotes = json.loads( str_json )\n",
    "\n",
    "\t# make a training and test split\n",
    "\tlist_files = list( dict_ontonotes.keys() )\n",
    "\tif len(list_files) > max_files :\n",
    "\t\tlist_files = list_files[ :max_files ]\n",
    "\tnSplit = math.floor( len(list_files)*0.9 )\n",
    "\tlist_train_files = list_files[ : nSplit ]\n",
    "\tlist_test_files = list_files[ nSplit : ]\n",
    "\n",
    "\t# sent = (tokens, pos, IOB_label)\n",
    "\tlist_train = []\n",
    "\tfor str_file in list_train_files :\n",
    "\t\tfor str_sent_index in dict_ontonotes[str_file] :\n",
    "\t\t\t# ignore sents with non-PENN POS tags\n",
    "\t\t\tif 'XX' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif 'VERB' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tlist_entry = []\n",
    "\n",
    "\t\t\t# compute IOB tags for named entities (if any)\n",
    "\t\t\tne_type_last = None\n",
    "\t\t\tfor nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])) :\n",
    "\t\t\t\tstrToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "\t\t\t\tstrPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "\t\t\t\tne_type = None\n",
    "\t\t\t\tif 'ne' in dict_ontonotes[str_file][str_sent_index] :\n",
    "\t\t\t\t\tdict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "\t\t\t\t\tif not 'parse_error' in dict_ne :\n",
    "\t\t\t\t\t\tfor str_NEIndex in dict_ne :\n",
    "\t\t\t\t\t\t\tif nTokenIndex in dict_ne[str_NEIndex]['tokens'] :\n",
    "\t\t\t\t\t\t\t\tne_type = dict_ne[str_NEIndex]['type']\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif ne_type != None :\n",
    "\t\t\t\t\tif ne_type == ne_type_last :\n",
    "\t\t\t\t\t\tstrIOB = 'I-' + ne_type\n",
    "\t\t\t\t\telse :\n",
    "\t\t\t\t\t\tstrIOB = 'B-' + ne_type\n",
    "\t\t\t\telse :\n",
    "\t\t\t\t\tstrIOB = 'O'\n",
    "\t\t\t\tne_type_last = ne_type\n",
    "\n",
    "\t\t\t\tlist_entry.append( ( strToken, strPOS, strIOB ) )\n",
    "\n",
    "\t\t\tlist_train.append( list_entry )\n",
    "\n",
    "\tlist_test = []\n",
    "\tfor str_file in list_test_files :\n",
    "\t\tfor str_sent_index in dict_ontonotes[str_file] :\n",
    "\t\t\t# ignore sents with non-PENN POS tags\n",
    "\t\t\tif 'XX' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif 'VERB' in dict_ontonotes[str_file][str_sent_index]['pos'] :\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tlist_entry = []\n",
    "\n",
    "\t\t\t# compute IOB tags for named entities (if any)\n",
    "\t\t\tne_type_last = None\n",
    "\t\t\tfor nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])) :\n",
    "\t\t\t\tstrToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "                strPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "\t\t\t\tne_type = None\n",
    "\t\t\t\tif 'ne' in dict_ontonotes[str_file][str_sent_index] :\n",
    "\t\t\t\t\tdict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "\t\t\t\t\tif not 'parse_error' in dict_ne :\n",
    "\t\t\t\t\t\tfor str_NEIndex in dict_ne :\n",
    "\t\t\t\t\t\t\tif nTokenIndex in dict_ne[str_NEIndex]['tokens'] :\n",
    "\t\t\t\t\t\t\t\tne_type = dict_ne[str_NEIndex]['type']\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif ne_type != None :\n",
    "\t\t\t\t\tif ne_type == ne_type_last :\n",
    "\t\t\t\t\t\tstrIOB = 'I-' + ne_type\n",
    "\t\t\t\t\telse :\n",
    "\t\t\t\t\t\tstrIOB = 'B-' + ne_type\n",
    "\t\t\t\telse :\n",
    "\t\t\t\t\tstrIOB = 'O'\n",
    "\t\t\t\tne_type_last = ne_type\n",
    "\n",
    "\t\t\t\tlist_entry.append( ( strToken, strPOS, strIOB ) )\n",
    "\n",
    "\t\t\tlist_test.append( list_entry )\n",
    "\n",
    "\treturn list_train, list_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we defined some helper functions to generate feature sets for each sentence, which the CRF model will use to train with. The word2features_func() function provided as an argument does all the work, and we will define several versions of it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent, word2features_func = None):\n",
    "\treturn [word2features_func(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "\treturn [label for token, postag, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "\treturn [token for token, postag, label in sent]\n",
    "\n",
    "def print_F1_scores( micro_F1 ) :\n",
    "\tfor label in micro_F1 :\n",
    "\t\tlogger.info( \"%-15s -> f1 %0.2f ; prec %0.2f ; recall %0.2f\" % ( label, micro_F1[label]['f1-score'], micro_F1[label]['precision'], micro_F1[label]['recall'] ) )\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "\tfor (label_from, label_to), weight in trans_features:\n",
    "\t\tlogger.info( \"%-15s -> %-15s %0.6f\" % (label_from, label_to, weight) )\n",
    "\n",
    "def print_state_features(state_features):\n",
    "\tfor (attr, label), weight in state_features:\n",
    "\t\tlogger.info( \"%0.6f %-15s %s\" % (weight, label, attr) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a function to train the CRF model on the ontonotes corpus, and then run the trained model to compute a macro F1 score on the testset.\n",
    "\n",
    "First we load the corpus and then use the helper functions to generate lists of features for every token in the dataset. We curate a set of NE labels and remove the 'O' label. This is done because the majority of words are not named entities, and so 'O' tags severly imbalance the dataset. We want a CRF model that has a good F1 score across non-O tags, and if we left the 'O' tag in the F1 score would be dominated by the 'O' tag performance only.\n",
    "\n",
    "Next we train the CRF model and log the weights it has learnt. We then run the trained model on the testset and report the macro F1 score results. We also log information about the state transitions and position/negative weighted features as this can reveal what has really been learnt by the CRF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_task( max_files = 10, max_iter = 20, display_label_subset = [], word2features_func = None, train_crf_model_func = None ) :\n",
    "\tlogger.info( 'max iterations = ' + repr(max_iter) )\n",
    "\tlogger.info( 'word2features_func = ' + word2features_func.__name__  )\n",
    "\tlogger.info( 'train_crf_model_func = ' + train_crf_model_func.__name__  )\n",
    "\n",
    "\t# make a dataset from english NE labelled ontonotes sents\n",
    "\ttrain_sents, test_sents = create_dataset( max_files = max_files )\n",
    "\tlogger.info( '# training sents = ' + str(len(train_sents)) )\n",
    "\tlogger.info( '# test sents = ' + str(len(test_sents)) )\n",
    "\n",
    "\t# print example sent (1st sent)\n",
    "\tlogger.info( '' )\n",
    "\tlogger.info( 'Example training sent annotated with IOB tags  = ' + repr(train_sents[0]) )\n",
    "\n",
    "\t# create feature vectors for every sent\n",
    "\tX_train = [sent2features(s, word2features_func = word2features_func) for s in train_sents]\n",
    "\tY_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "\tX_test = [sent2features(s, word2features_func = word2features_func) for s in test_sents]\n",
    "\tY_test = [sent2labels(s) for s in test_sents]\n",
    "\n",
    "\t# get the label set\n",
    "\tset_labels = set([])\n",
    "\tfor data in [Y_train,Y_test] :\n",
    "\t\tfor n_sent in range(len(data)) :\n",
    "\t\t\tfor str_label in data[n_sent] :\n",
    "\t\t\t\tset_labels.add( str_label )\n",
    "\tlabels = list( set_labels )\n",
    "\tlogger.info( '' )\n",
    "\tlogger.info( 'labels = ' + repr(labels) )\n",
    "\n",
    "\t# remove 'O' label as we are not usually interested in how well 'O' is predicted\n",
    "\t#labels = list( crf.classes_ )\n",
    "\tlabels.remove('O')\n",
    "\n",
    "\t# print example feature vector (12th word of 1st sent)\n",
    "\tlogger.info( '' )\n",
    "\tlogger.info( 'Example training feature = ' + repr(X_train[0][10]) )\n",
    "\n",
    "\t# Train CRF model\n",
    "\tcrf = train_crf_model_func( X_train, Y_train, max_iter, labels )\n",
    "\n",
    "\tlogger.info('Label transition weights learnt from dataset (for a subset of labels)')\n",
    "\tdisplay( eli5.show_weights(crf, top=10, targets = display_label_subset, show=['transition_features']) )\n",
    "\n",
    "\tlogger.info('Top 10 features per-target (for a subset of labels)')\n",
    "\tdisplay( eli5.show_weights(crf, top=20, targets = display_label_subset, show=['targets']) )\n",
    "\n",
    "\t# compute the macro F1 score (F1 for instances of each label class averaged) in the test set\n",
    "\tY_pred = crf.predict( X_test )\n",
    "\tsorted_labels = sorted(\n",
    "\t\tlabels, \n",
    "\t\tkey=lambda name: (name[1:], name[0])\n",
    "\t)\n",
    "\tmacro_scores = sklearn_crfsuite.metrics.flat_classification_report( Y_test, Y_pred, labels=sorted_labels, digits=3, output_dict = True )\n",
    "\tlogger.info( '' )\n",
    "\tlogger.info( 'macro F1 scores'  )\n",
    "\tprint_F1_scores( macro_scores )\n",
    "\n",
    "\t# inspect the transitions\n",
    "\tlogger.info( '' )\n",
    "\tlogger.info(\"Top 10 likely state transitions\")\n",
    "\tprint_transitions( Counter(crf.transition_features_).most_common(10) )\n",
    "\n",
    "\tlogger.info( '' )\n",
    "\tlogger.info(\"Top 10 unlikely state transitions\")\n",
    "\tprint_transitions( Counter(crf.transition_features_).most_common()[-10:] )\n",
    "\n",
    "\t# inspect the states\n",
    "\tlogger.info( '' )\n",
    "\tlogger.info(\"Top 10 positive states\")\n",
    "\tprint_state_features(Counter(crf.state_features_).most_common(10))\n",
    "\n",
    "\tlogger.info( '' )\n",
    "\tlogger.info(\"Top 10 negative states\")\n",
    "\tprint_state_features(Counter(crf.state_features_).most_common()[-10:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will define a basic function to create set of features for a token position within a sentence. This function uses only the word and POS tag, and will look ahead and behind by one token index position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_word2features(sent, i):\n",
    "\n",
    "\tword = sent[i][0]\n",
    "\tpostag = sent[i][1]\n",
    "\n",
    "\tfeatures = {\n",
    "\t\t# basic features - token and POS tag\n",
    "\t\t'word' : word,\n",
    "\t\t'postag': postag,\n",
    "\t}\n",
    "\tif i > 0:\n",
    "\t\t# features for previous word (context)\n",
    "\t\tword_prev = sent[i-1][0]\n",
    "\t\tpostag_prev = sent[i-1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:postag': postag_prev,\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['BOS'] = True\n",
    "\n",
    "\tif i < len(sent)-1:\n",
    "\t\t# features for next word (context)\n",
    "\t\tword_next = sent[i+1][0]\n",
    "\t\tpostag_next = sent[i+1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:postag': postag_next,\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['EOS'] = True\n",
    "\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the function to train the CRF model using the sklearn_crfsuite toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task1_train_crf_model( X_train, Y_train, max_iter, labels ) :\n",
    "\t# train the basic CRF model\n",
    "\tcrf = sklearn_crfsuite.CRF(\n",
    "\t\talgorithm='lbfgs',\n",
    "\t\tc1=0.1,\n",
    "\t\tc2=0.1,\n",
    "\t\tmax_iterations=max_iter,\n",
    "\t\tall_possible_transitions=False,\n",
    "\t)\n",
    "\tcrf.fit(X_train, Y_train)\n",
    "\treturn crf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** EXERCISE Task 1 **\n",
    "\n",
    "Run exec_task() below to build the crf model and look at the baseline F1 scores using just words and POS tags as features. Notice how top 10 features in the baseline are simple words or POS tags.\n",
    "\n",
    "** END EXERCISE Task 1 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_task( word2features_func = task1_word2features, train_crf_model_func = task1_train_crf_model, max_files = max_files, max_iter = max_iter, display_label_subset = display_label_subset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - Add word shape and morpheme features\n",
    "\n",
    "Further reading: Course Text - Speech and Language Processing >> Information Extraction >> Named Entity Recognition \n",
    "\n",
    "It is easy to overfit CRF models if the features provided are too specific to the corpus. Word shapes and morphemes are great ways to provide more generic features, which in turn allows the CRF model to learn patterns containing morphological features beyond the surface form of the sentence words.\n",
    "\n",
    "Think about how many ways you can write a sentence containing the named entity 'New York'. If you only used the surface form words in each sentence you would need an unbounded training set covering all possible ways to talk about 'New York'. Adding more generic morphological features allows the model to handle unseen surface forms much better.\n",
    "\n",
    "** EXERCISE Task 2 **\n",
    "\n",
    "Write your own word2feature function that adds extra word shape features (uppercase, title, digits) and morphemes such as word affix (suffix) and POS affix (prefix). Build the model and look how the F1 score improves, and the top 10 features include shape and suffix information.\n",
    "\n",
    "A model answer is below in task2_word2features(). Look at this once you have had a go at your own function!\n",
    "\n",
    "** END EXERCISE Task 2 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task2_word2features(sent, i):\n",
    "\n",
    "\tword = sent[i][0]\n",
    "\tpostag = sent[i][1]\n",
    "\n",
    "\tfeatures = {\n",
    "\t\t'word' : word,\n",
    "\t\t'postag': postag,\n",
    "\n",
    "\t\t# token shape\n",
    "\t\t'word.lower()': word.lower(),\n",
    "\t\t'word.isupper()': word.isupper(),\n",
    "\t\t'word.istitle()': word.istitle(),\n",
    "\t\t'word.isdigit()': word.isdigit(),\n",
    "\n",
    "\t\t# token suffix\n",
    "\t\t'word.suffix': word.lower()[-3:],\n",
    "\n",
    "\t\t# POS prefix\n",
    "\t\t'postag[:2]': postag[:2],\n",
    "\t}\n",
    "\tif i > 0:\n",
    "\t\tword_prev = sent[i-1][0]\n",
    "\t\tpostag_prev = sent[i-1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:postag': postag_prev,\n",
    "\t\t\t'-1:word.lower()': word_prev.lower(),\n",
    "\t\t\t'-1:word.isupper()': word_prev.isupper(),\n",
    "\t\t\t'-1:word.istitle()': word_prev.istitle(),\n",
    "\t\t\t'-1:word.isdigit()': word_prev.isdigit(),\n",
    "\t\t\t'-1:word.suffix': word_prev.lower()[-3:],\n",
    "\t\t\t'-1:postag[:2]': postag_prev[:2],\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['BOS'] = True\n",
    "\n",
    "\tif i < len(sent)-1:\n",
    "\t\tword_next = sent[i+1][0]\n",
    "\t\tpostag_next = sent[i+1][1]\n",
    "\t\tfeatures.update({\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:postag': postag_next,\n",
    "\t\t\t'+1:word.lower()': word_next.lower(),\n",
    "\t\t\t'+1:word.isupper()': word_next.isupper(),\n",
    "\t\t\t'+1:word.istitle()': word_next.istitle(),\n",
    "\t\t\t'+1:word.isdigit()': word_next.isdigit(),\n",
    "\t\t\t'+1:word.suffix': word_next.lower()[-3:],\n",
    "\t\t\t'+1:postag[:2]': postag_next[:2],\n",
    "\t\t})\n",
    "\telse:\n",
    "\t\tfeatures['EOS'] = True\n",
    "\n",
    "\treturn features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_task( word2features_func = task2_word2features, train_crf_model_func = task1_train_crf_model, max_files = max_files, max_iter = max_iter, display_label_subset = display_label_subset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 - Change L1 regularization\n",
    "\n",
    "Further reading: [Article on L1 and L2 regularization](https://explained.ai/regularization/L1vsL2.html) \n",
    "\n",
    "Increasing the CRF model's L1 regularization (c1 parameter) will leave only more generic features. This should remove instance names such as 'Korea' and 'Iraq' from the feature set. With L1 regularization coefficients of most features should be driven to zero, so patterns reply on POS and word shape.\n",
    "\n",
    "** EXERCISE Task 3 **\n",
    "\n",
    "Write your own train_crf_model function to build a CRF model with a c1 of 200 and look at the top 10 features being chosen for labels. See how the features are less reliant on particular words and more on word shape or POS tag.\n",
    "\n",
    "A model answer is below in task3_train_crf_model(). Look at this once you have had a go at your own function!\n",
    "\n",
    "** END EXERCISE Task 2 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task3_train_crf_model( X_train, Y_train, max_iter, labels ) :\n",
    "\t# train CRF model using L1 reg of 200 (high value)\n",
    "\tcrf = sklearn_crfsuite.CRF(\n",
    "\t\talgorithm='lbfgs',\n",
    "\t\tc1=200,\n",
    "\t\tc2=0.1,\n",
    "\t\tmax_iterations=max_iter,\n",
    "\t\tall_possible_transitions=False,\n",
    "\t)\n",
    "\tcrf.fit(X_train, Y_train)\n",
    "\treturn crf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exec_task( word2features_func = task2_word2features, train_crf_model_func = task3_train_crf_model, max_files = max_files, max_iter = max_iter, display_label_subset = display_label_subset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 - Use all possible transitions\n",
    "\n",
    "Further reading: [Scikit Learn CRF model](https://sklearn-crfsuite.readthedocs.io/en/latest/api.html#module-sklearn_crfsuite)\n",
    "\n",
    "Transitions like O -> I-PERSON should have large negative weights because they are impossible. but these transitions have zero weights, not negative weights, both in heavily the regularized model and the initial model. The reason they are zero is that crfsuite has not seen these transitions in training data, and assumed there is no need to learn weights for them, to save some computation time.\n",
    "\n",
    "This is the default behavior. It is possible to turn it off using sklearn_crfsuite.CRF all_possible_transitions option.\n",
    "\n",
    "** EXERCISE Task 4 **\n",
    "\n",
    "Change your train_crf_model function so it builds a CRF model with all_possible_transitions = True and look at the negative weighting of O -> I-xxx labels. See how these transitions are now explicitly negatively weighted.\n",
    "\n",
    "A model answer is below in task4_train_crf_model(). Look at this once you have had a go at your own function!\n",
    "\n",
    "** END EXERCISE Task 4 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task4_train_crf_model( X_train, Y_train, max_iter, labels ) :\n",
    "\t# train CRF model using all possible transitions\n",
    "\tcrf = sklearn_crfsuite.CRF(\n",
    "\t\talgorithm='lbfgs',\n",
    "\t\tc1=0.1,\n",
    "\t\tc2=0.1,\n",
    "\t\tmax_iterations=max_iter,\n",
    "\t\tall_possible_transitions=True,\n",
    "\t)\n",
    "\tcrf.fit(X_train, Y_train)\n",
    "\treturn crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_task( word2features_func = task2_word2features, train_crf_model_func = task4_train_crf_model, max_files = max_files, max_iter = max_iter, display_label_subset = display_label_subset )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 - Randomized search for hyperparameter tuning\n",
    "\n",
    "Further reading: [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "\n",
    "Choosing the right hyperparameters can be very hard to know at design time. Usually it requires some experimentation to choose the best ones. Using a grid or randomized search strategy is a good way to automatically explore the hyperparameter space and idcentify the best hyperparameter settings. If you already have a hypothesis for what parameter ranges might work best, simple constrain the search space to focus on the areas you think should work best.\n",
    "\n",
    "** EXERCISE Task 5 **\n",
    "\n",
    "Create a new train_crf_model function to perform a randomized search to find the best hyperparameters (c1, c2) for the crf model. Return the best CRF model.\n",
    "\n",
    "A model answer is below in task5_train_crf_model() which includes a visual display of the F1 scores in searched parameter space. Look at this once you have had a go at your own function!\n",
    "\n",
    "** END EXERCISE Task 5 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task5_train_crf_model( X_train, Y_train, max_iter, labels ) :\n",
    "\t# randomized search to discover best parameters for CRF model\n",
    "\tcrf = sklearn_crfsuite.CRF(\n",
    "\t\talgorithm='lbfgs', \n",
    "\t\tmax_iterations=max_iter, \n",
    "\t\tall_possible_transitions=True\n",
    "\t)\n",
    "\tparams_space = {\n",
    "\t\t'c1': scipy.stats.expon(scale=0.5),\n",
    "\t\t'c2': scipy.stats.expon(scale=0.05),\n",
    "\t}\n",
    "\n",
    "\t# optimize for micro F1 score\n",
    "\tf1_scorer = make_scorer( sklearn_crfsuite.metrics.flat_f1_score, average='weighted', labels=labels )\n",
    "\n",
    "\tlogger.info( 'starting randomized search for hyperparameters' )\n",
    "\tn_folds = 2\n",
    "\tn_candidates = 10\n",
    "\trs = sklearn.model_selection.RandomizedSearchCV(crf, params_space, cv=n_folds, verbose=1, n_jobs=-1, n_iter=n_candidates, scoring=f1_scorer)\n",
    "\trs.fit(X_train, Y_train)\n",
    "\n",
    "\t# output the results\n",
    "\tlogger.info( 'best params: {}'.format( rs.best_params_ ) )\n",
    "\tlogger.info( 'best micro F1 score: {}'.format( rs.best_score_ ) )\n",
    "\tlogger.info( 'model size: {:0.2f}M'.format( rs.best_estimator_.size_ / 1000000 ) )\n",
    "\tlogger.info( 'cv_results_ = ' + repr(rs.cv_results_) )\n",
    "\n",
    "\t# visualize the results in hyperparameter space\n",
    "\t_x = [s['c1'] for s in rs.cv_results_['params']]\n",
    "\t_y = [s['c2'] for s in rs.cv_results_['params']]\n",
    "\t_c = [s for s in rs.cv_results_['mean_test_score']]\n",
    "\n",
    "\tfig = plt.figure()\n",
    "\tfig.set_size_inches(12, 12)\n",
    "\tax = plt.gca()\n",
    "\tax.set_yscale('log')\n",
    "\tax.set_xscale('log')\n",
    "\tax.set_xlabel('C1')\n",
    "\tax.set_ylabel('C2')\n",
    "\tax.set_title(\"Randomized Hyperparameter Search - F1 scores (blue min={:0.2}, red max={:0.2})\".format( min(_c), max(_c) ))\n",
    "\tax.scatter(_x, _y, c=_c, s=60, alpha=0.9, edgecolors=[0,0,0])\n",
    "\n",
    "\t# return the best model\n",
    "\tcrf = rs.best_estimator_\n",
    "\treturn crf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exec_task( word2features_func = task2_word2features, train_crf_model_func = task5_train_crf_model, max_files = max_files, max_iter = max_iter, display_label_subset = display_label_subset )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
