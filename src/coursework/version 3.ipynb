{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "//FOR MAKING EVERYTHING TABS IN JUYPYTER NOTEBOOK, LEAVE HERE FOR NOW\n",
       "//ALSO MAKE SURE YOU *ALWAYS* RUN THIS WHENEVER YOU OPEN THE NOTEBOOK\n",
       "//BEFORE EVERY DOING ANYTHING\n",
       "IPython.tab_as_tab_everywhere = function(use_tabs) {\n",
       "    if (use_tabs === undefined) {\n",
       "        use_tabs = true; \n",
       "    }\n",
       "\n",
       "    // apply setting to all current CodeMirror instances\n",
       "    IPython.notebook.get_cells().map(\n",
       "        function(c) {  return c.code_mirror.options.indentWithTabs=use_tabs;  }\n",
       "    );\n",
       "    // make sure new CodeMirror instances created in the future also use this setting\n",
       "    CodeMirror.defaults.indentWithTabs=use_tabs;\n",
       "\n",
       "    };\n",
       "\n",
       "IPython.tab_as_tab_everywhere()\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "//FOR MAKING EVERYTHING TABS IN JUYPYTER NOTEBOOK, LEAVE HERE FOR NOW\n",
    "//ALSO MAKE SURE YOU *ALWAYS* RUN THIS WHENEVER YOU OPEN THE NOTEBOOK\n",
    "//BEFORE EVERY DOING ANYTHING\n",
    "IPython.tab_as_tab_everywhere = function(use_tabs) {\n",
    "    if (use_tabs === undefined) {\n",
    "        use_tabs = true; \n",
    "    }\n",
    "\n",
    "    // apply setting to all current CodeMirror instances\n",
    "    IPython.notebook.get_cells().map(\n",
    "        function(c) {  return c.code_mirror.options.indentWithTabs=use_tabs;  }\n",
    "    );\n",
    "    // make sure new CodeMirror instances created in the future also use this setting\n",
    "    CodeMirror.defaults.indentWithTabs=use_tabs;\n",
    "\n",
    "    };\n",
    "\n",
    "IPython.tab_as_tab_everywhere()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just keep as 50 for now, obviously make larger for handins\n",
    "max_iter = 20\n",
    "\n",
    "sample_size = 5000\n",
    "\n",
    "#turn this off for handin, whether the model is verbose or not\n",
    "verbose_on = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/george/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import sys, codecs, json, math, time, warnings\n",
    "warnings.simplefilter( action='ignore', category=FutureWarning )\n",
    "\n",
    "import nltk, scipy, sklearn, sklearn_crfsuite, sklearn_crfsuite.metrics, eli5\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.metrics import make_scorer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display    \n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "import tensorflow as tf\n",
    "import absl.logging\n",
    "formatter = logging.Formatter('[%(levelname)s|%(filename)s:%(lineno)s %(asctime)s] %(message)s')\n",
    "absl.logging.get_absl_handler().setFormatter(formatter)\n",
    "absl.logging._warn_preinit_stderr = False\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the whole book that has been read as a string and generates\n",
    "#the dictionary needed containing the table of contents of the book\n",
    "def task1(book_str):\n",
    "    \n",
    "\tcontents = r\"(CONTENTS|(C|c)ontents)\"\n",
    "\t\n",
    "\t#either a roman numeral or digit\n",
    "\tnumber = r\"([ivxlcdm]+|[IVXLCDM]+|\\d+)\"\n",
    "\t\n",
    "\t#the charcters which have been seen to appear in titles I've looked at\n",
    "\tchset = r\"[A-Za-z0-9\\?\\-\\.\\'\\\":“”’]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the chapter that has been read as a string and extracts all \n",
    "#questions from it into the required set\n",
    "def task2(chapter_str):\n",
    "    \n",
    "    #Anything can be part of a question\n",
    "    #starts with a capital letter followed by any number of \n",
    "    # \"non-ending\" characters\n",
    "\tquestion_regex = r\"[A-Z][^\\?\\.,!]*\\?\"\n",
    "    \n",
    "    #doesnt really need to be match objects, maybe change this later\n",
    "\treturn set([match_obj[0] for match_obj in re.finditer(question_regex, chapter_str, flags = re.MULTILINE | re.DOTALL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tasks 3 and 4 are sort of linked so MAYBE handling all their initialisation\n",
    "#stuff make more sense within a single function? Not sure, leave here for now\n",
    "\n",
    "#def init_NER(chapter_str):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the chapter that has been read as a string and finds the \n",
    "#NER entitites listed in the task 3 spec, generating a dictionary\n",
    "#containing these that the names found in task 4 can then be \n",
    "#injected into\n",
    "#def task3(chapter_str):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the chapter that has been read as a string and finds the names\n",
    "#of all characters, returning a list that can be put into the task 3/4\n",
    "#dictionary writer\n",
    "#def task4(chapter_str):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUBTASK 1, takes the whole book file as an argument\n",
    "\n",
    "#all you have to do is populate dictTOC as shown commented out and \n",
    "#the function will handle writing everything for you etc.\n",
    "def exec_regex_toc( file_book = None ) :\n",
    "\n",
    "\tbook_str = \"\"\n",
    "\twith open(file_book, 'r') as f:\n",
    "\t\tbook_str = f.read()\n",
    "    \n",
    "\tdictTOC = task1(book_str)\n",
    "        \n",
    "        \n",
    "\t# INSERT CODE TO USE REGEX TO BUILD A TABLE OF CONTENTS FOR A BOOK (subtask 1)\n",
    "\n",
    "#\t# hardcoded output to show exactly what is expected to be serialized\n",
    "#\tdictTOC = {\n",
    "#\t\t\t\"1\": \"I AM BORN\",\n",
    "#\t\t\t\"2\": \"I OBSERVE\",\n",
    "#\t\t\t\"3\": \"I HAVE A CHANGE\"\n",
    "#\t\t}\n",
    "\n",
    "\treturn dictTOC\n",
    "\n",
    "\t# DO NOT CHANGE THE BELOW CODE WHICH WILL SERIALIZE THE ANSWERS FOR THE AUTOMATED TEST HARNESS TO LOAD AND MARK\n",
    "\n",
    "\t\"\"\"\n",
    "\twriteHandle = codecs.open( 'toc.json', 'w', 'utf-8', errors = 'replace' )\n",
    "\tstrJSON = json.dumps( dictTOC, indent=2 )\n",
    "\twriteHandle.write( strJSON + '\\n' )\n",
    "\twriteHandle.close()\n",
    "\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SUBTASK 2, take the file chapter as argument and handles writing the list of questions\n",
    "\n",
    "#all you ahve to do is populate setQuestions as is shown commented out\n",
    "#and the function will then handle writing everything and stuff for you\n",
    "def exec_regex_questions( file_chapter = None ) :\n",
    "\n",
    "\tchapter_str = \"\"\n",
    "\twith open(file_chapter, 'r') as f:\n",
    "\t\tchapter_str = f.read()\n",
    "    \n",
    "\tsetQuestions = task2(chapter_str)\n",
    "    \n",
    "\t# INSERT CODE TO USE REGEX TO LIST ALL QUESTIONS IN THE CHAPTER OF TEXT (subtask 2)\n",
    "\n",
    "\t# hardcoded output to show exactly what is expected to be serialized\n",
    "#\tsetQuestions = set([\n",
    "#\t\t\"Traddles?\",\n",
    "#\t\t\"And another shilling or so in biscuits, and another in fruit, eh?\",\n",
    "#\t\t\"Perhaps you’d like to spend a couple of shillings or so, in a bottle of currant wine by and by, up in the bedroom?\",\n",
    "#\t\t\"Has that fellow’--to the man with the wooden leg--‘been here again?\"\n",
    "#\t\t])\n",
    "\n",
    "\t# DO NOT CHANGE THE BELOW CODE WHICH WILL SERIALIZE THE ANSWERS FOR THE AUTOMATED TEST HARNESS TO LOAD AND MARK\n",
    "\n",
    "\treturn setQuestions\n",
    "\t\n",
    "\t\"\"\"\n",
    "\twriteHandle = codecs.open( 'questions.txt', 'w', 'utf-8', errors = 'replace' )\n",
    "\tfor strQuestion in setQuestions :\n",
    "\t\twriteHandle.write( strQuestion + '\\n' )\n",
    "\twriteHandle.close()\n",
    "\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from nltk.corpus import names\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the file path of ontonotes and return sentences\n",
    "def load_ontonotes(dataset_file):\n",
    "\n",
    "\treadHandle = codecs.open(dataset_file, 'r', 'utf-8', errors = 'replace')\n",
    "\tstr_json = readHandle.read()\n",
    "\treadHandle.close()\n",
    "\tdict_ontonotes = json.loads(str_json)\n",
    "\torig_list = list(dict_ontonotes.keys())\n",
    "\n",
    "\t#the counter is just for testing, remove it later maybe\n",
    "\ti = 0\n",
    "\n",
    "\t#the sentences of triplets containing the tokens, nltk POS tags\n",
    "\t#and NER tags from ontonotes\n",
    "\tsentences = []\n",
    "\n",
    "\tfor str_file in orig_list:\n",
    "\t#REMOVE THIS ITS FOR TESTING\n",
    "\t\t#print(\"on file number:\", i)\n",
    "\t\ti += 1\n",
    "\t\tfor str_sent_index in dict_ontonotes[str_file] :\n",
    "\n",
    "\t\t\ttokens = []\n",
    "\t\t\tner_tags = []\n",
    "\t\t\t# compute IOB tags for named entities (if any)\n",
    "\t\t\tne_type_last = None\n",
    "\n",
    "\t\t\t#build up the list of tokens and ner tags\n",
    "\t\t\tfor nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])) :\n",
    "\t\t\t\tstrToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "\t\t\t\tstrPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "\t\t\t\tne_type = None\n",
    "\t\t\t\tif 'ne' in dict_ontonotes[str_file][str_sent_index] :\n",
    "\t\t\t\t\tdict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "\t\t\t\t\tif not 'parse_error' in dict_ne :\n",
    "\t\t\t\t\t\tfor str_NEIndex in dict_ne :\n",
    "\t\t\t\t\t\t\tif nTokenIndex in dict_ne[str_NEIndex]['tokens'] :\n",
    "\t\t\t\t\t\t\t\tne_type = dict_ne[str_NEIndex]['type']\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif ne_type != None :\n",
    "\t\t\t\t\tif ne_type == ne_type_last :\n",
    "\t\t\t\t\t\tstrIOB = 'I-' + ne_type\n",
    "\t\t\t\t\telse :\n",
    "\t\t\t\t\t\tstrIOB = 'B-' + ne_type\n",
    "\t\t\t\telse :\n",
    "\t\t\t\t\tstrIOB = 'O'\n",
    "\n",
    "\t\t\t\tne_type_last = ne_type\n",
    "\t\t\t\ttokens.append(strToken)\n",
    "\t\t\t\tner_tags.append(strIOB)\n",
    "\t\t\t\t#list_entry.append( ( strToken, strIOB ) )\n",
    "\n",
    "\t\t\t#this should probably be removed, leave it here for now though\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tskip = True\n",
    "\n",
    "\t\t\tfor tag in ner_tags:\n",
    "\t\t\t\t#display label subset contains the NER tags that we are looking for\n",
    "\t\t\t\tif tag in display_label_subset:\n",
    "\t\t\t\t\tskip = False\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t#this sentence does not contain any of the named entity tags\n",
    "\t\t\t#that we are looking for so it is useless, don't bother using it\n",
    "\t\t\t#if skip:\n",
    "\t\t\t\t#continue\n",
    "\t\t\t\"\"\"\n",
    "\n",
    "\t\t\tlist_entry = []\n",
    "\n",
    "\t\t\t#use nltk pos tags instead\n",
    "\t\t\tfor (index, tup) in enumerate(pos_tag(tokens)):\n",
    "\t\t\t\t#print(\"tup is:\", tup)\n",
    "\t\t\t\tlist_entry.append((tup[0], tup[1], ner_tags[index]))\n",
    "\t\t\tsentences.append( list_entry )\n",
    "\treturn sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package names to /home/george/nltk_data...\n",
      "[nltk_data]   Package names is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from nltk.corpus import names\n",
    "from copy import copy\n",
    "from random import shuffle\n",
    "nltk.download(\"names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_sequences(tag_list, sequences, exact):\n",
    "\t\n",
    "\tdct = {tag : [] for tag in tag_list}\n",
    "\t\n",
    "\tfor pair_list in tok_NER_pair_lists:\n",
    "\t\t\n",
    "\t\tfor i in range(len(pair_list)):\n",
    "\t\t\t\n",
    "\t\t\tactual_tag = pair_list[i][1][2:]\n",
    "\t\t\tentity = pair_list[i][0]\n",
    "\t\t\ti += 1\n",
    "\t\t\t\n",
    "\t\t\twhile pair_list[i][1][2:] == actual_tag:\n",
    "\t\t\t\tentity += \" \" + pair_list[i][0]\n",
    "\t\t\t\ti += 1\n",
    "\t\t\t\n",
    "\t\t\tif (not exact):\n",
    "\t\t\t\tentity = entity.lower()\n",
    "\t\t\t\n",
    "\t\t\tdct[actual_tag].append(entity)\n",
    "\t\n",
    "\treturn dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#They should be exactly the same length as the predictions have been made from them\n",
    "def gen_tok_NER_pair_lists(sentences_with_unknown_NER, NER_predictions):\n",
    "\t\n",
    "\tpair_lists = []\n",
    "\t\n",
    "\tfor (i, sentence) in enumerate(sentences_with_unknown_NER):\n",
    "\t\tpair_list = []\n",
    "\t\tfor thing in enumerate(sentence):\n",
    "\t\t\tprint(\"Thing is: (testing)\")\n",
    "\t\t\tprint(thing)\n",
    "\t\t\tpair_list.append((token, NER_predictions[i][j]))\n",
    "\t\tpair_lists.append(pair_list)\n",
    "\t\n",
    "\treturn pair_lists\n",
    "\t\"\"\"\n",
    "\tpair_lists = []\n",
    "\tfor i in range(len(NER_predictions)):\n",
    "\t\tpair_list = []\n",
    "\t\tfor j in range(len(NER_predictions[i])):\n",
    "\t\t\ttoken = sentences_with_unknown_NER[i][j][0]\n",
    "\t\t\tner_tag = NER_predictions[i][j]\n",
    "\t\t\tpair_list.append((token, ner_tag))\n",
    "\t\tpair_lists.append(pair_list)\n",
    "\treturn pair_lists\n",
    "\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#given the loaded ontonotes sentences and the sentences from the chapter\n",
    "#which is having entities extracted from it, return the final NER dictionary for tasks 3 and 4\n",
    "def run_NER(sentences, sentences_to_predict):\n",
    "\n",
    "\t#given a specific NER tag, return 2 sets consisting of the \n",
    "\t#\"inside parts\" and the \"before parts\"\n",
    "\tdef extract_gazetteers(target, sentences):\n",
    "\t\tbefore = set()\n",
    "\t\tinside = set()\n",
    "\t\tbefore_tag = \"B-\" + target\n",
    "\t\tinside_tag = \"I-\" + target\n",
    "\t\tfor sent in sentences:\n",
    "\t\t\tfor tok, _, ner in sent:\n",
    "\t\t\t\tif ner == before_tag:\n",
    "\t\t\t\t\tbefore.add(tok.lower())\n",
    "\t\t\t\tif ner == inside_tag:\n",
    "\t\t\t\t\tinside.add(tok.lower())\n",
    "\t\treturn before, inside\n",
    "\n",
    "\n",
    "\t#convert all the NER tags you don't care about into O's in sentences that may have mixed NER tags that we care about\n",
    "\t#with NER tags that we don't care about\n",
    "\tdef only_Os(desired_set, sentences):\n",
    "\t\tactual_tags = []\n",
    "\t\tfor tag in desired_set:\n",
    "\t\t\tactual_tags.append(\"B-\" + tag)\n",
    "\t\t\tactual_tags.append(\"I-\" + tag)\n",
    "\t\treturn [[(tok, pos, ner if ner in actual_tags else 'O') for tok, pos, ner in sent] for sent in sentences]\n",
    "\n",
    "\n",
    "\t#If the dataset has already been split and we KNOW that none of these NER tags are the ones we care about,\n",
    "\t#then clearly we can just replace ALL NER tags with O\n",
    "\tdef all_Os(sentences):\n",
    "\t\treturn [[(tok, pos, 'O') for tok, pos, _ in sent] for sent in sentences]\n",
    "\n",
    "\n",
    "\t#returns a tuples of 2 lists of sentences, where one list contains all the sentences that contain the tags \n",
    "\t#specified in the argument and the other list does not\n",
    "\t#you just give it the tag strings and it adds the Bs and Is for you\n",
    "\tdef split_tags(tag_set, sentences):\n",
    "\n",
    "\t\tactual_tags = []\n",
    "\t\tfor tag in tag_set:\n",
    "\t\t\tactual_tags.append(\"B-\" + tag)\n",
    "\t\t\tactual_tags.append(\"I-\" + tag)\n",
    "\n",
    "\t\tcontains_tags = []\n",
    "\t\tdoesnt_contain_tags = []\n",
    "\n",
    "\t\tfor sent in sentences:\n",
    "\n",
    "\t\t\tif(any([tag in actual_tags for _,_,tag in sent])):\n",
    "\t\t\t\tcontains_tags.append(sent)\n",
    "\t\t\telse:\n",
    "\t\t\t\tdoesnt_contain_tags.append(sent)\n",
    "\n",
    "\t\treturn (contains_tags, doesnt_contain_tags)\n",
    "\n",
    "\t#the sentences containing task 3 tags and the sentences that dont\n",
    "\twtask3, wotask3 = split_tags([\"DATE\", \"CARDINAL\", \"ORDINAL\", \"NORP\"], sentences)\n",
    "\n",
    "\t#the sentences containing task 4 tags (only person) and the sentences that dont\n",
    "\twtask4, wotask4 = split_tags([\"PERSON\"], sentences)\n",
    "\n",
    "\t#all other tags can be converted to O because we dont care about finding them\n",
    "\ttask3_processed = only_Os([\"DATE\", \"CARDINAL\", \"ORDINAL\", \"NORP\"], wtask3)\n",
    "\ttask4_processed = only_Os([\"PERSON\"], wtask4)\n",
    "\n",
    "\t\n",
    "\t#take a random sample with the specified size from the list of sentences\n",
    "\tdef random_sample(sentences, num):\n",
    "\t\tcp = copy(sentences)\n",
    "\t\tshuffle(cp)\n",
    "\t\treturn cp[-num:]\n",
    "\t\n",
    "\t#split the items into training and testing denoted by the testing ratio argument\n",
    "\tdef random_split(items, ratio):\n",
    "\t\tshuffle(copy(items))\n",
    "\t\ttrain_size = int(ratio * len(items))\n",
    "\t\ttest_size = len(items) - train_size\n",
    "\t\treturn items[:train_size], items[-test_size:]\n",
    "\n",
    "\t#don't need training and testing for handin, just use the whole thing\n",
    "\t#task3_training, task3_testing = random_split(task3_processed, 0.9)\n",
    "\t#task4_training, task4_testing = random_split(task4_processed, 0.9)\n",
    "\n",
    "\t#generate the gazetteers to be used from ontonotes\n",
    "\t#before_name, inside_name = extract_gazetteers(\"PERSON\", wtask4)\n",
    "\t#before_date, inside_date = extract_gazetteers(\"DATE\", wtask3)\n",
    "\t#before_cardinal, inside_cardinal = extract_gazetteers(\"CARDINAL\", wtask3)\n",
    "\t#before_norp, inside_norp = extract_gazetteers(\"NORP\", wtask3)\n",
    "\t#before_ord, inside_ord = extract_gazetteers(\"ORDINAL\", wtask3)\n",
    "\n",
    "\t\t#assumed to be in the environment of the function that \n",
    "\t#generates the feature dictionary for task 3\n",
    "\tnumber_gazetteer = set([\n",
    "\t\t\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "\t\t\"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "\t\t\"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"fourty\", \"fifty\",\n",
    "\t\t\"sixty\", \"seventy\", \"eighty\", \"ninety\", \"hundred\", \"onehundred\", \"one-hundred\", \"thousand\", \"million\"\n",
    "\t\t])\n",
    "\n",
    "\t#Python sets are implemented as hash tables so lookup is always around O(1)\n",
    "\t#even for large sets\n",
    "\n",
    "\tordinal_gazetteer = set([\n",
    "\t\t\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\",\n",
    "\t\t\"eigth\", \"ninth\", \"tenth\", \"eleventh\", \"twelth\", \"thirteenth\",\n",
    "\t\t\"fourteenth\", \"fifteenth\", \"sixteenth\", \"seventeenth\", \"eighteenth\",\n",
    "\t\t\"nineteenth\", \"twentieth\", \"thirtieth\", \"fourtieth\", \"fiftieth\", \n",
    "\t\t\"sixtieth\", \"seventieth\", \"eightieth\", \"ninetieth\", \"hundreth\", \"thousandth\",\n",
    "\t\t\"millionth\", \"firstly\", \"secondly\", \"thirdly\", \"fourthly\", \"fifthly\", \"sixthly\",\n",
    "\t\t\"seventhly\", \"eigthly\", \"ninethly\", \"tenthly\"\n",
    "\t])\n",
    "\n",
    "\tdate_gazetteer = set([\n",
    "\t\t\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\", \"day\", \"week\", \"month\", \"year\"\n",
    "\t\t\"jan\", \"january\", \"feb\", \"february\", \"mar\", \"march\", \"apr\", \"april\", \"may\", \"jun\", \"june\", \"jul\", \"july\",\n",
    "\t\t\"aug\", \"august\", \"sep\", \"september\", \"oct\", \"october\", \"nov\", \"november\", \"dec\", \"december\", \"holiday\", \"holidays\"\n",
    "\t])\n",
    "\n",
    "\ttime_gazetteer = set([\"second\", \"seconds\", \"minute\", \"minutes\", \"hour\", \"hours\", \"time\", \"clock\", \"o'clock\", \"past\", \"oclock\"])\n",
    "\n",
    "\t# a feature for detecting e.g. 1st, 2nd, 3rd, 4th etc.\n",
    "\tdef is_st_th(candidate):\n",
    "\t\tsuffixes = set([\"st\", \"nd\", \"rd\", \"th\"])\n",
    "\t\treturn candidate[-2:] in suffixes and candidate[:-2].isdigit()\n",
    "\n",
    "\t#use the nltk name corpus\n",
    "\tname_gazetteer = set(map(lambda name: name.lower(), names.words(\"male.txt\") + names.words(\"female.txt\")))\n",
    "\n",
    "\t#task 4 has less features as it only revolves around names\n",
    "\tdef gen_feature_dict_task4(sentence, i):\n",
    "\n",
    "\t\tdef gen_features(index, relative_str):\n",
    "\t\t\treturn {\n",
    "\t\t\t\trelative_str + \"word.lower()\": sentence[index][0].lower(),\n",
    "\t\t\t\trelative_str + \"word_prefix\": sentence[index][0][:3],\n",
    "\t\t\t\trelative_str + \"word_suffix\": sentence[index][0][-3:],\n",
    "\t\t\t\trelative_str + \"postag\": sentence[index][1],\n",
    "\t\t  #relative_str + \"postag_prefix2\": sentence[index][1][:2], THINK THESE HARM THE MODEL\n",
    "\t\t\t\t#relative_str + \"postag_prefix3\": sentence[index][1][:3], #think this is useless?\n",
    "\t\t\t\trelative_str + \"istitle\": sentence[index][0].istitle(),\n",
    "\t\t\t\trelative_str + \"isupper\": sentence[index][0].isupper()     \n",
    "\t\t\t}\n",
    "\n",
    "\t\t#extra features are limited to center 3 in the context window of size 5\n",
    "\t\tdef gen_extra_features(index, relative_str):\n",
    "\t\t\treturn {\n",
    "\t\t\t\trelative_str + \"contains_digit\" : any(char.isdigit() for char in sentence[index][0]),\n",
    "\t\t\t\trelative_str + \"in_nltk_name_gaz\" :  sentence[index][0].lower() in name_gazetteer,\n",
    "#\t\t\t\trelative_str + \"in_B_name\" : sentence[index][0].lower() in before_name,\n",
    "#\t\t\t\trelative_str + \"in_I_name\" : sentence[index][0].lower() in inside_name\n",
    "\t\t\t}\n",
    "\n",
    "\t\tfeatures = gen_features(i, \"\")\n",
    "\t\tfeatures.update(gen_extra_features(i, \"\"))\n",
    "\n",
    "\t\t#at least 1 word behind\n",
    "\t\tif i > 0:#generate a list of feature dictionaries for each word in the sentence\n",
    "\t\t\tfeatures.update(gen_features(i - 1, \"-1:\"))\n",
    "\t\t\tfeatures.update(gen_extra_features(i - 1, \"-1:\"))\n",
    "\n",
    "\t\t#at least 2 words behind\n",
    "\t\tif i > 1:\n",
    "\t\t\tfeatures.update(gen_features(i - 2, \"-2:\"))\n",
    "\n",
    "\t\t#at least 1 word ahead\n",
    "\t\tif i < len(sentence) - 1:\n",
    "\t\t\tfeatures.update(gen_features(i + 1, \"+1:\"))\n",
    "\t\t\tfeatures.update(gen_extra_features(i + 1, \"+1:\"))\n",
    "\n",
    "\t\t#at least 2 words ahead\n",
    "\t\tif i < len(sentence) - 2:\n",
    "\t\t\tfeatures.update(gen_features(i + 2, \"+2:\"))\n",
    "\n",
    "\t\treturn features\n",
    "\n",
    "\tdef gen_feature_dict_task3(sentence, i):\n",
    "\n",
    "\t\t#token = sentence[i][0]\n",
    "\t\t#pos_tag = sentence[i][1]\n",
    "\n",
    "\t\tdef gen_features(index, relative_str):\n",
    "\n",
    "\t\t\treturn {\n",
    "\t\t\t\trelative_str + \"word.lower()\": sentence[index][0].lower(),\n",
    "\t\t\t\trelative_str + \"word_prefix\": sentence[index][0][:3],\n",
    "\t\t\t\trelative_str + \"word_suffix\": sentence[index][0][-3:],\n",
    "\t\t\t\trelative_str + \"postag\": sentence[index][1],\n",
    "\t\t  #relative_str + \"postag_prefix2\": sentence[index][1][:2], THINK THESE HARM THE MODEL\n",
    "\t\t\t\t#relative_str + \"postag_prefix3\": sentence[index][1][:3], #think this is useless?\n",
    "\t\t\t\trelative_str + \"istitle\": sentence[index][0].istitle(),\n",
    "\t\t\t\trelative_str + \"isupper\": sentence[index][0].isupper(),        \n",
    "\t\t\t}\n",
    "\n",
    "\t\t#extra features are limited to center 3 in the context window of size 5\n",
    "\t\tdef gen_extra_features(index, relative_str):\n",
    "\t\t\treturn {\n",
    "\t\t\t\trelative_str + \"contains_digit\": any(char.isdigit() for char in sentence[index][0]), \n",
    "\t\t\t\trelative_str + \"contains_day\": \"day\" in sentence[index][0].lower(),\n",
    "\n",
    "\t\t\t\t#python sets are around O(1) lookup so its fine to have large gazetteers\n",
    "\t\t\t\trelative_str + \"in_num_gaz\": sentence[index][0].lower() in number_gazetteer,\n",
    "\t\t\t\trelative_str + \"in_ord_gaz\": sentence[index][0].lower() in ordinal_gazetteer,\n",
    "\t\t\t\trelative_str + \"in_date_gaz\": sentence[index][0].lower() in date_gazetteer,\n",
    "\t\t\t\trelative_str + \"in_time_gaz\": sentence[index][0].lower() in time_gazetteer,    \n",
    "\n",
    "\t\t\t\t#COMMENT THESE OUT MAYBE COS NOT SURE ABOUT THEM\n",
    "#\t\t\t\trelative_str + \"in_B_date\": sentence[index][0].lower() in before_date,\n",
    "#\t\t\t\trelative_str + \"in_I_date\": sentence[index][0].lower() in inside_date,\n",
    "#\t\t\t\trelative_str + \"in_B_card\": sentence[index][0].lower() in before_cardinal,\n",
    "#\t\t\t\trelative_str + \"in_I_card\": sentence[index][0].lower() in inside_cardinal,\n",
    "#\t\t\t\trelative_str + \"in_B_norp\": sentence[index][0].lower() in before_norp,\n",
    "#\t\t\t\trelative_str + \"in_I_norp\": sentence[index][0].lower() in inside_norp,\n",
    "#\t\t\t\trelative_str + \"in_B_ord\": sentence[index][0].lower() in before_ord,\n",
    "#\t\t\t\trelative_str + \"in_I_ord\": sentence[index][0].lower() in inside_ord,\n",
    "\t\t\t\trelative_str + \"is_st_th\": is_st_th(sentence[index][0])\n",
    "\t\t\t}\n",
    "\n",
    "\t\tfeatures = gen_features(i, \"\")\n",
    "\t\tfeatures.update(gen_extra_features(i, \"\"))\n",
    "\n",
    "\t\t#there is at least 1 word behind\n",
    "\t\tif i > 0:\n",
    "\t\t\tfeatures.update(gen_features(i - 1, \"-1:\"))\n",
    "\t\t\tfeatures.update(gen_extra_features(i - 1, \"-1:\"))\n",
    "\n",
    "\t\t#there are at least 2 words behind\n",
    "\t\tif i > 1:\n",
    "\t\t\tfeatures.update(gen_features(i - 2, \"-2:\"))\n",
    "\n",
    "\t\t#there is at least 1 word ahead\n",
    "\t\tif i < len(sentence) - 1:\n",
    "\t\t\tfeatures.update(gen_features(i + 1, \"+1:\"))\n",
    "\t\t\tfeatures.update(gen_extra_features(i + 1, \"+1:\"))\n",
    "\n",
    "\t\t#there are at least 2 words ahead\n",
    "\t\tif i < len(sentence) - 2:\n",
    "\t\t\tfeatures.update(gen_features(i + 2, \"+2:\"))\n",
    "\n",
    "\t\treturn features\n",
    "\n",
    "\t\t#helper functions\n",
    "\tdef sent2labels(sentence):\n",
    "\t\treturn [label for _, _, label in sentence] \n",
    "\tdef sent2tokens(sent):\n",
    "\t\treturn [token for token, _, _ in sentence]\n",
    "\n",
    "\tdef sentences2labels(sentences):\n",
    "\t\treturn [sent2labels(sent) for sent in sentences]\n",
    "\n",
    "\t\n",
    "\t\t#generate a list of feature dictionaries for each word in the sentence\n",
    "\tdef gen_task3_features(sentence):\n",
    "\t\treturn [gen_feature_dict_task3(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "\tdef gen_task4_features(sentence):\n",
    "\t\treturn [gen_feature_dict_task4(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "\t#generate lists of feature dictionaries for each sentence\n",
    "\tdef task3(ontonotes_data, sentences_to_predict):\n",
    "\t\t\n",
    "\t\tprint(\"Task 3 sentences to predict length:\", len(sentences_to_predict))\n",
    "\t\t\n",
    "\t\tfeature_lists = [gen_task3_features(sent) for sent in ontonotes_data]\n",
    "\t\tfeature_dicts_to_predict = [gen_task4_features(sent) for sent in sentences_to_predict]\n",
    "\t\tprint(\"The first feature dict in the first feature dict list is:\")\n",
    "\t\tprint(feature_lists[0][0])\n",
    "\t\tprint(\"The first feature dict in the first feature dict list (the ones that should be predicted) is:\")\n",
    "\t\tprint(feature_dicts_to_predict[0][0])\n",
    "\t\t\n",
    "\t\tprint(\"Made the task 3 feature lists\")\n",
    "\t\tlabel_lists = [sent2labels(sent) for sent in ontonotes_data]\n",
    "\t\tcrf = sklearn_crfsuite.CRF(\n",
    "\t\t\talgorithm = \"lbfgs\",\n",
    "\t\t\tc1 = 1.587763,\n",
    "\t\t\tc2 = 0.149853957,\n",
    "\t\t\tmax_iterations = max_iter,\n",
    "\t\t\tall_possible_transitions = True,\n",
    "\t\t\tverbose = verbose_on\n",
    "\t\t)\n",
    "\t\tcrf.fit(feature_lists, label_lists)\n",
    "\t\t\n",
    "\t\tfor i in range(len(feature_dicts_to_predict)):\n",
    "\t\t\tpredictions = crf.predict(feature_dicts_to_predict[i])\n",
    "\t\t\tprint(gen_tok_NER_pair_lists(sentences_to_predict[i], predictions))\n",
    "\t\t\n",
    "\t\t#predictions = crf.predict(feature_dicts_to_predict)\n",
    "\t\tprint(\"TASK 3 PREDICTIONS:\")\n",
    "\t\t#print(gen_tok_NER_pair_lists(sentences_to_predict, predictions))\n",
    "\t\treturn {}\n",
    "\t\t#return extract_entities_from_sequences(\n",
    "\t\t#\t[\"CARDINAL\", \"ORDINAL\", \"DATE\", \"NORP\"], predictions, True)\n",
    "\n",
    "\t#try seeing if different hyperparamaters for each model improves\n",
    "\t#their respsective performances, might be worth them not both being the same\n",
    "\tdef task4(ontonotes_data, sentences_to_predict):\n",
    "\t\t\n",
    "\t\tprint(\"Task 4 sentences to predict length:\", len(sentences_to_predict))\n",
    "\t\tfeature_lists = [gen_task4_features(sent) for sent in ontonotes_data]\n",
    "\t\tfeature_dicts_to_predict = [gen_task4_features(sent) for sent in sentences_to_predict]\n",
    "\t\tprint(\"The first feature dict in the first feature dict list is:\")\n",
    "\t\tprint(feature_lists[0][0])\n",
    "\t\tprint(\"The first feature dict in the first feature dict list (the ones that should be predicted) is:\")\n",
    "\t\tprint(feature_dicts_to_predict[0][0])\n",
    "\t\tlabel_lists = [sent2labels(sent) for sent in ontonotes_data]\n",
    "\t\tcrf = sklearn_crfsuite.CRF(\n",
    "\t\t\talgorithm = \"lbfgs\",\n",
    "\t\t\tc1 = 1.587763,\n",
    "\t\t\tc2 = 0.149853957,\n",
    "\t\t\tmax_iterations = max_iter,\n",
    "\t\t\tall_possible_transitions = True,\n",
    "\t\t\tverbose = verbose_on\n",
    "\t\t)\n",
    "\t\tcrf.fit(feature_lists, label_lists)\n",
    "\t\t#predictions = crf.predict(feature_dicts_to_predict)\n",
    "\t\tprint(\"TASK 4 PREDICTIONS:\")\n",
    "\t\t#print(gen_tok_NER_pair_lists(sentences_to_predict, predictions))\n",
    "\t\treturn {}\n",
    "\t\t#return extract_entities_from_sequences([\"PERSON\"], predictions, False)\n",
    "\n",
    "\t#Think this is all that my compie can handle :)\n",
    "\ttask3_processed = random_sample(task3_processed, sample_size)\n",
    "\ttask4_processed = random_sample(task4_processed, sample_size)\n",
    "\t\n",
    "\tprint(\"TASK 3 SIZE:\")\n",
    "\tprint(len(task3_processed))\n",
    "\t\n",
    "\tprint(\"TASK 4 SIZE:\")\n",
    "\tprint(len(task4_processed))\n",
    "\t\n",
    "\tner_dict = task3(task3_processed, sentences_to_predict)\n",
    "\tner_dict.update(task4(task4_processed, sentences_to_predict))\n",
    "\n",
    "\t#phewwww finally :)\n",
    "\treturn ner_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-93e1dfce0ca2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0montonotes_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ontonotes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ontonotes_parsed.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-01c57ce7e077>\u001b[0m in \u001b[0;36mload_ontonotes\u001b[0;34m(dataset_file)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mstr_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreadHandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mreadHandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mdict_ontonotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0morig_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_ontonotes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#DOOOOOO NOT RUN THIS CELl!\n",
    "#(unless its the first time opening the notebook though obviously)\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "ontonotes_sentences = load_ontonotes(\"ontonotes_parsed.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\t# DO NOT CHANGE THE BELOW CODE WHICH WILL SERIALIZE THE ANSWERS FOR THE AUTOMATED TEST HARNESS TO LOAD AND MARK\\n\\n\\t# write out all PERSON entries for character list for subtask 4\\n\\twriteHandle = codecs.open( 'characters.txt', 'w', 'utf-8', errors = 'replace' )\\n\\tif 'PERSON' in dictNE :\\n\\t\\tfor strNE in dictNE['PERSON'] :\\n\\t\\t\\twriteHandle.write( strNE.strip().lower()+ '\\n' )\\n\\twriteHandle.close()\\n\\n\\t# FILTER NE dict by types required for subtask 3\\n\\tlistAllowedTypes = [ 'DATE', 'CARDINAL', 'ORDINAL', 'NORP' ]\\n\\tlistKeys = list( dictNE.keys() )\\n\\tfor strKey in listKeys :\\n\\t\\tfor nIndex in range(len(dictNE[strKey])) :\\n\\t\\t\\tdictNE[strKey][nIndex] = dictNE[strKey][nIndex].strip().lower()\\n\\t\\tif not strKey in listAllowedTypes :\\n\\t\\t\\tdel dictNE[strKey]\\n\\n\\t# write filtered NE dict\\n\\twriteHandle = codecs.open( 'ne.json', 'w', 'utf-8', errors = 'replace' )\\n\\tstrJSON = json.dumps( dictNE, indent=2 )\\n\\twriteHandle.write( strJSON + '\\n' )\\n\\twriteHandle.close()\\n\\t\""
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Used for taking the chapter file and ontonotes dataset as an argument\n",
    "#then training the task 3 and task 4 models and writing the dictionary\n",
    "#results for each of them\n",
    "\n",
    "#all you have to do is fill up dictNE as is shown commented out and\n",
    "#the function handles writing everything for you\n",
    "def exec_ner( file_chapter = None, ontonotes_file = None ) :\n",
    "\n",
    "\t#First read the whole chapter in as a string\n",
    "\tchapter_str = \"\"\n",
    "\twith open(file_chapter, 'r') as f:\n",
    "\t\tchapter_str = f.read()\n",
    "\t\n",
    "\tsentences_to_predict = []\n",
    "\tfor sent in sent_tokenize(chapter_str):\n",
    "\t\ttokens = word_tokenize(sent)\n",
    "\t\ttags = list(map(lambda x: x[0], pos_tag(tokens)))\n",
    "\t\tsentence = []\n",
    "\t\t#Start with initially all \"O\" tags (these are then replaced by NER obviously)\n",
    "\t\tfor i in range(len(tokens)):\n",
    "\t\t\tsentence.append((tokens[i], tags[i], \"O\"))\n",
    "\t\tsentences_to_predict.append(sentence)\n",
    "\t\n",
    "\t#ontonotes_sentences = load_ontonotes(ontonotes_file)\n",
    "\tdictNE = run_NER(ontonotes_sentences, sentences_to_predict)\n",
    "\t\n",
    "\t# INSERT CODE TO TRAIN A CRF NER MODEL TO TAG THE CHAPTER OF TEXT (subtask 3)\n",
    "\t# USING NER MODEL AND REGEX GENERATE A SET OF BOOK CHARACTERS AND FILTERED SET OF NE TAGS (subtask 4)\n",
    "\n",
    "\t# hardcoded output to show exactly what is expected to be serialized (you should change this)\n",
    "#\tdictNE = {\n",
    "#\t\t\t\"CARDINAL\": [\n",
    "#\t\t\t\t\"two\",\n",
    "#\t\t\t\t\"three\",\n",
    "#\t\t\t\t\"one\"\n",
    "#\t\t\t],\n",
    "#\t\t\t\"ORDINAL\": [\n",
    "#\t\t\t\t\"first\"\n",
    "#\t\t\t],\n",
    "#\t\t\t\"DATE\": [\n",
    "\t#\t\t\t\"saturday\",\n",
    "#\t\t\t],\n",
    "#\t\t\t\"NORP\": [\n",
    "#\t\t\t\t\"indians\"\n",
    "#\t\t\t],\n",
    "#\t\t\t\"PERSON\": [\n",
    "#\t\t\t\t\"creakle\",\n",
    "#\t\t\t\t\"mr. creakle\",\n",
    "#\t\t\t\t\"mrs. creakle\",\n",
    "#\t\t\t\t\"miss creakle\"\n",
    "#\t\t\t]\n",
    "#\t\t}\n",
    "\n",
    "\treturn dictNE\n",
    "\n",
    "\"\"\"\n",
    "\t# DO NOT CHANGE THE BELOW CODE WHICH WILL SERIALIZE THE ANSWERS FOR THE AUTOMATED TEST HARNESS TO LOAD AND MARK\n",
    "\n",
    "\t# write out all PERSON entries for character list for subtask 4\n",
    "\twriteHandle = codecs.open( 'characters.txt', 'w', 'utf-8', errors = 'replace' )\n",
    "\tif 'PERSON' in dictNE :\n",
    "\t\tfor strNE in dictNE['PERSON'] :\n",
    "\t\t\twriteHandle.write( strNE.strip().lower()+ '\\n' )\n",
    "\twriteHandle.close()\n",
    "\n",
    "\t# FILTER NE dict by types required for subtask 3\n",
    "\tlistAllowedTypes = [ 'DATE', 'CARDINAL', 'ORDINAL', 'NORP' ]\n",
    "\tlistKeys = list( dictNE.keys() )\n",
    "\tfor strKey in listKeys :\n",
    "\t\tfor nIndex in range(len(dictNE[strKey])) :\n",
    "\t\t\tdictNE[strKey][nIndex] = dictNE[strKey][nIndex].strip().lower()\n",
    "\t\tif not strKey in listAllowedTypes :\n",
    "\t\t\tdel dictNE[strKey]\n",
    "\n",
    "\t# write filtered NE dict\n",
    "\twriteHandle = codecs.open( 'ne.json', 'w', 'utf-8', errors = 'replace' )\n",
    "\tstrJSON = json.dumps( dictNE, indent=2 )\n",
    "\twriteHandle.write( strJSON + '\\n' )\n",
    "\twriteHandle.close()\n",
    "\t\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test each of the subtasks showing all the dictionary results\n",
    "#use eval_book and eval_chapter\n",
    "def do_testing():\n",
    "    \n",
    "\t#task1dict = exec_regex_toc(\"eval_book.txt\")\n",
    "\tprint(\"---TASK 1 DICTIONARY RESULTS---\")\n",
    "\t#print(task1dict)\n",
    "\t#task2question_set = exec_regex_questions(\"eval_chapter.txt\")\n",
    "\tprint(\"---TASK 2 QUESTIONS FOUND---\")\n",
    "\t#print(task2question_set)\n",
    "\ttask3and4_NER_dict = exec_ner(\"eval_chapter.txt\", \"ontonotes_parsed.json\")\n",
    "\tprint(\"---TASK 3 and 4 DICTIONARY FOUND---\")\n",
    "\tprint(task3and4_NER_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---TASK 1 DICTIONARY RESUTLS---\n",
      "---TASK 2 QUESTIONS FOUND---\n",
      "TASK 3 SIZE:\n",
      "10000\n",
      "TASK 4 SIZE:\n",
      "10000\n",
      "Task 3 sentences to predict length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite:   1%|          | 123/10000 [00:00<00:08, 1229.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first feature dict in the first feature dict list is:\n",
      "{'word.lower()': 'that', 'word_prefix': 'Tha', 'word_suffix': 'hat', 'postag': 'DT', 'istitle': True, 'isupper': False, 'contains_digit': False, 'contains_day': False, 'in_num_gaz': False, 'in_ord_gaz': False, 'in_date_gaz': False, 'in_time_gaz': False, 'is_st_th': False, '+1:word.lower()': 'would', '+1:word_prefix': 'wou', '+1:word_suffix': 'uld', '+1:postag': 'MD', '+1:istitle': False, '+1:isupper': False, '+1:contains_digit': False, '+1:contains_day': False, '+1:in_num_gaz': False, '+1:in_ord_gaz': False, '+1:in_date_gaz': False, '+1:in_time_gaz': False, '+1:is_st_th': False, '+2:word.lower()': 'be', '+2:word_prefix': 'be', '+2:word_suffix': 'be', '+2:postag': 'VB', '+2:istitle': False, '+2:isupper': False}\n",
      "The first feature dict in the first feature dict list (the ones that should be predicted) is:\n",
      "{'word.lower()': '\\ufeffchapter', 'word_prefix': '\\ufeffCH', 'word_suffix': 'TER', 'postag': ('\\ufeffCHAPTER', 'RB'), 'istitle': False, 'isupper': True, 'contains_digit': False, 'in_nltk_name_gaz': False, '+1:word.lower()': '6', '+1:word_prefix': '6', '+1:word_suffix': '6', '+1:postag': ('6', 'CD'), '+1:istitle': False, '+1:isupper': False, '+1:contains_digit': True, '+1:in_nltk_name_gaz': False, '+2:word.lower()': '.', '+2:word_prefix': '.', '+2:word_suffix': '.', '+2:postag': ('.', '.'), '+2:istitle': False, '+2:isupper': False}\n",
      "Made the task 3 feature lists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 10000/10000 [00:08<00:00, 1153.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 196071\n",
      "Seconds required: 1.527\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 1.587763\n",
      "c2: 0.149854\n",
      "num_memories: 6\n",
      "max_iterations: 20\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=1.03  loss=334278.63 active=115056 feature_norm=1.00\n",
      "Iter 2   time=1.01  loss=231508.89 active=96349 feature_norm=5.41\n",
      "Iter 3   time=0.50  loss=197682.49 active=69247 feature_norm=4.78\n",
      "Iter 4   time=3.03  loss=100394.32 active=48203 feature_norm=3.28\n",
      "Iter 5   time=0.51  loss=80481.46 active=43757 feature_norm=4.72\n",
      "Iter 6   time=0.50  loss=53371.30 active=38915 feature_norm=8.14\n",
      "Iter 7   time=0.52  loss=42740.43 active=35744 feature_norm=11.20\n",
      "Iter 8   time=1.03  loss=38433.66 active=36954 feature_norm=12.72\n",
      "Iter 9   time=0.53  loss=35910.12 active=37215 feature_norm=13.31\n",
      "Iter 10  time=0.52  loss=32799.17 active=35878 feature_norm=14.84\n",
      "Iter 11  time=0.53  loss=30341.30 active=34165 feature_norm=15.96\n",
      "Iter 12  time=0.62  loss=28533.09 active=32851 feature_norm=17.03\n",
      "Iter 13  time=0.54  loss=25582.28 active=29948 feature_norm=19.40\n",
      "Iter 14  time=0.54  loss=24673.75 active=25150 feature_norm=23.70\n",
      "Iter 15  time=0.52  loss=21301.33 active=25555 feature_norm=25.35\n",
      "Iter 16  time=0.54  loss=20625.81 active=25152 feature_norm=25.99\n",
      "Iter 17  time=0.53  loss=19498.84 active=23193 feature_norm=27.97\n",
      "Iter 18  time=0.52  loss=18500.84 active=21307 feature_norm=30.48\n",
      "Iter 19  time=0.52  loss=17931.06 active=21077 feature_norm=31.70\n",
      "Iter 20  time=0.51  loss=17281.18 active=21431 feature_norm=32.11\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 14.570\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 21431 (196071)\n",
      "Number of active attributes: 9898 (145605)\n",
      "Number of active labels: 9 (9)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.008\n",
      "\n",
      "Thing is: (testing)\n",
      "(0, '\\ufeffCHAPTER')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'token' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-8199a7e86655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdo_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-8377ce2b0a28>\u001b[0m in \u001b[0;36mdo_testing\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---TASK 2 QUESTIONS FOUND---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m#print(task2question_set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtask3and4_NER_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexec_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"eval_chapter.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ontonotes_parsed.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---TASK 3 and 4 DICTIONARY FOUND---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask3and4_NER_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-d5073b7bed81>\u001b[0m in \u001b[0;36mexec_ner\u001b[0;34m(file_chapter, ontonotes_file)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m#ontonotes_sentences = load_ontonotes(ontonotes_file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mdictNE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_NER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0montonotes_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_to_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# INSERT CODE TO TRAIN A CRF NER MODEL TO TAG THE CHAPTER OF TEXT (subtask 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-7b4a8614c65e>\u001b[0m in \u001b[0;36mrun_NER\u001b[0;34m(sentences, sentences_to_predict)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask4_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0mner_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask3_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_to_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0mner_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask4_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_to_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-7b4a8614c65e>\u001b[0m in \u001b[0;36mtask3\u001b[0;34m(ontonotes_data, sentences_to_predict)\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_dicts_to_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_dicts_to_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_tok_NER_pair_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences_to_predict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m                 \u001b[0;31m#predictions = crf.predict(feature_dicts_to_predict)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-89-56305e9982bc>\u001b[0m in \u001b[0;36mgen_tok_NER_pair_lists\u001b[0;34m(sentences_with_unknown_NER, NER_predictions)\u001b[0m\n\u001b[1;32m      9\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Thing is: (testing)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                         \u001b[0mpair_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNER_predictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mpair_lists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'token' is not defined"
     ]
    }
   ],
   "source": [
    "do_testing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
