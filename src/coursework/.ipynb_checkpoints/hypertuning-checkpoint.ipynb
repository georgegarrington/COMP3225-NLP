{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2021-05-12 11:12:48,527 logging started\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import sys, codecs, json, math, time, warnings, re, logging\n",
    "warnings.simplefilter( action='ignore', category=FutureWarning )\n",
    "\n",
    "import nltk, numpy, scipy, sklearn, sklearn_crfsuite, sklearn_crfsuite.metrics\n",
    "\n",
    "LOG_FORMAT = ('%(levelname) -s %(asctime)s %(message)s')\n",
    "logger = logging.getLogger( __name__ )\n",
    "logging.basicConfig( level=logging.INFO, format=LOG_FORMAT )\n",
    "logger.info('logging started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_on = False\n",
    "max_iter = 100\n",
    "task3_sample_size = 20000\n",
    "task4_sample_size = 20000\n",
    "optimization_its = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only print if verbose_on is true\n",
    "def printfn(thing): \n",
    "    if verbose_on: print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the match result clean multilines\n",
    "#if there is a multiline between 2 non-space characters then replace with space\n",
    "#if there is a newline with a space either side of it then just remove it\n",
    "def clean_multilines(unclean):\n",
    "    \n",
    "    cleaning = []\n",
    "    cleaning[:] = unclean.strip()\n",
    "    \n",
    "    i = 1\n",
    "    end_index = len(cleaning) - 1\n",
    "    \n",
    "    while i < end_index:\n",
    "        \n",
    "        if(cleaning[i] != '\\n'):\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        space_nb = cleaning[i - 1] == ' ' or cleaning[i + 1] == ' '\n",
    "        newline_nb = cleaning[i - 1] == '\\n' or cleaning[i + 1] == '\\n'\n",
    "        \n",
    "        if space_nb or newline_nb:\n",
    "            del cleaning[i]\n",
    "            end_index -= 1\n",
    "        else:\n",
    "            cleaning[i] = ' ' #replace with space\n",
    "            i += 1\n",
    "    \n",
    "    return \"\".join(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tell if they are 2 different number systems i.e. arabic and roman numerals\n",
    "def diff_num_systems(str1, str2):\n",
    "    return (str1.isdigit() and (not str2.isdigit())) or ((not str1.isdigit()) and str2.isdigit())\n",
    "\n",
    "def extract_toc(chapter_str):\n",
    "    \n",
    "    regex = \"\".join([\n",
    "        \n",
    "        #chapter marker\n",
    "        r\"(?:CHAPTER|Chapter)\", \n",
    "        \n",
    "        #space between chapter and number\n",
    "        r\"[ ]*\", \n",
    "        \n",
    "        #chapter number identifier, either digits or roman numerals\n",
    "        r\"([IVXLCDM]+|\\d+)\", \n",
    "        \n",
    "        #space between number then possibly a '.' symbol, then any number of new lines between\n",
    "        #chapter/number and the title\n",
    "        r\"[ ]*\\.?[ ]*\\n*[ ]*\",\n",
    "        \n",
    "        #many of newline followed by not a new line or a non-new line character forming\n",
    "        #part of the title, to allow for it to span across multiple lines   \n",
    "        #encapsulate the whole thing in a capture group to get the title in a group\n",
    "        r\"((?:\\n(?!\\n)|.)*)\",\n",
    "        \n",
    "        #end of a chapter declaration is 2 new lines or another chapter keyword (if in table of contents)\n",
    "        r\"(?:CHAPTER|Chapter|\\n\\n)\"\n",
    "        \n",
    "    ])\n",
    "    \n",
    "    match_pairs = [mobj.groups() for mobj in re.finditer(regex, chapter_str, flags = re.UNICODE | re.MULTILINE)]\n",
    "    \n",
    "    #the table of contents dictionary\n",
    "    toc = {}\n",
    "    first_num, first_title = match_pairs[0]\n",
    "    toc[first_num] = clean_multilines(first_title)\n",
    "    \n",
    "    i = 1\n",
    "    \n",
    "    while i < len(match_pairs):\n",
    "        \n",
    "        num, title = match_pairs[i]\n",
    "        \n",
    "        if num == first_num or diff_num_systems(num, first_num):\n",
    "            break\n",
    "        \n",
    "        toc[num] = clean_multilines(title)\n",
    "        i += 1\n",
    "    \n",
    "    #If we have already looked at all chapters (i.e. there was no table of contents detected)\n",
    "    #then just return it straight away\n",
    "    if i == len(match_pairs): return toc\n",
    "    \n",
    "    #the index will now start after table of contents\n",
    "    \n",
    "    #the chapters that appear in the book\n",
    "    chapters = {}\n",
    "    first_chapter_num, first_chapter_title = match_pairs[i]\n",
    "    chapters[first_chapter_num] = clean_multilines(first_chapter_title)\n",
    "    \n",
    "    while i < len(match_pairs):\n",
    "        \n",
    "        num, title = match_pairs[i]\n",
    "        chapters[num] = clean_multilines(title)\n",
    "        i += 1\n",
    "    \n",
    "    return chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given an entire chapter that has been read as a string, \n",
    "#extract all questions from it\n",
    "def extract_questions(chapter_str):\n",
    "    \n",
    "    #Start with a letter, any number of non-sentence ending characters followed by a question mark\n",
    "    #Check there was the end of a sentence before the letter i.e. some puncuation mark\n",
    "    regex = r\"(?:(?<=([‘“\\\"\\'\\.\\?\\!]))[ ]*)([A-Za-z][^\\?\\.!]*\\?)\"\n",
    "    matches = re.findall(regex, chapter_str, flags = re.DOTALL | re.UNICODE)\n",
    "    \n",
    "    #Now check if any of the questions contain questions themselves \n",
    "    #e.g. the Traddles example\n",
    "    inner_matches = []\n",
    "    for _, match in matches:\n",
    "        inner_matches += re.findall(regex, match, flags = re.DOTALL | re.UNICODE)\n",
    "    \n",
    "    matches += inner_matches\n",
    "    \n",
    "    return set(clean_multilines(match) for _, match in matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_gazetteer = set([\n",
    "    \"mr\", \"mr.\", \"mrs\", \"mrs.\", \"miss\", \"miss.\", \"madam\", \"ms\", \"ms.\",\n",
    "    \"mz\", \"mz.\", \"mam.\", \"mam\", \"sir\", \"sir.\", \"lord\", \"lord.\", \"mam\", \"mister\", \n",
    "    \"mister.\", \"missus\", \"missus.\", \"dame\", \"chairman\", \"king\", \"maam\", \"maam.\"\n",
    "    \"queen\", \"president\", \"mm\", \"mm.\", \"prince\", \"princess\", \"duke\", \"duchess\"])\n",
    "\n",
    "\n",
    "#The ontonotes dataset does not label mr miss etc. as B-PERSON it labels\n",
    "#it as O instead which is not what we want, so go through and change all the tags\n",
    "#of mr and mrs etc. to B-PERSON\n",
    "\n",
    "#Do this all in-place so it modifies the list that its called on\n",
    "def fix_mr_mrs(sentences):\n",
    "        \n",
    "    for sent in sentences:\n",
    "    \n",
    "        for i in range(len(sent)):\n",
    "\n",
    "            #print(\"sentences[i]:\")\n",
    "            #print(sentences[i])\n",
    "            tok, pos, nertag = sent[i]\n",
    "\n",
    "            if nertag != \"B-PERSON\":\n",
    "                continue\n",
    "\n",
    "            #at least two words behind\n",
    "            if i > 1:\n",
    "                tokm1, posm1, _ = sent[i - 1]\n",
    "                tokm2, posm2, _ = sent[i - 2]\n",
    "\n",
    "                if tokm1.lower() in title_gazetteer:\n",
    "                    sent[i - 1] = tokm1, posm1, \"B-PERSON\"\n",
    "                    sent[i] = tok, pos, \"I-PERSON\"\n",
    "                elif tokm2.lower() in title_gazetteer:\n",
    "                    sent[i - 2] = tokm2, posm2, \"B-PERSON\"\n",
    "                    sent[i - 1] = tokm1, posm1, \"I-PERSON\"\n",
    "                    sent[i] = tok, pos, \"I-PERSON\"\n",
    "\n",
    "            #at least one word behind\n",
    "            if i > 0:\n",
    "                tokm1, posm1, _ = sent[i - 1]\n",
    "\n",
    "                if tokm1.lower() in title_gazetteer:\n",
    "                    sent[i - 1] = tokm1, posm1, \"B-PERSON\"\n",
    "                    sent[i] = tok, pos, \"I-PERSON\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#They should be exactly the same length as the predictions have been made from them\n",
    "def gen_tok_NER_pair_lists(tag_list, sentence_with_unknown_NER, NER_predictions):\n",
    "    \n",
    "    \n",
    "    tokens = [tok for (tok,_,_) in sentence_with_unknown_NER]\n",
    "    \n",
    "    pair_list = []\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(NER_predictions):\n",
    "\n",
    "        if(NER_predictions[i] == \"O\"):\n",
    "            i += 1\n",
    "            continue\n",
    "        \n",
    "        #if(not (NER_predictions[i][2:] in tag_list)):\n",
    "        #    i += 1\n",
    "        #    continue\n",
    "        \n",
    "        #remove the B- or I-\n",
    "        actual_tag = NER_predictions[i][2:]\n",
    "        \n",
    "        #The indices will correspond, start the entity with the first token with the tag\n",
    "        entity = tokens[i]\n",
    "        i += 1\n",
    "        \n",
    "        while i < len(NER_predictions) and NER_predictions[i][2:] == actual_tag:\n",
    "            entity += \" \" + tokens[i]\n",
    "            i += 1\n",
    "        \n",
    "        pair_list.append((entity, actual_tag))\n",
    "        i += 1\n",
    "    \n",
    "    return pair_list\n",
    "\n",
    "def gen_NER_dict(tag_list, sentences, sentence_NER_predictions):\n",
    "    \n",
    "    dct = {tag : [] for tag in tag_list}\n",
    "    \n",
    "    for (sentence, predictions) in zip(sentences,sentence_NER_predictions):\n",
    "        pairs = gen_tok_NER_pair_lists(tag_list, sentence, predictions)\n",
    "        for (entity, tag) in pairs:\n",
    "            dct[tag].append(entity.lower())\n",
    "    \n",
    "    dct = {tag : list(set(entities)) for (tag,entities) in dct.items()}\n",
    "    return dct\n",
    "\n",
    "#turn the list of tokens into a sentence that can be used for NER\n",
    "def prepare_sentence(tokens):\n",
    "    tags = list(map(lambda x: x[1], pos_tag(tokens)))\n",
    "    sentence = []\n",
    "    for i in range(len(tokens)):\n",
    "        sentence.append((tokens[i],tags[i],\"O\"))\n",
    "    return sentence\n",
    "\n",
    "#detect if the chapter string has \"CHAPTER . SOMETHING\" at the start of it\n",
    "#and return the string without it\n",
    "def without_chapter(chapter_str):\n",
    "    \n",
    "    #Define the end of a title as there being at least 2 new line characters\n",
    "    regex = r\"(\\n|[ ])*.*(?:CHAPTER|(C|c)hapter)[ ]*\\.?(?:[ivxlcdm]+|[IVXLCDM]+|\\d+)[ ]*\\.?(\\n|[ ])*.*\\n.*\\n(\\n|[ ])*\"\n",
    "    result = re.match(regex, chapter_str, flags = re.MULTILINE | re.UNICODE)\n",
    "    \n",
    "    return chapter_str[len(result[0]):]\n",
    "    \n",
    "    #if there is a Chapter N part at the start of the chapter then remove it\n",
    "    #length = len(result[0])\n",
    "    #return chapter_str[length:]\n",
    "\n",
    "#take the file path of ontonotes and return sentences\n",
    "def load_ontonotes(dataset_file):\n",
    "\n",
    "    readHandle = codecs.open(dataset_file, 'r', 'utf-8', errors = 'replace')\n",
    "    str_json = readHandle.read()\n",
    "    readHandle.close()\n",
    "    dict_ontonotes = json.loads(str_json)\n",
    "    orig_list = list(dict_ontonotes.keys())\n",
    "    sentences = []    \n",
    "    for str_file in orig_list:\n",
    "        for str_sent_index in dict_ontonotes[str_file] :\n",
    "\n",
    "            tokens = []\n",
    "            ner_tags = []\n",
    "            # compute IOB tags for named entities (if any)\n",
    "            ne_type_last = None\n",
    "\n",
    "            #build up the list of tokens and ner tags\n",
    "            for nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])) :\n",
    "                strToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "                strPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "                ne_type = None\n",
    "                if 'ne' in dict_ontonotes[str_file][str_sent_index] :\n",
    "                    dict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "                    if not 'parse_error' in dict_ne :\n",
    "                        for str_NEIndex in dict_ne :\n",
    "                            if nTokenIndex in dict_ne[str_NEIndex]['tokens'] :\n",
    "                                ne_type = dict_ne[str_NEIndex]['type']\n",
    "                                break\n",
    "                if ne_type != None :\n",
    "                                        \n",
    "                    if ne_type == ne_type_last :\n",
    "                        strIOB = 'I-' + ne_type\n",
    "                    else :\n",
    "                        strIOB = 'B-' + ne_type\n",
    "                else :\n",
    "                    strIOB = 'O'\n",
    "\n",
    "                ne_type_last = ne_type\n",
    "                tokens.append(strToken)\n",
    "                ner_tags.append(strIOB)\n",
    "            list_entry = []\n",
    "\n",
    "            #use nltk pos tags instead\n",
    "            for (index, tup) in enumerate(pos_tag(tokens)):\n",
    "                list_entry.append((tup[0], tup[1], ner_tags[index]))\n",
    "            sentences.append(list_entry)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedShuffleSplit, ShuffleSplit\n",
    "\n",
    "#helper functions\n",
    "def sent2labels(sentence):\n",
    "    return [label for _, _, label in sentence] \n",
    "\n",
    "# In[56]:\n",
    "\n",
    "\n",
    "def optimize(sentences, n_its, dictmaker):\n",
    "    \n",
    "    #feature dictionaries for each word in a sentence\n",
    "    feature_lists = [dictmaker(sent) for sent in sentences]\n",
    "    print(\"length of the feature lists:\",len(feature_lists))\n",
    "    print(\"First element:\",feature_lists[0])\n",
    "\n",
    "    #NER labels for each word in a sentence\n",
    "    label_lists = [sent2labels(sent) for sent in sentences]\n",
    "    print(\"length of the label lists:\", len(label_lists))\n",
    "    print(\"First element:\",label_lists[0])\n",
    "    \n",
    "    crf = sklearn_crfsuite.CRF(\n",
    "        algorithm = \"lbfgs\",\n",
    "        max_iterations = max_iter,\n",
    "        all_possible_transitions = True,\n",
    "        verbose = False\n",
    "    )\n",
    "    \n",
    "    params_space = {\n",
    "        #let's try a 50/50 chance of using either random variable so we can see if the larger/smaller ones are better\n",
    "        'c1': scipy.stats.expon(scale = 2), #if np.random.choice([True,False], p = [0.5,0.5]) else scipy.stats.expon(scale = 50),\n",
    "        'c2': scipy.stats.expon(scale = 0.1)\n",
    "    }\n",
    "    \n",
    "    f1_scorer = make_scorer(metrics.flat_f1_score, average = \"weighted\", labels = display_label_subset)\n",
    "    \n",
    "    rs = RandomizedSearchCV(\n",
    "        crf,\n",
    "        params_space,\n",
    "        cv = ShuffleSplit(n_splits = 2, test_size = 0.1, train_size = 0.9), #, train_size = 0.45, test_size = 0.05),\n",
    "        verbose = 30,\n",
    "        n_jobs = -1,\n",
    "        n_iter = n_its,\n",
    "        scoring = f1_scorer\n",
    "    )\n",
    "    \n",
    "    rs.fit(feature_lists, label_lists)\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "from copy import copy\n",
    "from random import shuffle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the ontonotes data sentences and the sentences \n",
    "#to predict from reading the chapter file\n",
    "def run_NER(sentences):\n",
    "    \n",
    "        #given a specific NER tag, return 2 sets consisting of the \n",
    "    #\"inside parts\" and the \"before parts\"\n",
    "    def extract_gazetteers(target, sentences):\n",
    "        before = set()\n",
    "        inside = set()\n",
    "        before_tag = \"B-\" + target\n",
    "        inside_tag = \"I-\" + target\n",
    "        for sent in sentences:\n",
    "            for tok, _, ner in sent:\n",
    "                if ner == before_tag:\n",
    "                    before.add(tok.lower())\n",
    "                if ner == inside_tag:\n",
    "                    inside.add(tok.lower())\n",
    "        return before, inside\n",
    "\n",
    "\n",
    "    #convert all the NER tags you don't care about into O's in sentences that may have mixed NER tags that we care about\n",
    "    #with NER tags that we don't care about\n",
    "    def only_Os(desired_set, sentences):\n",
    "        actual_tags = []\n",
    "        for tag in desired_set:\n",
    "            actual_tags.append(\"B-\" + tag)\n",
    "            actual_tags.append(\"I-\" + tag)\n",
    "        return [[(tok, pos, ner if ner in actual_tags else 'O') for tok, pos, ner in sent] for sent in sentences]\n",
    "\n",
    "\n",
    "    #If the dataset has already been split and we KNOW that none of these NER tags are the ones we care about,\n",
    "    #then clearly we can just replace ALL NER tags with O\n",
    "    def all_Os(sentences):\n",
    "        return [[(tok, pos, 'O') for tok, pos, _ in sent] for sent in sentences]\n",
    "\n",
    "\n",
    "    #returns a tuples of 2 lists of sentences, where one list contains all the sentences that contain the tags \n",
    "    #specified in the argument and the other list does not\n",
    "    #you just give it the tag strings and it adds the Bs and Is for you\n",
    "    def split_tags(tag_set, sentences):\n",
    "\n",
    "        actual_tags = []\n",
    "        for tag in tag_set:\n",
    "            actual_tags.append(\"B-\" + tag)\n",
    "            actual_tags.append(\"I-\" + tag)\n",
    "\n",
    "        contains_tags = []\n",
    "        doesnt_contain_tags = []\n",
    "\n",
    "        for sent in sentences:\n",
    "\n",
    "            if(any([tag in actual_tags for _,_,tag in sent])):\n",
    "                contains_tags.append(sent)\n",
    "            else:\n",
    "                doesnt_contain_tags.append(sent)\n",
    "\n",
    "        return (contains_tags, doesnt_contain_tags)\n",
    "\n",
    "    #the sentences containing task 3 tags and the sentences that dont\n",
    "    wtask3, wotask3 = split_tags([\"DATE\", \"CARDINAL\", \"ORDINAL\", \"NORP\"], sentences)\n",
    "\n",
    "\n",
    "    #the sentences containing task 4 tags (only person) and the sentences that dont\n",
    "    wtask4, wotask4 = split_tags([\"PERSON\"], sentences)\n",
    "\n",
    "    #all other tags can be converted to O because we dont care about finding them\n",
    "    wtask3 = only_Os([\"DATE\", \"CARDINAL\", \"ORDINAL\", \"NORP\"], wtask3)\n",
    "    wtask4 = only_Os([\"PERSON\"], wtask4)\n",
    "\n",
    "    #Any sentences that don't contain any entities we care about can\n",
    "    #ALL be converted to Os!\n",
    "    #wotask3 = all_Os(wotask3)\n",
    "    #wotask4 = all_Os(wotask4)\n",
    "\n",
    "    del wotask3\n",
    "    del wotask4\n",
    "    del sentences\n",
    "    gc.collect()\n",
    "    \n",
    "        #take a random sample with the specified size from the list of sentences\n",
    "    def random_sample(sentences, num):\n",
    "\n",
    "        if num > len(sentences):\n",
    "            return sentences\n",
    "\n",
    "        cp = copy(sentences)\n",
    "        shuffle(cp)\n",
    "        return cp[-num:]\n",
    "\n",
    "    #process data, putting in a ratio of sentences that contain the NER tags we care about\n",
    "    #and the complement ration of ones we dont\n",
    "    def process_data(care_about, dont_care):\n",
    "\n",
    "        num_care = floor(care_ratio * sample_size)\n",
    "        num_dont_care = sample_size - num_care\n",
    "\n",
    "        care_sample = random_sample(care_about, num_care)\n",
    "        dont_care_sample = random_sample(dont_care, num_dont_care)\n",
    "\n",
    "        processed = care_sample + dont_care_sample\n",
    "        shuffle(processed)\n",
    "        return processed\n",
    "\n",
    "    #task3_processed = process_data(wtask3, wotask3)\n",
    "    #task4_processed = process_data(wtask4, wotask4)\n",
    "\n",
    "\n",
    "    #split the items into training and testing denoted by the testing ratio argument\n",
    "    def random_split(items, ratio):\n",
    "        shuffle(copy(items))\n",
    "        train_size = int(ratio * len(items))\n",
    "        test_size = len(items) - train_size\n",
    "        return items[:train_size], items[-test_size:]\n",
    "\n",
    "    #don't need training and testing for handin, just use the whole thing\n",
    "    #task3_training, task3_testing = random_split(task3_processed, 0.9)\n",
    "    #task4_training, task4_testing = random_split(task4_processed, 0.9)\n",
    "\n",
    "    #generate the gazetteers to be used from ontonotes\n",
    "    #before_name, inside_name = extract_gazetteers(\"PERSON\", wtask4)\n",
    "    #before_date, inside_date = extract_gazetteers(\"DATE\", wtask3)\n",
    "    #before_cardinal, inside_cardinal = extract_gazetteers(\"CARDINAL\", wtask3)\n",
    "    #before_norp, inside_norp = extract_gazetteers(\"NORP\", wtask3)\n",
    "    #before_ord, inside_ord = extract_gazetteers(\"ORDINAL\", wtask3)\n",
    "\n",
    "        #assumed to be in the environment of the function that \n",
    "    #generates the feature dictionary for task 3\n",
    "    number_gazetteer = set([\n",
    "        \"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "        \"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "        \"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"fourty\", \"fifty\",\n",
    "        \"sixty\", \"seventy\", \"eighty\", \"ninety\", \"hundred\", \"onehundred\", \"one-hundred\", \"thousand\", \"million\"\n",
    "        ])\n",
    "\n",
    "    #Python sets are implemented as hash tables so lookup is always around O(1)\n",
    "    #even for large sets\n",
    "\n",
    "    ordinal_gazetteer = set([\n",
    "        \"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\",\n",
    "        \"eigth\", \"ninth\", \"tenth\", \"eleventh\", \"twelth\", \"thirteenth\",\n",
    "        \"fourteenth\", \"fifteenth\", \"sixteenth\", \"seventeenth\", \"eighteenth\",\n",
    "        \"nineteenth\", \"twentieth\", \"thirtieth\", \"fourtieth\", \"fiftieth\", \n",
    "        \"sixtieth\", \"seventieth\", \"eightieth\", \"ninetieth\", \"hundreth\", \"thousandth\",\n",
    "        \"millionth\", \"firstly\", \"secondly\", \"thirdly\", \"fourthly\", \"fifthly\", \"sixthly\",\n",
    "        \"seventhly\", \"eigthly\", \"ninethly\", \"tenthly\"\n",
    "    ])\n",
    "\n",
    "    date_gazetteer = set([\n",
    "        \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\", \"day\", \"days\", \n",
    "        \"week\", \"weeks\", \"month\", \"months\", \"year\", \"years\", \"decade\", \"decades\", \"century\", \"centuries\", \n",
    "        \"millennium\", \"millennia\", \"jan\", \"january\", \"feb\", \"february\", \"mar\", \"march\", \"apr\", \"april\", \n",
    "        \"may\", \"jun\", \"june\", \"jul\", \"july\", \"aug\", \"august\", \"sep\", \"september\", \"oct\", \"october\", \n",
    "        \"nov\", \"november\", \"dec\", \"december\", \"holiday\", \"holidays\"\n",
    "    ])\n",
    "\n",
    "    time_gazetteer = set([\"milliseconds\", \"millis\", \"milis\", \"second\", \"seconds\", \"minute\", \"minutes\", \"hour\", \"hours\", \"time\", \"clock\", \"o'clock\", \"past\", \"oclock\"])\n",
    "\n",
    "    # a feature for detecting e.g. 1st, 2nd, 3rd, 4th etc.\n",
    "    def is_st_th(candidate):\n",
    "        suffixes = set([\"st\", \"nd\", \"rd\", \"th\"])\n",
    "        return candidate[-2:] in suffixes and candidate[:-2].isdigit()\n",
    "\n",
    "    #use the nltk name corpus\n",
    "    #Keep all names uppercase as by convention names will only ever really \n",
    "    #appear in books as uppercase, this rules out things like \"will\" being\n",
    "    #detected as a name\n",
    "    name_gazetteer = set(names.words(\"male.txt\") + names.words(\"female.txt\"))\n",
    "\n",
    "    #task 4 has less features as it only revolves around names\n",
    "    def gen_feature_dict_task4(sentence, i):\n",
    "\n",
    "        def gen_features(index, relative_str):\n",
    "            return {\n",
    "                #relative_str + \"word.lower()\": sentence[index][0].lower(),\n",
    "                relative_str + \"word_prefix\": sentence[index][0][:3],\n",
    "                relative_str + \"word_suffix\": sentence[index][0][-3:],\n",
    "                relative_str + \"postag\": sentence[index][1],\n",
    "\n",
    "                #relative_str + \"postag[:2]\": sentence[index][1][:2],\n",
    "                #relative_str + \"is_mr_mrs\": sentence[index][0][:2].lower() == \"mr\" or sentence[index][0][:3].lower() == \"mrs\",\n",
    "                #relative_str + \"in_title_gaz\": sentence[index][0].lower() in title_gazetteer,\n",
    "                relative_str + \"not_alnum\": not sentence[index][0].isalnum(),\n",
    "                relative_str + \"is_single_letter\": sentence[index][0].isalpha() and len(sentence[index][0]) == 1\n",
    "            }\n",
    "\n",
    "        #extra features are limited to center 3 in the context window of size 5\n",
    "        def gen_extra_features(index, relative_str):\n",
    "            return {\n",
    "                relative_str + \"istitle\": sentence[index][0].istitle(),\n",
    "                relative_str + \"isupper\": sentence[index][0].isupper(),\n",
    "                relative_str + \"in_nltk_name_gaz\" :  sentence[index][0] in name_gazetteer\n",
    "            }\n",
    "\n",
    "        features = gen_features(i, \"\")\n",
    "        #features.update({\"word\" : sentence[i][0]})\n",
    "        features.update(gen_extra_features(i, \"\"))\n",
    "\n",
    "        #there is at least 1 word behind\n",
    "        if i > 0:#generate a list of feature dictionaries for each word in the sentence\n",
    "            features.update(gen_features(i - 1, \"-1:\"))\n",
    "            features.update(gen_extra_features(i - 1, \"-1:\"))\n",
    "                    \n",
    "        #there are at least 2 words behind\n",
    "        if i > 1:\n",
    "            features.update(gen_features(i - 2, \"-2:\"))\n",
    "            features.update(gen_extra_features(i - 2, \"-2:\"))\n",
    "\n",
    "        #there are at least 3 words behind\n",
    "        #if i > 2:\n",
    "        #    features.update(gen_features(i - 3, \"-3:\"))\n",
    "            \n",
    "        #there is at least 1 word ahead\n",
    "        if i < len(sentence) - 1:\n",
    "            features.update(gen_features(i + 1, \"+1:\"))\n",
    "            features.update(gen_extra_features(i + 1, \"+1:\"))\n",
    "\n",
    "        #there are at least 2 words ahead\n",
    "        if i < len(sentence) - 2:\n",
    "            features.update(gen_features(i + 2, \"+2:\"))\n",
    "            features.update(gen_extra_features(i + 2, \"+2:\"))\n",
    "\n",
    "        #there are at least 3 words ahead\n",
    "        #if i < len(sentence) - 3:\n",
    "        #    features.update(gen_features(i + 3, \"+3:\"))\n",
    "            \n",
    "        return features\n",
    "\n",
    "    def gen_feature_dict_task3(sentence, i):\n",
    "\n",
    "        #token = sentence[i][0]\n",
    "        #pos_tag = sentence[i][1]\n",
    "\n",
    "        def gen_features(index, relative_str):\n",
    "\n",
    "            return {\n",
    "                #relative_str + \"word.lower()\": sentence[index][0].lower(),\n",
    "                relative_str + \"word_prefix\": sentence[index][0][:3],\n",
    "                relative_str + \"word_suffix\": sentence[index][0][-3:],\n",
    "                relative_str + \"postag\": sentence[index][1],\n",
    "                #relative_str + \"postag[:2]\": sentence[index][1][:2],\n",
    "                relative_str + \"isdigit\": sentence[index][0].isdigit()\n",
    "            }\n",
    "\n",
    "        #extra features are limited to center 3 in the context window of size 5\n",
    "        def gen_extra_features(index, relative_str):\n",
    "            return {\n",
    "                relative_str + \"not_alnum\": not sentence[index][0].isalnum(),\n",
    "                relative_str + \"istitle\": sentence[index][0].istitle(),\n",
    "                relative_str + \"isupper\": sentence[index][0].isupper(),\n",
    "                #python sets are around O(1) lookup so its fine to have large gazetteers\n",
    "                relative_str + \"in_num_gaz\": sentence[index][0].lower() in number_gazetteer,\n",
    "                relative_str + \"in_ord_gaz\": sentence[index][0].lower() in ordinal_gazetteer,\n",
    "                relative_str + \"in_date_gaz\": sentence[index][0].lower() in date_gazetteer,\n",
    "                relative_str + \"in_time_gaz\": sentence[index][0].lower() in time_gazetteer,\n",
    "                #relative_str + \"in_nltk_name_gaz\": sentence[index][0].lower() in name_gazetteer,\n",
    "\n",
    "                #COMMENT THESE OUT MAYBE COS NOT SURE ABOUT THEM\n",
    "    #                relative_str + \"in_B_date\": sentence[index][0].lower() in before_date,\n",
    "    #                relative_str + \"in_I_date\": sentence[index][0].lower() in inside_date,\n",
    "    #                relative_str + \"in_B_card\": sentence[index][0].lower() in before_cardinal,\n",
    "    #                relative_str + \"in_I_card\": sentence[index][0].lower() in inside_cardinal,\n",
    "    #                relative_str + \"in_B_norp\": sentence[index][0].lower() in before_norp,\n",
    "    #                relative_str + \"in_I_norp\": sentence[index][0].lower() in inside_norp,\n",
    "    #                relative_str + \"in_B_ord\": sentence[index][0].lower() in before_ord,\n",
    "    #                relative_str + \"in_I_ord\": sentence[index][0].lower() in inside_ord,\n",
    "\n",
    "                #e.g. 1st, 2nd, 3rd, 4th etc.\n",
    "                relative_str + \"is_st_th\": is_st_th(sentence[index][0])\n",
    "            }\n",
    "\n",
    "        features = gen_features(i, \"\")\n",
    "        #features.update({\"word\" : sentence[i][0]})\n",
    "        features.update(gen_extra_features(i, \"\"))\n",
    "\n",
    "        #there is at least 1 word behind\n",
    "        if i > 0:\n",
    "            features.update(gen_features(i - 1, \"-1:\"))\n",
    "            features.update(gen_extra_features(i - 1, \"-1:\"))\n",
    "\n",
    "        #there are at least 2 words behind\n",
    "        if i > 1:\n",
    "            features.update(gen_features(i - 2, \"-2:\"))\n",
    "            features.update(gen_extra_features(i - 2, \"-2:\"))\n",
    "\n",
    "        #there are at least 3 words behind\n",
    "        #if i > 2:\n",
    "        #    features.update(gen_features(i - 3, \"-3:\"))\n",
    "\n",
    "        #there is at least 1 word ahead\n",
    "        if i < len(sentence) - 1:\n",
    "            features.update(gen_features(i + 1, \"+1:\"))\n",
    "            features.update(gen_extra_features(i + 1, \"+1:\"))\n",
    "\n",
    "        #there are at least 2 words ahead\n",
    "        if i < len(sentence) - 2:\n",
    "            features.update(gen_features(i + 2, \"+2:\"))\n",
    "            features.update(gen_extra_features(i + 2, \"+2:\"))\n",
    "\n",
    "        #there are at least 3 words ahead\n",
    "        #if i < len(sentence) - 3:\n",
    "        #    features.update(gen_features(i + 3, \"+3:\"))\n",
    "\n",
    "        return features\n",
    "\n",
    "\n",
    "    def sent2tokens(sent):\n",
    "        return [token for token, _, _ in sentence]\n",
    "\n",
    "    def sentences2labels(sentences):\n",
    "        return [sent2labels(sent) for sent in sentences]\n",
    "\n",
    "\n",
    "        #generate a list of feature dictionaries for each word in the sentence\n",
    "    def gen_task3_features(sentence):\n",
    "        return [gen_feature_dict_task3(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "    def gen_task4_features(sentence):\n",
    "        return [gen_feature_dict_task4(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "    display_label_subset = [\"B-DATE\", \"I-DATE\", \"B-CARDINAL\", \"I-CARDINAL\", \"B-ORDINAL\", \"I-ORDINAL\", \"B-NORP\", \"I-NORP\"]\n",
    "\n",
    "\n",
    "    #generate lists of feature dictionaries for each sentence\n",
    "    def task3(sentences):\n",
    "        return optimize(sentences, optimization_its, gen_task3_features)\n",
    "        \n",
    "    #try seeing if different hyperparamaters for each model improves\n",
    "    #their respsective performances, might be worth them not both being the same\n",
    "    def task4(sentences):\n",
    "        return optimize(sentences, optimization_its, gen_task4_features)\n",
    "\n",
    "    task3sample = random_sample(wtask3,task3_sample_size)\n",
    "    del wtask3\n",
    "    gc.collect()\n",
    "    task3optimal = task3(task3sample)\n",
    "    \n",
    "    task4sample = random_sample(wtask4,task4_sample_size)\n",
    "    del wtask4\n",
    "    gc.collect()\n",
    "    fix_mr_mrs(task4sample)\n",
    "    task4optimal = task4(task4sample)\n",
    "\n",
    "    return task3optimal, task4optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_ner( file_chapter = None, ontonotes_file = None ) :\n",
    "\n",
    "    # INSERT CODE TO TRAIN A CRF NER MODEL TO TAG THE CHAPTER OF TEXT (subtask 3)\n",
    "    # USING NER MODEL AND REGEX GENERATE A SET OF BOOK CHARACTERS AND FILTERED SET OF NE TAGS (subtask 4)\n",
    "\n",
    "    chapter_str = \"\"\n",
    "    with open(file_chapter, 'r') as f:\n",
    "        chapter_str = without_chapter(f.read())\n",
    "        \n",
    "    sentences_to_predict = []\n",
    "    for sent in sent_tokenize(chapter_str):    # DO NOT CHANGE THE CODE IN THIS FUNCTION\n",
    "\n",
    "    #\n",
    "    # subtask 1 >> extract chapter headings and create a table of contents from a provided plain text book (from www.gutenberg.org)\n",
    "    # Input >> www.gutenberg.org sourced plain text file for a whole book\n",
    "    # Output >> toc.json = { <chapter_number_text> : <chapter_title_text> }\n",
    "    #\n",
    "\n",
    "        tokens = word_tokenize(sent)\n",
    "        sentences_to_predict.append(prepare_sentence(tokens))\n",
    "    \n",
    "    ontonotes_sentences = load_ontonotes(ontonotes_file)\n",
    "    \n",
    "    dictNE = run_NER(ontonotes_sentences, sentences_to_predict)\n",
    "    \n",
    "    #printfn(\"---NER DICTIONARY---\\n\")\n",
    "    #printfn(dictNE)\n",
    "\n",
    "    # DO NOT CHANGE THE BELOW CODE WHICH WILL SERIALIZE THE ANSWERS FOR THE AUTOMATED TEST HARNESS TO LOAD AND MARK\n",
    "\n",
    "    # write out all PERSON entries for character list for subtask 4\n",
    "    writeHandle = codecs.open( 'characters.txt', 'w', 'utf-8', errors = 'replace' )\n",
    "    if 'PERSON' in dictNE :\n",
    "        for strNE in dictNE['PERSON'] :\n",
    "            writeHandle.write( strNE.strip().lower()+ '\\n' )\n",
    "    writeHandle.close()\n",
    "\n",
    "    # FILTER NE dict by types required for subtask 3\n",
    "    listAllowedTypes = [ 'DATE', 'CARDINAL', 'ORDINAL', 'NORP' ]\n",
    "    listKeys = list( dictNE.keys() )\n",
    "    for strKey in listKeys :\n",
    "        for nIndex in range(len(dictNE[strKey])) :\n",
    "            dictNE[strKey][nIndex] = dictNE[strKey][nIndex].strip().lower()\n",
    "        if not strKey in listAllowedTypes :\n",
    "            del dictNE[strKey]\n",
    "\n",
    "    # write filtered NE dict\n",
    "    writeHandle = codecs.open( 'ne.json', 'w', 'utf-8', errors = 'replace' )\n",
    "    strJSON = json.dumps( dictNE, indent=2 )\n",
    "    writeHandle.write( strJSON + '\\n' )\n",
    "    writeHandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_regex_toc( file_book = None ) :\n",
    "\n",
    "    book_str = \"\"\n",
    "    with open(file_book, 'r') as f:\n",
    "        book_str = f.read()\n",
    "    \n",
    "    dictTOC = extract_toc(book_str)\n",
    "\n",
    "    #printfn(\"---TOC DICTIONARY---\\n\")\n",
    "    #printfn(dictTOC)\n",
    "    \n",
    "    # DO NOT CHANGE THE BELOW CODE WHICH WILL SERIALIZE THE ANSWERS FOR THE AUTOMATED TEST HARNESS TO LOAD AND MARK\n",
    "\n",
    "    writeHandle = codecs.open( 'toc.json', 'w', 'utf-8', errors = 'replace' )\n",
    "    strJSON = json.dumps( dictTOC, indent=2 )\n",
    "    writeHandle.write( strJSON + '\\n' )\n",
    "    writeHandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_regex_questions( file_chapter = None ) :\n",
    "\n",
    "    chapter_str = \"\"\n",
    "    with open(file_chapter, 'r') as f:\n",
    "        chapter_str = f.read()\n",
    "    \n",
    "    setQuestions = extract_questions(chapter_str)\n",
    "    #printfn(\"---QUESTIONS---\")\n",
    "    #printfn(setQuestions)\n",
    "\n",
    "    writeHandle = codecs.open( 'questions.txt', 'w', 'utf-8', errors = 'replace' )\n",
    "    for strQuestion in setQuestions :\n",
    "        writeHandle.write( strQuestion + '\\n' )\n",
    "    writeHandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9c265795cbe4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt3optimal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt4optimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_NER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0montonotes_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-18f6a45cad9b>\u001b[0m in \u001b[0;36mrun_NER\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mwtask3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mtask3optimal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask3sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[0mtask4sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwtask4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtask4_sample_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-18f6a45cad9b>\u001b[0m in \u001b[0;36mtask3\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;31m#generate lists of feature dictionaries for each sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtask3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimization_its\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_task3_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m#try seeing if different hyperparamaters for each model improves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-ff5cc2efcfbb>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(sentences, n_its, dictmaker)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#feature dictionaries for each word in a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mfeature_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictmaker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of the feature lists:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First element:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-ff5cc2efcfbb>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#feature dictionaries for each word in a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mfeature_lists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictmaker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of the feature lists:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_lists\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"First element:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeature_lists\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-18f6a45cad9b>\u001b[0m in \u001b[0;36mgen_task3_features\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m#generate a list of feature dictionaries for each word in the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgen_task3_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgen_feature_dict_task3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgen_task4_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-18f6a45cad9b>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m#generate a list of feature dictionaries for each word in the sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgen_task3_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgen_feature_dict_task3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgen_task4_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-18f6a45cad9b>\u001b[0m in \u001b[0;36mgen_feature_dict_task3\u001b[0;34m(sentence, i)\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m#features.update({\"word\" : sentence[i][0]})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_extra_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m#there is at least 1 word behind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t3optimal, t4optimal = run_NER(ontonotes_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontonotes_sentences = load_ontonotes(\"ontonotes_parsed.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
