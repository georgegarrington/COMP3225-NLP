{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2021-05-10 14:43:50,544 logging started\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import sys, codecs, json, math, time, warnings, re, logging\n",
    "warnings.simplefilter( action='ignore', category=FutureWarning )\n",
    "\n",
    "import nltk, numpy, scipy, sklearn, sklearn_crfsuite, sklearn_crfsuite.metrics\n",
    "\n",
    "LOG_FORMAT = ('%(levelname) -s %(asctime)s %(message)s')\n",
    "logger = logging.getLogger( __name__ )\n",
    "logging.basicConfig( level=logging.INFO, format=LOG_FORMAT )\n",
    "logger.info('logging started')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_on = False\n",
    "max_iter = 150\n",
    "task3_sample_size = 17000\n",
    "task4_sample_size = 17000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only print if verbose_on is true\n",
    "def printfn(thing):\n",
    "\tif verbose_on: print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_toc(chapter_str):\n",
    "    \n",
    "    chapter = r\"(?:CHAPTER|(C|c)hapter)\"\n",
    "    \n",
    "    #either roman numerals or a digit\n",
    "    num = r\"([ivxlcdm]+|[IVXLCDM]+|\\d+)\"\n",
    "    \n",
    "    #the charcters which have been seen to appear in titles I've looked at\n",
    "    #OR match any non-ASCII character (i.e. the special unicode apostraphes\n",
    "    #that often appear)\n",
    "    #chset = r\"(?:[A-Za-z0-9\\?\\-\\.\\'\\\":“”’ ]|[^\\x00-\\x7F])\"\n",
    "    chset = \"[^\\n]\"\n",
    "    \n",
    "    regex = \"\".join([\n",
    "        chapter, r\"[ ]*\", num, r\"[ ]*\\.?[ ]*\\n*[ ]*\",\n",
    "        r\"([A-Za-z]\", chset, \"*)\"\n",
    "    ])\n",
    "    \n",
    "    return {\n",
    "        mobj.groups()[1] : mobj.groups()[2] \n",
    "        for mobj in re.finditer(regex, chapter_str)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given an entire chapter that has been read as a string, \n",
    "#extract all questions from it\n",
    "def extract_questions(chapter_str):\n",
    "    \n",
    "    #Start with a capital letter, any number of non-sentence ending characters followed by a question mark\n",
    "    regex = r\"(?:(?<=([‘“\\\"\\'\\.\\?\\!]))[ ]*)([A-Z][^\\?\\.!]*\\?)\"\n",
    "    matches = re.findall(regex, chapter_str, flags = re.MULTILINE | re.DOTALL | re.UNICODE)\n",
    "    return set(match.replace('\\n', ' ') for _, match in matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#They should be exactly the same length as the predictions have been made from them\n",
    "def gen_tok_NER_pair_lists(tag_list, sentence_with_unknown_NER, NER_predictions):\n",
    "\t\n",
    "\t\n",
    "\ttokens = [tok for (tok,_,_) in sentence_with_unknown_NER]\n",
    "\t#print(\"tokens:\")\n",
    "\t#print(tokens)\n",
    "\t#print(\"predictions\")\n",
    "\t#print(NER_predictions)\n",
    "\t\n",
    "\tpair_list = []\n",
    "\t\n",
    "\ti = 0\n",
    "\twhile i < len(NER_predictions):\n",
    "\n",
    "\t\tif(NER_predictions[i] == \"O\"):\n",
    "\t\t\ti += 1\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\t#if(not (NER_predictions[i][2:] in tag_list)):\n",
    "\t\t#\ti += 1\n",
    "\t\t#\tcontinue\n",
    "\t\t\n",
    "\t\t#remove the B- or I-\n",
    "\t\tactual_tag = NER_predictions[i][2:]\n",
    "\t\t\n",
    "\t\t#The indices will correspond, start the entity with the first token with the tag\n",
    "\t\tentity = tokens[i]\n",
    "\t\ti += 1\n",
    "\t\t\n",
    "\t\twhile i < len(NER_predictions) and NER_predictions[i][2:] == actual_tag:\n",
    "\t\t\tentity += \" \" + tokens[i]\n",
    "\t\t\ti += 1\n",
    "\t\t\n",
    "\t\tpair_list.append((entity, actual_tag))\n",
    "\t\ti += 1\n",
    "\t\n",
    "\treturn pair_list\n",
    "\n",
    "def gen_NER_dict(tag_list, sentences, sentence_NER_predictions):\n",
    "\t\n",
    "\tdct = {tag : [] for tag in tag_list}\n",
    "\t\n",
    "\tfor (sentence, predictions) in zip(sentences,sentence_NER_predictions):\n",
    "\t\tpairs = gen_tok_NER_pair_lists(tag_list, sentence, predictions)\n",
    "\t\tfor (entity, tag) in pairs:\n",
    "\t\t\tdct[tag].append(entity.lower())\n",
    "\t\n",
    "\tdct = {tag : list(set(entities)) for (tag,entities) in dct.items()}\n",
    "\treturn dct\n",
    "\n",
    "#turn the list of tokens into a sentence that can be used for NER\n",
    "def prepare_sentence(tokens):\n",
    "\ttags = list(map(lambda x: x[1], pos_tag(tokens)))\n",
    "\tsentence = []\n",
    "\tfor i in range(len(tokens)):\n",
    "\t\tsentence.append((tokens[i],tags[i],\"O\"))\n",
    "\treturn sentence\n",
    "\n",
    "#detect if the chapter string has \"CHAPTER . SOMETHING\" at the start of it\n",
    "#and return the string without it\n",
    "def without_chapter(chapter_str):\n",
    "\t\n",
    "\t#Define the end of a title as there being at least 2 new line characters\n",
    "\tregex = r\"(\\n|[ ])*.*(?:CHAPTER|(C|c)hapter)[ ]*\\.?(?:[ivxlcdm]+|[IVXLCDM]+|\\d+)[ ]*\\.?(\\n|[ ])*.*\\n.*\\n(\\n|[ ])*\"\n",
    "\tresult = re.match(regex, chapter_str, flags = re.MULTILINE | re.UNICODE)\n",
    "\t\n",
    "\treturn chapter_str[len(result[0]):]\n",
    "\t\n",
    "\t#if there is a Chapter N part at the start of the chapter then remove it\n",
    "\t#length = len(result[0])\n",
    "\t#return chapter_str[length:]\n",
    "\n",
    "#take the file path of ontonotes and return sentences\n",
    "def load_ontonotes(dataset_file):\n",
    "\n",
    "\treadHandle = codecs.open(dataset_file, 'r', 'utf-8', errors = 'replace')\n",
    "\tstr_json = readHandle.read()\n",
    "\treadHandle.close()\n",
    "\tdict_ontonotes = json.loads(str_json)\n",
    "\torig_list = list(dict_ontonotes.keys())\n",
    "\tsentences = []\n",
    "\n",
    "\tfor str_file in orig_list:\n",
    "\t\tfor str_sent_index in dict_ontonotes[str_file] :\n",
    "\n",
    "\t\t\ttokens = []\n",
    "\t\t\tner_tags = []\n",
    "\t\t\t# compute IOB tags for named entities (if any)\n",
    "\t\t\tne_type_last = None\n",
    "\n",
    "\t\t\t#build up the list of tokens and ner tags\n",
    "\t\t\tfor nTokenIndex in range(len(dict_ontonotes[str_file][str_sent_index]['tokens'])) :\n",
    "\t\t\t\tstrToken = dict_ontonotes[str_file][str_sent_index]['tokens'][nTokenIndex]\n",
    "\t\t\t\tstrPOS = dict_ontonotes[str_file][str_sent_index]['pos'][nTokenIndex]\n",
    "\t\t\t\tne_type = None\n",
    "\t\t\t\tif 'ne' in dict_ontonotes[str_file][str_sent_index] :\n",
    "\t\t\t\t\tdict_ne = dict_ontonotes[str_file][str_sent_index]['ne']\n",
    "\t\t\t\t\tif not 'parse_error' in dict_ne :\n",
    "\t\t\t\t\t\tfor str_NEIndex in dict_ne :\n",
    "\t\t\t\t\t\t\tif nTokenIndex in dict_ne[str_NEIndex]['tokens'] :\n",
    "\t\t\t\t\t\t\t\tne_type = dict_ne[str_NEIndex]['type']\n",
    "\t\t\t\t\t\t\t\tbreak\n",
    "\t\t\t\tif ne_type != None :\n",
    "\t\t\t\t\tif ne_type == ne_type_last :\n",
    "\t\t\t\t\t\tstrIOB = 'I-' + ne_type\n",
    "\t\t\t\t\telse :\n",
    "\t\t\t\t\t\tstrIOB = 'B-' + ne_type\n",
    "\t\t\t\telse :\n",
    "\t\t\t\t\tstrIOB = 'O'\n",
    "\n",
    "\t\t\t\tne_type_last = ne_type\n",
    "\t\t\t\ttokens.append(strToken)\n",
    "\t\t\t\tner_tags.append(strIOB)\n",
    "\t\t\tlist_entry = []\n",
    "\n",
    "\t\t\t#use nltk pos tags instead\n",
    "\t\t\tfor (index, tup) in enumerate(pos_tag(tokens)):\n",
    "\t\t\t\t#print(\"tup is:\", tup)\n",
    "\t\t\t\tlist_entry.append((tup[0], tup[1], ner_tags[index]))\n",
    "\t\t\tsentences.append(list_entry)\n",
    "\treturn sentences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import names\n",
    "from copy import copy\n",
    "from random import shuffle\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the ontonotes data sentences and the sentences \n",
    "#to predict from reading the chapter file\n",
    "def run_NER(sentences, sentences_to_predict):\n",
    "    \n",
    "\t\t#given a specific NER tag, return 2 sets consisting of the \n",
    "\t#\"inside parts\" and the \"before parts\"\n",
    "\tdef extract_gazetteers(target, sentences):\n",
    "\t\tbefore = set()\n",
    "\t\tinside = set()\n",
    "\t\tbefore_tag = \"B-\" + target\n",
    "\t\tinside_tag = \"I-\" + target\n",
    "\t\tfor sent in sentences:\n",
    "\t\t\tfor tok, _, ner in sent:\n",
    "\t\t\t\tif ner == before_tag:\n",
    "\t\t\t\t\tbefore.add(tok.lower())\n",
    "\t\t\t\tif ner == inside_tag:\n",
    "\t\t\t\t\tinside.add(tok.lower())\n",
    "\t\treturn before, inside\n",
    "\n",
    "\n",
    "\t#convert all the NER tags you don't care about into O's in sentences that may have mixed NER tags that we care about\n",
    "\t#with NER tags that we don't care about\n",
    "\tdef only_Os(desired_set, sentences):\n",
    "\t\tactual_tags = []\n",
    "\t\tfor tag in desired_set:\n",
    "\t\t\tactual_tags.append(\"B-\" + tag)\n",
    "\t\t\tactual_tags.append(\"I-\" + tag)\n",
    "\t\treturn [[(tok, pos, ner if ner in actual_tags else 'O') for tok, pos, ner in sent] for sent in sentences]\n",
    "\n",
    "\n",
    "\t#If the dataset has already been split and we KNOW that none of these NER tags are the ones we care about,\n",
    "\t#then clearly we can just replace ALL NER tags with O\n",
    "\tdef all_Os(sentences):\n",
    "\t\treturn [[(tok, pos, 'O') for tok, pos, _ in sent] for sent in sentences]\n",
    "\n",
    "\n",
    "\t#returns a tuples of 2 lists of sentences, where one list contains all the sentences that contain the tags \n",
    "\t#specified in the argument and the other list does not\n",
    "\t#you just give it the tag strings and it adds the Bs and Is for you\n",
    "\tdef split_tags(tag_set, sentences):\n",
    "\n",
    "\t\tactual_tags = []\n",
    "\t\tfor tag in tag_set:\n",
    "\t\t\tactual_tags.append(\"B-\" + tag)\n",
    "\t\t\tactual_tags.append(\"I-\" + tag)\n",
    "\n",
    "\t\tcontains_tags = []\n",
    "\t\tdoesnt_contain_tags = []\n",
    "\n",
    "\t\tfor sent in sentences:\n",
    "\n",
    "\t\t\tif(any([tag in actual_tags for _,_,tag in sent])):\n",
    "\t\t\t\tcontains_tags.append(sent)\n",
    "\t\t\telse:\n",
    "\t\t\t\tdoesnt_contain_tags.append(sent)\n",
    "\n",
    "\t\treturn (contains_tags, doesnt_contain_tags)\n",
    "\n",
    "\t#the sentences containing task 3 tags and the sentences that dont\n",
    "\twtask3, wotask3 = split_tags([\"DATE\", \"CARDINAL\", \"ORDINAL\", \"NORP\"], sentences)\n",
    "\n",
    "\n",
    "\t#the sentences containing task 4 tags (only person) and the sentences that dont\n",
    "\twtask4, wotask4 = split_tags([\"PERSON\"], sentences)\n",
    "\n",
    "\t#all other tags can be converted to O because we dont care about finding them\n",
    "\twtask3 = only_Os([\"DATE\", \"CARDINAL\", \"ORDINAL\", \"NORP\"], wtask3)\n",
    "\twtask4 = only_Os([\"PERSON\"], wtask4)\n",
    "\n",
    "\t#Any sentences that don't contain any entities we care about can\n",
    "\t#ALL be converted to Os!\n",
    "\t#wotask3 = all_Os(wotask3)\n",
    "\t#wotask4 = all_Os(wotask4)\n",
    "\n",
    "\tdel wotask3\n",
    "\tdel wotask4\n",
    "\tdel sentences\n",
    "\tgc.collect()\n",
    "\t\n",
    "\t\t#take a random sample with the specified size from the list of sentences\n",
    "\tdef random_sample(sentences, num):\n",
    "\n",
    "\t\tif num > len(sentences):\n",
    "\t\t\treturn sentences\n",
    "\n",
    "\t\tcp = copy(sentences)\n",
    "\t\tshuffle(cp)\n",
    "\t\treturn cp[-num:]\n",
    "\n",
    "\t#process data, putting in a ratio of sentences that contain the NER tags we care about\n",
    "\t#and the complement ration of ones we dont\n",
    "\tdef process_data(care_about, dont_care):\n",
    "\n",
    "\t\tnum_care = floor(care_ratio * sample_size)\n",
    "\t\tnum_dont_care = sample_size - num_care\n",
    "\n",
    "\t\tcare_sample = random_sample(care_about, num_care)\n",
    "\t\tdont_care_sample = random_sample(dont_care, num_dont_care)\n",
    "\n",
    "\t\tprocessed = care_sample + dont_care_sample\n",
    "\t\tshuffle(processed)\n",
    "\t\treturn processed\n",
    "\n",
    "\t#task3_processed = process_data(wtask3, wotask3)\n",
    "\t#task4_processed = process_data(wtask4, wotask4)\n",
    "\n",
    "\n",
    "\t#split the items into training and testing denoted by the testing ratio argument\n",
    "\tdef random_split(items, ratio):\n",
    "\t\tshuffle(copy(items))\n",
    "\t\ttrain_size = int(ratio * len(items))\n",
    "\t\ttest_size = len(items) - train_size\n",
    "\t\treturn items[:train_size], items[-test_size:]\n",
    "\n",
    "\t#don't need training and testing for handin, just use the whole thing\n",
    "\t#task3_training, task3_testing = random_split(task3_processed, 0.9)\n",
    "\t#task4_training, task4_testing = random_split(task4_processed, 0.9)\n",
    "\n",
    "\t#generate the gazetteers to be used from ontonotes\n",
    "\t#before_name, inside_name = extract_gazetteers(\"PERSON\", wtask4)\n",
    "\t#before_date, inside_date = extract_gazetteers(\"DATE\", wtask3)\n",
    "\t#before_cardinal, inside_cardinal = extract_gazetteers(\"CARDINAL\", wtask3)\n",
    "\t#before_norp, inside_norp = extract_gazetteers(\"NORP\", wtask3)\n",
    "\t#before_ord, inside_ord = extract_gazetteers(\"ORDINAL\", wtask3)\n",
    "\n",
    "\t\t#assumed to be in the environment of the function that \n",
    "\t#generates the feature dictionary for task 3\n",
    "\tnumber_gazetteer = set([\n",
    "\t\t\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\", \"eight\",\n",
    "\t\t\"nine\", \"ten\", \"eleven\", \"twelve\", \"thirteen\", \"fourteen\", \"fifteen\",\n",
    "\t\t\"sixteen\", \"seventeen\", \"eighteen\", \"nineteen\", \"twenty\", \"thirty\", \"fourty\", \"fifty\",\n",
    "\t\t\"sixty\", \"seventy\", \"eighty\", \"ninety\", \"hundred\", \"onehundred\", \"one-hundred\", \"thousand\", \"million\"\n",
    "\t\t])\n",
    "\n",
    "\t#Python sets are implemented as hash tables so lookup is always around O(1)\n",
    "\t#even for large sets\n",
    "\n",
    "\tordinal_gazetteer = set([\n",
    "\t\t\"first\", \"second\", \"third\", \"fourth\", \"fifth\", \"sixth\", \"seventh\",\n",
    "\t\t\"eigth\", \"ninth\", \"tenth\", \"eleventh\", \"twelth\", \"thirteenth\",\n",
    "\t\t\"fourteenth\", \"fifteenth\", \"sixteenth\", \"seventeenth\", \"eighteenth\",\n",
    "\t\t\"nineteenth\", \"twentieth\", \"thirtieth\", \"fourtieth\", \"fiftieth\", \n",
    "\t\t\"sixtieth\", \"seventieth\", \"eightieth\", \"ninetieth\", \"hundreth\", \"thousandth\",\n",
    "\t\t\"millionth\", \"firstly\", \"secondly\", \"thirdly\", \"fourthly\", \"fifthly\", \"sixthly\",\n",
    "\t\t\"seventhly\", \"eigthly\", \"ninethly\", \"tenthly\"\n",
    "\t])\n",
    "\n",
    "\tdate_gazetteer = set([\n",
    "\t\t\"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\", \"saturday\", \"sunday\", \"day\", \"days\", \n",
    "\t\t\"week\", \"weeks\", \"month\", \"months\", \"year\", \"years\", \"decade\", \"decades\", \"century\", \"centuries\", \n",
    "\t\t\"millennium\", \"millennia\", \"jan\", \"january\", \"feb\", \"february\", \"mar\", \"march\", \"apr\", \"april\", \n",
    "\t\t\"may\", \"jun\", \"june\", \"jul\", \"july\", \"aug\", \"august\", \"sep\", \"september\", \"oct\", \"october\", \n",
    "\t\t\"nov\", \"november\", \"dec\", \"december\", \"holiday\", \"holidays\"\n",
    "\t])\n",
    "\n",
    "\ttime_gazetteer = set([\"second\", \"seconds\", \"minute\", \"minutes\", \"hour\", \"hours\", \"time\", \"clock\", \"o'clock\", \"past\", \"oclock\"])\n",
    "\n",
    "\ttitle_gazetteer = set([\n",
    "\t\t\"mr\", \"mr.\", \"mrs\", \"mrs.\", \"miss\", \"miss.\", \"madam\", \n",
    "\t\t\"mam.\", \"sir\", \"sir.\", \"lord\", \"lord.\", \"mam\", \"mister\", \n",
    "\t\t\"mister.\", \"missus\", \"missus.\", \"dame\", \"chairman\", \"king\", \n",
    "\t\t\"queen\", \"president\"])\n",
    "\n",
    "\t# a feature for detecting e.g. 1st, 2nd, 3rd, 4th etc.\n",
    "\tdef is_st_th(candidate):\n",
    "\t\tsuffixes = set([\"st\", \"nd\", \"rd\", \"th\"])\n",
    "\t\treturn candidate[-2:] in suffixes and candidate[:-2].isdigit()\n",
    "\n",
    "\t#use the nltk name corpus\n",
    "\t#Keep all names uppercase as by convention names will only ever really \n",
    "\t#appear in books as uppercase, this rules out things like \"will\" being\n",
    "\t#detected as a name\n",
    "\t#name_gazetteer = set(names.words(\"male.txt\") + names.words(\"female.txt\"))\n",
    "\n",
    "\t#task 4 has less features as it only revolves around names\n",
    "\tdef gen_feature_dict_task4(sentence, i):\n",
    "\n",
    "\t\tdef gen_features(index, relative_str):\n",
    "\t\t\treturn {\n",
    "\t\t\t\trelative_str + \"word.lower()\": sentence[index][0].lower(),\n",
    "\t\t\t\trelative_str + \"word_prefix\": sentence[index][0][:3],\n",
    "\t\t\t\trelative_str + \"word_suffix\": sentence[index][0][-3:],\n",
    "\t\t\t\trelative_str + \"postag\": sentence[index][1],\n",
    "\t\t\t\trelative_str + \"is_mr_mrs\": sentence[index][0][:2].lower() == \"mr\" or sentence[index][0][:3].lower() == \"mrs\",\n",
    "\t\t\t\t#relative_str + \"in_title_gaz\": sentence[index][0].lower() in title_gazetteer,\n",
    "\t\t\t\trelative_str + \"not_alnum\": not sentence[index][0].isalnum(),\n",
    "\t\t\t\trelative_str + \"is_single_letter\": sentence[index][0].isalpha() and len(sentence[index][0]) == 1\n",
    "\t\t\t}\n",
    "\n",
    "\t\t#extra features are limited to center 3 in the context window of size 5\n",
    "\t\tdef gen_extra_features(index, relative_str):\n",
    "\t\t\treturn {\n",
    "\t\t\t\trelative_str + \"istitle\": sentence[index][0].istitle(),\n",
    "\t\t\t\trelative_str + \"isupper\": sentence[index][0].isupper()\n",
    "\t\t\t\t#relative_str + \"in_nltk_name_gaz\" :  sentence[index][0] in name_gazetteer\n",
    "\t\t\t}\n",
    "\n",
    "\t\tfeatures = gen_features(i, \"\")\n",
    "\t\tfeatures.update(gen_extra_features(i, \"\"))\n",
    "\n",
    "\t\t#there is at least 1 word behind\n",
    "\t\tif i > 0:#generate a list of feature dictionaries for each word in the sentence\n",
    "\t\t\tfeatures.update(gen_features(i - 1, \"-1:\"))\n",
    "\t\t\tfeatures.update(gen_extra_features(i - 1, \"-1:\"))\n",
    "\n",
    "\t\t#there are at least 2 words behind\n",
    "\t\tif i > 1:\n",
    "\t\t\tfeatures.update(gen_features(i - 2, \"-2:\"))\n",
    "\n",
    "\t\t#there are at least 3 words behind\n",
    "\t\t#if i > 2:\n",
    "\t\t#\tfeatures.update(gen_features(i - 3, \"-3:\"))\n",
    "\t\t\t\n",
    "\t\t#there is at least 1 word ahead\n",
    "\t\tif i < len(sentence) - 1:\n",
    "\t\t\tfeatures.update(gen_features(i + 1, \"+1:\"))\n",
    "\t\t\tfeatures.update(gen_extra_features(i + 1, \"+1:\"))\n",
    "\n",
    "\t\t#there are at least 2 words ahead\n",
    "\t\tif i < len(sentence) - 2:\n",
    "\t\t\tfeatures.update(gen_features(i + 2, \"+2:\"))\n",
    "\n",
    "\t\t#there are at least 3 words ahead\n",
    "\t\t#if i < len(sentence) - 3:\n",
    "\t\t#\tfeatures.update(gen_features(i + 3, \"+3:\"))\n",
    "\t\t\t\n",
    "\t\treturn features\n",
    "\n",
    "\tdef gen_feature_dict_task3(sentence, i):\n",
    "\n",
    "\t\t#token = sentence[i][0]\n",
    "\t\t#pos_tag = sentence[i][1]\n",
    "\n",
    "\t\tdef gen_features(index, relative_str):\n",
    "\n",
    "\t\t\treturn {\n",
    "\t\t\t\trelative_str + \"word.lower()\": sentence[index][0].lower(),\n",
    "\t\t\t\trelative_str + \"word_prefix\": sentence[index][0][:3],\n",
    "\t\t\t\trelative_str + \"word_suffix\": sentence[index][0][-3:],\n",
    "\t\t\t\trelative_str + \"postag\": sentence[index][1],\n",
    "\t\t\t\trelative_str + \"isdigit\": sentence[index][0].isdigit()\n",
    "\t\t\t}\n",
    "\n",
    "\t\t#extra features are limited to center 3 in the context window of size 5\n",
    "\t\tdef gen_extra_features(index, relative_str):\n",
    "\t\t\treturn {\n",
    "\t\t\t\trelative_str + \"not_alnum\": not sentence[index][0].isalnum(),\n",
    "\t\t\t\trelative_str + \"istitle\": sentence[index][0].istitle(),\n",
    "\t\t\t\trelative_str + \"isupper\": sentence[index][0].isupper(),\n",
    "\t\t\t\t#python sets are around O(1) lookup so its fine to have large gazetteers\n",
    "\t\t\t\trelative_str + \"in_num_gaz\": sentence[index][0].lower() in number_gazetteer,\n",
    "\t\t\t\trelative_str + \"in_ord_gaz\": sentence[index][0].lower() in ordinal_gazetteer,\n",
    "\t\t\t\trelative_str + \"in_date_gaz\": sentence[index][0].lower() in date_gazetteer,\n",
    "\t\t\t\trelative_str + \"in_time_gaz\": sentence[index][0].lower() in time_gazetteer,\n",
    "\t\t\t\t#relative_str + \"in_nltk_name_gaz\": sentence[index][0].lower() in name_gazetteer,\n",
    "\n",
    "\t\t\t\t#COMMENT THESE OUT MAYBE COS NOT SURE ABOUT THEM\n",
    "\t#\t\t\t\trelative_str + \"in_B_date\": sentence[index][0].lower() in before_date,\n",
    "\t#\t\t\t\trelative_str + \"in_I_date\": sentence[index][0].lower() in inside_date,\n",
    "\t#\t\t\t\trelative_str + \"in_B_card\": sentence[index][0].lower() in before_cardinal,\n",
    "\t#\t\t\t\trelative_str + \"in_I_card\": sentence[index][0].lower() in inside_cardinal,\n",
    "\t#\t\t\t\trelative_str + \"in_B_norp\": sentence[index][0].lower() in before_norp,\n",
    "\t#\t\t\t\trelative_str + \"in_I_norp\": sentence[index][0].lower() in inside_norp,\n",
    "\t#\t\t\t\trelative_str + \"in_B_ord\": sentence[index][0].lower() in before_ord,\n",
    "\t#\t\t\t\trelative_str + \"in_I_ord\": sentence[index][0].lower() in inside_ord,\n",
    "\n",
    "\t\t\t\t#e.g. 1st, 2nd, 3rd, 4th etc.\n",
    "\t\t\t\trelative_str + \"is_st_th\": is_st_th(sentence[index][0])\n",
    "\t\t\t}\n",
    "\n",
    "\t\tfeatures = gen_features(i, \"\")\n",
    "\t\tfeatures.update(gen_extra_features(i, \"\"))\n",
    "\n",
    "\t\t#there is at least 1 word behind\n",
    "\t\tif i > 0:\n",
    "\t\t\tfeatures.update(gen_features(i - 1, \"-1:\"))\n",
    "\t\t\tfeatures.update(gen_extra_features(i - 1, \"-1:\"))\n",
    "\n",
    "\t\t#there are at least 2 words behind\n",
    "\t\tif i > 1:\n",
    "\t\t\tfeatures.update(gen_features(i - 2, \"-2:\"))\n",
    "\n",
    "\t\t#there are at least 3 words behind\n",
    "\t\t#if i > 2:\n",
    "\t\t#\tfeatures.update(gen_features(i - 3, \"-3:\"))\n",
    "\n",
    "\t\t#there is at least 1 word ahead\n",
    "\t\tif i < len(sentence) - 1:\n",
    "\t\t\tfeatures.update(gen_features(i + 1, \"+1:\"))\n",
    "\t\t\tfeatures.update(gen_extra_features(i + 1, \"+1:\"))\n",
    "\n",
    "\t\t#there are at least 2 words ahead\n",
    "\t\tif i < len(sentence) - 2:\n",
    "\t\t\tfeatures.update(gen_features(i + 2, \"+2:\"))\n",
    "\n",
    "\t\t#there are at least 3 words ahead\n",
    "\t\t#if i < len(sentence) - 3:\n",
    "\t\t#\tfeatures.update(gen_features(i + 3, \"+3:\"))\n",
    "\n",
    "\t\treturn features\n",
    "\n",
    "\t\t#helper functions\n",
    "\tdef sent2labels(sentence):\n",
    "\t\treturn [label for _, _, label in sentence] \n",
    "\tdef sent2tokens(sent):\n",
    "\t\treturn [token for token, _, _ in sentence]\n",
    "\n",
    "\tdef sentences2labels(sentences):\n",
    "\t\treturn [sent2labels(sent) for sent in sentences]\n",
    "\n",
    "\n",
    "\t\t#generate a list of feature dictionaries for each word in the sentence\n",
    "\tdef gen_task3_features(sentence):\n",
    "\t\treturn [gen_feature_dict_task3(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "\tdef gen_task4_features(sentence):\n",
    "\t\treturn [gen_feature_dict_task4(sentence, i) for i in range(len(sentence))]\n",
    "\n",
    "\tdisplay_label_subset = [\"B-DATE\", \"I-DATE\", \"B-CARDINAL\", \"I-CARDINAL\", \"B-ORDINAL\", \"I-ORDINAL\", \"B-NORP\", \"I-NORP\"]\n",
    "\n",
    "\n",
    "\t#generate lists of feature dictionaries for each sentence\n",
    "\tdef task3(ontonotes_data, sentences_to_predict):\n",
    "\n",
    "\n",
    "\t\tfeature_lists = [gen_task3_features(sent) for sent in ontonotes_data]\n",
    "\t\tfeature_dicts_to_predict = [gen_task3_features(sent) for sent in sentences_to_predict]\n",
    "\t\tlabel_lists = [sent2labels(sent) for sent in ontonotes_data]\n",
    "\n",
    "\t\tcrf = sklearn_crfsuite.CRF(\n",
    "\t\t\talgorithm = \"lbfgs\",\n",
    "\t\t\tc1 = 1,\n",
    "\t\t\tc2 = 0.149853957,\n",
    "\t\t\tmax_iterations = max_iter,\n",
    "\t\t\tall_possible_transitions = True,\n",
    "\t\t\tverbose = verbose_on\n",
    "\t\t)\n",
    "\n",
    "\t\tcrf.fit(feature_lists, label_lists)\n",
    "\t\tpredictions = crf.predict(feature_dicts_to_predict)\n",
    "\t\tdct = gen_NER_dict([\"CARDINAL\", \"ORDINAL\", \"DATE\", \"NORP\"], sentences_to_predict, predictions)\n",
    "\t\t\n",
    "\t\treturn dct#, crf\n",
    "\n",
    "\t#try seeing if different hyperparamaters for each model improves\n",
    "\t#their respsective performances, might be worth them not both being the same\n",
    "\tdef task4(ontonotes_data, sentences_to_predict):\n",
    "\n",
    "\t\t#print(\"Task 4 sentences to predict length:\", len(sentences_to_predict))\n",
    "\t\tfeature_lists = [gen_task4_features(sent) for sent in ontonotes_data]\n",
    "\t\tfeature_dicts_to_predict = [gen_task4_features(sent) for sent in sentences_to_predict]\n",
    "\t\t#print(\"The first feature dict in the first feature dict list is:\")\n",
    "\t\t#print(feature_lists[0][0])\n",
    "\t\t#print(\"The first feature dict in the first feature dict list (the ones that should be predicted) is:\")\n",
    "\t\t#print(feature_dicts_to_predict[0][0])\n",
    "\t\tlabel_lists = [sent2labels(sent) for sent in ontonotes_data]\n",
    "\t\tcrf = sklearn_crfsuite.CRF(\n",
    "\t\t\talgorithm = \"lbfgs\",\n",
    "\t\t\tc1 = 1,\n",
    "\t\t\tc2 = 0.149853957,\n",
    "\t\t\tmax_iterations = max_iter,\n",
    "\t\t\tall_possible_transitions = True,\n",
    "\t\t\tverbose = verbose_on)\n",
    "\t\tcrf.fit(feature_lists, label_lists)\n",
    "\n",
    "\t\t#REPORT STUFF GOES HERE\n",
    "\n",
    "\t\tpredictions = crf.predict(feature_dicts_to_predict)\n",
    "\n",
    "\t\tdct = gen_NER_dict([\"PERSON\"], sentences_to_predict, predictions)\n",
    "        \n",
    "\t\t#predictions = crf.predict(feature_dicts_to_predict)\n",
    "\t\t#print(\"TASK 4 PREDICTIONS:\")\n",
    "\t\t#print(gen_tok_NER_pair_lists(sentences_to_predict, predictions))\n",
    "\t\treturn dct#, crf\n",
    "\t\t#return extract_entities_from_sequences([\"PERSON\"], predictions, False)\n",
    "\n",
    "\ttask3sample = random_sample(wtask3,task3_sample_size)\n",
    "\tdel wtask3\n",
    "\tgc.collect()\n",
    "\ttask3dict = task3(task3sample, sentences_to_predict)\n",
    "\t\n",
    "\ttask4sample = random_sample(wtask4,task4_sample_size)\n",
    "\tdel wtask4\n",
    "\tgc.collect()\n",
    "\ttask4dict = task4(task4sample, sentences_to_predict)\n",
    "\n",
    "\t#combine into one dict as required\n",
    "\ttask3dict.update(task4dict)\n",
    "\n",
    "\treturn task3dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_ner( file_chapter = None, ontonotes_file = None ) :\n",
    "\n",
    "\t# INSERT CODE TO TRAIN A CRF NER MODEL TO TAG THE CHAPTER OF TEXT (subtask 3)\n",
    "\t# USING NER MODEL AND REGEX GENERATE A SET OF BOOK CHARACTERS AND FILTERED SET OF NE TAGS (subtask 4)\n",
    "\n",
    "\tchapter_str = \"\"\n",
    "\twith open(file_chapter, 'r') as f:\n",
    "\t\tchapter_str = without_chapter(f.read())\n",
    "\t\t\n",
    "\tsentences_to_predict = []\n",
    "\tfor sent in sent_tokenize(chapter_str):\t# DO NOT CHANGE THE CODE IN THIS FUNCTION\n",
    "\n",
    "\t#\n",
    "\t# subtask 1 >> extract chapter headings and create a table of contents from a provided plain text book (from www.gutenberg.org)\n",
    "\t# Input >> www.gutenberg.org sourced plain text file for a whole book\n",
    "\t# Output >> toc.json = { <chapter_number_text> : <chapter_title_text> }\n",
    "\t#\n",
    "\n",
    "\t\ttokens = word_tokenize(sent)\n",
    "\t\tsentences_to_predict.append(prepare_sentence(tokens))\n",
    "\t\n",
    "\tontonotes_sentences = load_ontonotes(ontonotes_file)\n",
    "\t\n",
    "\tdictNE = run_NER(ontonotes_sentences, sentences_to_predict)\n",
    "\t\n",
    "\tprintfn(\"---NER DICTIONARY---\\n\")\n",
    "\tprintfn(dictNE)\n",
    "\n",
    "\t# DO NOT CHANGE THE BELOW CODE WHICH WILL SERIALIZE THE ANSWERS FOR THE AUTOMATED TEST HARNESS TO LOAD AND MARK\n",
    "\n",
    "\t# write out all PERSON entries for character list for subtask 4\n",
    "\twriteHandle = codecs.open( 'characters.txt', 'w', 'utf-8', errors = 'replace' )\n",
    "\tif 'PERSON' in dictNE :\n",
    "\t\tfor strNE in dictNE['PERSON'] :\n",
    "\t\t\twriteHandle.write( strNE.strip().lower()+ '\\n' )\n",
    "\twriteHandle.close()\n",
    "\n",
    "\t# FILTER NE dict by types required for subtask 3\n",
    "\tlistAllowedTypes = [ 'DATE', 'CARDINAL', 'ORDINAL', 'NORP' ]\n",
    "\tlistKeys = list( dictNE.keys() )\n",
    "\tfor strKey in listKeys :\n",
    "\t\tfor nIndex in range(len(dictNE[strKey])) :\n",
    "\t\t\tdictNE[strKey][nIndex] = dictNE[strKey][nIndex].strip().lower()\n",
    "\t\tif not strKey in listAllowedTypes :\n",
    "\t\t\tdel dictNE[strKey]\n",
    "\n",
    "\t# write filtered NE dict\n",
    "\twriteHandle = codecs.open( 'ne.json', 'w', 'utf-8', errors = 'replace' )\n",
    "\tstrJSON = json.dumps( dictNE, indent=2 )\n",
    "\twriteHandle.write( strJSON + '\\n' )\n",
    "\twriteHandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_regex_toc( file_book = None ) :\n",
    "\n",
    "\tbook_str = \"\"\n",
    "\twith open(file_book, 'r') as f:\n",
    "\t\tbook_str = f.read()\n",
    "\t\n",
    "\tdictTOC = extract_toc(book_str)\n",
    "\n",
    "\tprintfn(\"---TOC DICTIONARY---\\n\")\n",
    "\tprintfn(dictTOC)\n",
    "\t\n",
    "\t# DO NOT CHANGE THE BELOW CODE WHICH WILL SERIALIZE THE ANSWERS FOR THE AUTOMATED TEST HARNESS TO LOAD AND MARK\n",
    "\n",
    "\twriteHandle = codecs.open( 'toc.json', 'w', 'utf-8', errors = 'replace' )\n",
    "\tstrJSON = json.dumps( dictTOC, indent=2 )\n",
    "\twriteHandle.write( strJSON + '\\n' )\n",
    "\twriteHandle.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_regex_questions( file_chapter = None ) :\n",
    "\n",
    "\tchapter_str = \"\"\n",
    "\twith open(file_chapter, 'r') as f:\n",
    "\t\tchapter_str = f.read()\n",
    "\t\n",
    "\tsetQuestions = extract_questions(chapter_str)\n",
    "\tprintfn(\"---QUESTIONS---\")\n",
    "\tprintfn(setQuestions)\n",
    "\n",
    "\twriteHandle = codecs.open( 'questions.txt', 'w', 'utf-8', errors = 'replace' )\n",
    "\tfor strQuestion in setQuestions :\n",
    "\t\twriteHandle.write( strQuestion + '\\n' )\n",
    "\twriteHandle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2021-05-10 13:11:48,977 ontonotes = 'ontonotes_parsed.json'\n",
      "INFO 2021-05-10 13:11:48,978 book = 'eval_book.txt'\n",
      "INFO 2021-05-10 13:11:48,979 chapter = 'eval_chapter.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to make feature dict lists and label lists...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite:   1%|          | 120/20000 [00:00<00:16, 1185.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lists successfully created\n",
      "Now going to start training the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 20000/20000 [00:17<00:00, 1152.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 266246\n",
      "Seconds required: 2.592\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 1.000000\n",
      "c2: 0.149854\n",
      "num_memories: 6\n",
      "max_iterations: 5\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=2.00  loss=659002.89 active=167416 feature_norm=1.00\n",
      "Iter 2   time=1.99  loss=449754.87 active=165679 feature_norm=5.25\n",
      "Iter 3   time=0.99  loss=383253.24 active=122774 feature_norm=4.65\n",
      "Iter 4   time=5.91  loss=195368.93 active=87355 feature_norm=3.29\n",
      "Iter 5   time=0.99  loss=157791.62 active=76456 feature_norm=4.85\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 11.873\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 76456 (266246)\n",
      "Number of active attributes: 45150 (193565)\n",
      "Number of active labels: 9 (9)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.029\n",
      "\n",
      "model trained. Now going to do predictions...\n",
      "Predictions made\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading training data to CRFsuite: 100%|██████████| 20000/20000 [00:14<00:00, 1388.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature generation\n",
      "type: CRF1d\n",
      "feature.minfreq: 0.000000\n",
      "feature.possible_states: 0\n",
      "feature.possible_transitions: 1\n",
      "0....1....2....3....4....5....6....7....8....9....10\n",
      "Number of features: 250130\n",
      "Seconds required: 2.234\n",
      "\n",
      "L-BFGS optimization\n",
      "c1: 1.000000\n",
      "c2: 0.149854\n",
      "num_memories: 6\n",
      "max_iterations: 5\n",
      "epsilon: 0.000010\n",
      "stop: 10\n",
      "delta: 0.000010\n",
      "linesearch: MoreThuente\n",
      "linesearch.max_iterations: 20\n",
      "\n",
      "Iter 1   time=0.81  loss=215016.30 active=152955 feature_norm=1.00\n",
      "Iter 2   time=0.42  loss=156040.64 active=121557 feature_norm=1.60\n",
      "Iter 3   time=0.42  loss=139189.36 active=109447 feature_norm=1.75\n",
      "Iter 4   time=0.42  loss=107703.56 active=80741 feature_norm=2.97\n",
      "Iter 5   time=0.41  loss=83822.88 active=85528 feature_norm=2.90\n",
      "L-BFGS terminated with the maximum number of iterations\n",
      "Total seconds required for training: 2.487\n",
      "\n",
      "Storing the model\n",
      "Number of active features: 85528 (250130)\n",
      "Number of active attributes: 55858 (198384)\n",
      "Number of active labels: 3 (3)\n",
      "Writing labels\n",
      "Writing attributes\n",
      "Writing feature references for transitions\n",
      "Writing feature references for attributes\n",
      "Seconds required: 0.034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\tif len(sys.argv) < 4 :\n",
    "\t\traise Exception( 'missing command line args : ' + repr(sys.argv) )\n",
    "ontonotes_file = sys.argv[1]\n",
    "book_file = sys.argv[2]\n",
    "chapter_file = sys.argv[3]\n",
    "\n",
    "logger.info( 'ontonotes = ' + repr(ontonotes_file) )\n",
    "logger.info( 'book = ' + repr(book_file) )\n",
    "logger.info( 'chapter = ' + repr(chapter_file) )\n",
    "\n",
    "# DO NOT CHANGE THE CODE IN THIS FUNCTION\n",
    "\n",
    "#\n",
    "# subtask 1 >> extract chapter headings and create a table of contents from a provided plain text book (from www.gutenberg.org)\n",
    "# Input >> www.gutenberg.org sourced plain text file for a whole book\n",
    "# Output >> toc.json = { <chapter_number_text> : <chapter_title_text> }\n",
    "#\n",
    "\n",
    "exec_regex_toc( book_file )\n",
    "\n",
    "#\n",
    "# subtask 2 >> extract every question from a provided plain text chapter of text\n",
    "# Input >> www.gutenberg.org sourced plain text file for a chapter of a book\n",
    "# Output >> questions.txt = plain text set of extracted questions. one line per question.\n",
    "#\n",
    "\n",
    "exec_regex_questions( chapter_file )\n",
    "\n",
    "#\n",
    "# subtask 3 (NER) >> train NER using ontonotes dataset, then extract DATE, CARDINAL, ORDINAL, NORP entities from a provided chapter of text\n",
    "# Input >> www.gutenberg.org sourced plain text file for a chapter of a book\n",
    "# Output >> ne.json = { <ne_type> : [ <phrase>, <phrase>, ... ] }\n",
    "#\n",
    "# subtask 4 (text classifier) >> compile a list of characters from the target chapter\n",
    "# Input >> www.gutenberg.org sourced plain text file for a chapter of a book\n",
    "# Output >> characters.txt = plain text set of extracted character names. one line per character name.\n",
    "#\n",
    "\n",
    "exec_ner( chapter_file, ontonotes_file )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO 2021-05-10 14:44:18,627 ontonotes = 'ontonotes_parsed.json'\n",
      "INFO 2021-05-10 14:44:18,631 book = 'eval_book.txt'\n",
      "INFO 2021-05-10 14:44:18,632 chapter = 'eval_chapter.txt'\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-2d7ffc2da0f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mexec_ner\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mchapter_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0montonotes_file\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mdo_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-2d7ffc2da0f0>\u001b[0m in \u001b[0;36mdo_testing\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mexec_regex_toc\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbook_file\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mexec_regex_questions\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mchapter_file\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mexec_ner\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mchapter_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0montonotes_file\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdo_testing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-66-d988aa7700aa>\u001b[0m in \u001b[0;36mexec_ner\u001b[0;34m(file_chapter, ontonotes_file)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0msentences_to_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0montonotes_sentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_ontonotes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0montonotes_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdictNE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_NER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0montonotes_sentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences_to_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-63-b5b1116146ff>\u001b[0m in \u001b[0;36mload_ontonotes\u001b[0;34m(dataset_file)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                         \u001b[0;31m#use nltk pos tags instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                                 \u001b[0;31m#print(\"tup is:\", tup)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                                 \u001b[0mlist_entry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mner_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \"\"\"\n\u001b[0;32m--> 160\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             AP_MODEL_LOC = \"file:\" + str(\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"taggers/averaged_perceptron_tagger/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             )\n\u001b[1;32m    170\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    537\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mGzipFileSystemPathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl2pathname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, _path)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \"\"\"\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file or directory: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/posixpath.py\u001b[0m in \u001b[0;36mabspath\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;34m\"\"\"Return an absolute path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0mcwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwdb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/posixpath.py\u001b[0m in \u001b[0;36misabs\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0misabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;34m\"\"\"Test whether a path is absolute\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#REMOVE THIS CELL FROM HANDIN\n",
    "def do_testing():\n",
    "\t\n",
    "\tontonotes_file = \"ontonotes_parsed.json\"\n",
    "\tbook_file = \"eval_book.txt\"\n",
    "\tchapter_file = \"eval_chapter.txt\"\n",
    "\t\n",
    "\tlogger.info( 'ontonotes = ' + repr(ontonotes_file) )\n",
    "\tlogger.info( 'book = ' + repr(book_file) )\n",
    "\tlogger.info( 'chapter = ' + repr(chapter_file) )\n",
    "\n",
    "\texec_regex_toc( book_file )\n",
    "\texec_regex_questions( chapter_file )\n",
    "\texec_ner( chapter_file, ontonotes_file )\n",
    "\n",
    "do_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
